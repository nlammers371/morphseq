{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f3a4f4",
   "metadata": {},
   "source": [
    "# GroundedDINO Annotation Utilities Demo\n",
    "\n",
    "This notebook demonstrates how to use the `grounded_sam_utils.py` module to:\n",
    "1. Load the GroundingDINO model.\n",
    "2. Create and manage a structured JSON file for annotations.\n",
    "3. Run inference on images with different prompts.\n",
    "4. Save, retrieve, and visualize annotations for the same image from multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7aa841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Utilities imported successfully!\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Add project root to path to allow importing utils\n",
    "SANDBOX_ROOT = Path(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\")\n",
    "if str(SANDBOX_ROOT) not in sys.path:\n",
    "    sys.path.append(str(SANDBOX_ROOT))a\n",
    "\n",
    "# Import the new utilities\n",
    "from scripts.utils.grounded_sam_utils import (\n",
    "    load_config,\n",
    "    load_groundingdino_model,\n",
    "    GroundedDinoAnnotations,\n",
    "    run_inference,\n",
    "    visualize_detections,\n",
    "    inference_and_visualize\n",
    ")\n",
    "from scripts.utils.experiment_metadata_utils import load_experiment_metadata, get_image_id_paths\n",
    "\n",
    "print(\"âœ… Utilities imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb7e9a",
   "metadata": {},
   "source": [
    "# Cell 2: Load Models and Data\n",
    "\n",
    "# Load pipeline configuration\n",
    "config_path = SANDBOX_ROOT / \"configs\" / \"pipeline_config.yaml\"\n",
    "config = load_config(config_path)\n",
    "print(\"âœ… Pipeline config loaded.\")\n",
    "\n",
    "# Load experiment metadata\n",
    "experiment_metadata_path = SANDBOX_ROOT / \"data\" / \"raw_data_organized\" / \"experiment_metadata.json\"\n",
    "experiment_metadata = load_experiment_metadata(experiment_metadata_path)\n",
    "print(\"âœ… Experiment metadata loaded.\")\n",
    "\n",
    "# Load GroundingDINO model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = load_groundingdino_model(config, device=device)\n",
    "\n",
    "# Initialize the annotation manager\n",
    "annotations_path = SANDBOX_ROOT / \"data\" / \"intermediate\" / \"demo_annotations.json\"\n",
    "if annotations_path.exists():\n",
    "    annotations_path.unlink() # Start fresh for the demo\n",
    "    print(f\"Removed existing demo annotations file.\")\n",
    "    \n",
    "annotations_manager = GroundedDinoAnnotations(annotations_path, experiment_metadata)\n",
    "print(\"âœ… Annotation manager initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a5ddc",
   "metadata": {},
   "source": [
    "### Step 1: Run Inference and Save First Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e4f4e",
   "metadata": {},
   "source": [
    "# Cell 3: Run Inference on a Sample Image\n",
    "\n",
    "# Select a sample image\n",
    "sample_image_id = experiment_metadata[\"image_ids\"][100] # Pick an arbitrary image\n",
    "image_path = get_image_id_paths(sample_image_id, experiment_metadata)\n",
    "print(f\"Selected image: {sample_image_id}\")\n",
    "print(f\"Image path: {image_path}\")\n",
    "\n",
    "# Define the first prompt\n",
    "prompt_embryo = \"zebrafish embryo\"\n",
    "detection_params = {\"box_threshold\": 0.4, \"text_threshold\": 0.25}\n",
    "\n",
    "# Run inference\n",
    "boxes, logits, phrases, image_source = run_inference(\n",
    "    model,\n",
    "    image_path,\n",
    "    prompt_embryo,\n",
    "    box_threshold=detection_params[\"box_threshold\"],\n",
    "    text_threshold=detection_params[\"text_threshold\"]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(boxes)} detections for prompt: '{prompt_embryo}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417aafc",
   "metadata": {},
   "source": [
    "# Cell 4: Add Annotation to Storage and Save\n",
    "\n",
    "# Define the model config for this annotation\n",
    "model_config = {\n",
    "    \"model\": \"GroundingDINO_SwinT_OGC\",\n",
    "    \"box_threshold\": detection_params[\"box_threshold\"],\n",
    "    \"text_threshold\": detection_params[\"text_threshold\"]\n",
    "}\n",
    "\n",
    "# Add the annotation\n",
    "annotations_manager.add_annotation(\n",
    "    image_id=sample_image_id,\n",
    "    prompt=prompt_embryo,\n",
    "    model_config=model_config,\n",
    "    boxes=boxes.cpu().numpy(),\n",
    "    logits=logits.cpu().numpy(),\n",
    "    phrases=phrases\n",
    ")\n",
    "\n",
    "# Save the annotations file\n",
    "annotations_manager.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dab5a4",
   "metadata": {},
   "source": [
    "### Step 2: Run Inference with a Different Prompt on the Same Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1dcd8",
   "metadata": {},
   "source": [
    "# Cell 5: Run Inference with a Second Prompt\n",
    "\n",
    "prompt_yolk = \"yolk\"\n",
    "\n",
    "boxes_yolk, logits_yolk, phrases_yolk, _ = run_inference(\n",
    "    model,\n",
    "    image_path,\n",
    "    prompt_yolk,\n",
    "    box_threshold=0.3,\n",
    "    text_threshold=0.25\n",
    ")\n",
    "\n",
    "print(f\"Found {len(boxes_yolk)} detections for prompt: '{prompt_yolk}'\")\n",
    "\n",
    "# Add this second annotation to the same image\n",
    "yolk_model_config = {\"model\": \"GroundingDINO_SwinT_OGC\", \"box_threshold\": 0.3, \"text_threshold\": 0.25}\n",
    "\n",
    "annotations_manager.add_annotation(\n",
    "    image_id=sample_image_id,\n",
    "    prompt=prompt_yolk,\n",
    "    model_config=yolk_model_config,\n",
    "    boxes=boxes_yolk.cpu().numpy(),\n",
    "    logits=logits_yolk.cpu().numpy(),\n",
    "    phrases=phrases_yolk\n",
    ")\n",
    "\n",
    "# Save again\n",
    "annotations_manager.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b2aa2",
   "metadata": {},
   "source": [
    "### Step 3: Retrieve and Visualize All Annotations for the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Retrieve and Visualize Stored Annotations\n",
    "\n",
    "# Retrieve all annotations for the image\n",
    "all_image_annotations = annotations_manager.get_annotations_for_image(sample_image_id)\n",
    "print(f\"Found {len(all_image_annotations)} stored annotation sets for image {sample_image_id}.\")\n",
    "\n",
    "# Loop through and visualize each one\n",
    "for i, annotation_set in enumerate(all_image_annotations):\n",
    "    prompt = annotation_set['prompt']\n",
    "    num_detections = annotation_set['num_detections']\n",
    "    print(f\"\\n--- Visualizing Annotation Set {i+1} ---\")\n",
    "    print(f\"Prompt: '{prompt}' ({num_detections} detections)\")\n",
    "    \n",
    "    # Extract detection data\n",
    "    dets = annotation_set['detections']\n",
    "    boxes_stored = torch.tensor([d['box_xywh'] for d in dets])\n",
    "    logits_stored = torch.tensor([d['confidence'] for d in dets])\n",
    "    phrases_stored = [d['phrase'] for d in dets]\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_detections(\n",
    "        image_source, \n",
    "        boxes_stored, \n",
    "        logits_stored, \n",
    "        phrases_stored,\n",
    "        title=f\"Stored Detections for {sample_image_id}\\nPrompt: '{prompt}'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132514a0",
   "metadata": {},
   "source": [
    "### Step 4: Demonstrate the `inference_and_visualize` Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a388c4",
   "metadata": {},
   "source": [
    "# Cell 7: Use the All-in-One Wrapper\n",
    "\n",
    "print(\"Demonstrating the `inference_and_visualize` wrapper function...\")\n",
    "\n",
    "# Pick a new image and prompt\n",
    "another_image_id = experiment_metadata[\"image_ids\"][200]\n",
    "another_image_path = get_image_id_paths(another_image_id, experiment_metadata)\n",
    "another_prompt = \"dead embryo\"\n",
    "\n",
    "# Run inference and visualize in one step\n",
    "_ = inference_and_visualize(\n",
    "    model,\n",
    "    another_image_path,\n",
    "    another_prompt,\n",
    "    box_threshold=0.3,\n",
    "    text_threshold=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9782888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Simple Summary\n",
    "\n",
    "print(\"=== Demo Summary ===\")\n",
    "\n",
    "# Show what we've accomplished\n",
    "summary = annotations_manager.export_summary()\n",
    "print(f\"âœ… Total images with annotations: {summary['total_images']}\")\n",
    "print(f\"âœ… Total annotation sets: {summary['total_annotations']}\")\n",
    "\n",
    "# Show prompts used for our sample image\n",
    "prompts_for_sample = annotations_manager.get_all_prompts_for_image(sample_image_id)\n",
    "print(f\"âœ… Prompts used for {sample_image_id}: {prompts_for_sample}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Demo complete! Annotations saved to: {annotations_path}\")\n",
    "print(\"\\nðŸ’¡ Key features demonstrated:\")\n",
    "print(\"  - Load GroundingDINO model\")\n",
    "print(\"  - Run inference with different prompts\")  \n",
    "print(\"  - Save annotations to JSON\")\n",
    "print(\"  - Retrieve and visualize saved annotations\")\n",
    "print(\"  - Multiple prompts per image support\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation_grounded_sam",
   "language": "python",
   "name": "segmentation_grounded_sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
