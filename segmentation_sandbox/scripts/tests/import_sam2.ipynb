{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dcaece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "cwd =os.getcwd()\n",
    "\n",
    "sys.path.append(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2\")\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "# os.chdir(\"/Users/marazzanocolon/coding/sam2\")\n",
    "\n",
    "os.chdir(cwd)\n",
    "cwd\n",
    "\n",
    "# sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "# model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "os.chdir(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2/sam2\")\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device= \"cuda\")\n",
    "os.chdir(cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6fc9bc7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\n"
     ]
    }
   ],
   "source": [
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90671ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting working directory: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\n",
      "Restored to: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\n",
      "Changed to SAM2 directory: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2/sam2\n",
      "Building predictor with:\n",
      "  Config: configs/sam2.1/sam2.1_hiera_l.yaml\n",
      "  Checkpoint: ../checkpoints/sam2.1_hiera_large.pt\n",
      "Final directory: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\n",
      "‚úÖ SAM2 model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test the exact working approach\n",
    "import sys\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(f\"Starting working directory: {cwd}\")\n",
    "\n",
    "sys.path.append(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2\")\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "os.chdir(cwd)\n",
    "print(f\"Restored to: {os.getcwd()}\")\n",
    "\n",
    "# Change to SAM2 directory\n",
    "os.chdir(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2/sam2\")\n",
    "print(f\"Changed to SAM2 directory: {os.getcwd()}\")\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "print(f\"Building predictor with:\")\n",
    "print(f\"  Config: {model_cfg}\")\n",
    "print(f\"  Checkpoint: {sam2_checkpoint}\")\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=\"cuda\")\n",
    "\n",
    "# Restore original directory\n",
    "os.chdir(cwd)\n",
    "print(f\"Final directory: {os.getcwd()}\")\n",
    "print(\"‚úÖ SAM2 model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204564c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing load_sam2_model function:\n",
      "üîß Loading SAM2 model...\n",
      "   Config: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2/sam2/configs/sam2.1/sam2.1_hiera_l.yaml\n",
      "   Checkpoint: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2/checkpoints/sam2.1_hiera_large.pt\n",
      "   Device: cuda\n",
      "   Starting directory: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\n",
      "   Changing to SAM2 directory: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2/sam2\n",
      "   Now in: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2/sam2\n",
      "   Using config: configs/sam2.1/sam2.1_hiera_l.yaml\n",
      "   Using checkpoint: ../checkpoints/sam2.1_hiera_large.pt\n",
      "‚úÖ SAM2 model loaded successfully\n",
      "   Restored to: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\n",
      "Function test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test the updated load_sam2_model function\n",
    "sys.path.append(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\")\n",
    "\n",
    "from scripts.utils.sam2_utils import load_sam2_model\n",
    "\n",
    "# Test with the same paths as your working example\n",
    "config_path = \"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2/sam2/configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "checkpoint_path = \"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/models/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "\n",
    "print(\"Testing load_sam2_model function:\")\n",
    "predictor2 = load_sam2_model(config_path, checkpoint_path, device=\"cuda\")\n",
    "print(\"Function test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6878f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the simplified load_sam2_model function\n",
    "# Reload the module to get the updated function\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\")\n",
    "\n",
    "import scripts.utils.sam2_utils as sam2_utils\n",
    "importlib.reload(sam2_utils)\n",
    "\n",
    "print(\"Testing simplified load_sam2_model function:\")\n",
    "# The function parameters don't matter now since it uses hardcoded working paths\n",
    "predictor3 = sam2_utils.load_sam2_model(\"dummy\", \"dummy\", device=\"cuda\")\n",
    "print(\"Simplified function test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dd5c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Add your project paths\n",
    "SANDBOX_ROOT = Path(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox\")\n",
    "if str(SANDBOX_ROOT) not in sys.path:\n",
    "    sys.path.append(str(SANDBOX_ROOT))\n",
    "\n",
    "# Import your utilities\n",
    "from scripts.utils.sam2_utils import GroundedSamAnnotations\n",
    "\n",
    "print(\"‚úÖ Imports complete\")\n",
    "\n",
    "# Cell 2: Load GroundedSamAnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "271d243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading GroundedSamAnnotations from: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/annotation_and_masks/sam2_annotations/grounded_sam_annotations.json\n",
      "üìÅ Seed annotations: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/annotation_and_masks/gdino_annotations/gdino_annotations.json\n",
      "üìÅ Experiment metadata: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/raw_data_organized/experiment_metadata.json\n",
      "   ‚úÖ Results: Found\n",
      "   ‚úÖ Seed annotations: Found\n",
      "   ‚úÖ Experiment metadata: Found\n",
      "üìÅ Loading experiment metadata from: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/raw_data_organized/experiment_metadata.json\n",
      "üìÅ Loading seed annotations from: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/annotation_and_masks/gdino_annotations/gdino_annotations.json\n",
      "üìÅ Loading existing SAM2 results from: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/annotation_and_masks/sam2_annotations/grounded_sam_annotations.json\n",
      "‚úÖ Loaded 5 videos successfully\n",
      "üé¨ GroundedSamAnnotations initialized\n",
      "   Target prompt: 'individual embryo'\n",
      "   Segmentation format: rle\n",
      "   ‚úÖ High-quality seed annotations: 79 experiments\n",
      "   üîß Seed model: GroundedDINO (groundingdino_swint_ogc.pth)\n",
      "‚úÖ Experiment metadata loaded successfully!\n",
      "‚úÖ GroundedSamAnnotations loaded successfully!\n",
      "\n",
      "üìä GROUNDED SAM2 SUMMARY\n",
      "===================================\n",
      "üß™ Experiments: 5\n",
      "üé¨ Videos: 5\n",
      "üß¨ Embryos: 5\n",
      "üéØ Target prompt: 'individual embryo'\n",
      "üì¶ Format: rle\n",
      "üå± Seed model: GroundedDINO (groundingdino_swint_ogc.pth)\n",
      "üå± Seed quality: ‚úÖ High-quality\n",
      "üïí Last updated: 2025-07-10T15:42:48.664979\n",
      "‚úÖ Video generation functions loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your saved GroundedSamAnnotations results\n",
    "grounded_sam_results_path = SANDBOX_ROOT / \"data/annotation_and_masks/sam2_annotations/grounded_sam_annotations.json\"\n",
    "seed_annotations_path = SANDBOX_ROOT / \"data/annotation_and_masks/gdino_annotations/gdino_annotations.json\"\n",
    "experiment_metadata_path = SANDBOX_ROOT / \"data/raw_data_organized/experiment_metadata.json\"\n",
    "\n",
    "print(f\"üìÅ Loading GroundedSamAnnotations from: {grounded_sam_results_path}\")\n",
    "print(f\"üìÅ Seed annotations: {seed_annotations_path}\")\n",
    "print(f\"üìÅ Experiment metadata: {experiment_metadata_path}\")\n",
    "\n",
    "# Check if files exist\n",
    "for path_name, path in [(\"Results\", grounded_sam_results_path), \n",
    "                       (\"Seed annotations\", seed_annotations_path), \n",
    "                       (\"Experiment metadata\", experiment_metadata_path)]:\n",
    "    if path.exists():\n",
    "        print(f\"   ‚úÖ {path_name}: Found\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {path_name}: Not found - {path}\")\n",
    "\n",
    "# Initialize GroundedSamAnnotations with your data\n",
    "grounded_sam_ann = GroundedSamAnnotations(\n",
    "    filepath=grounded_sam_results_path,\n",
    "    seed_annotations_path=seed_annotations_path,\n",
    "    target_prompt=\"individual embryo\",\n",
    "    segmentation_format=\"rle\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Load experiment metadata separately if needed\n",
    "from scripts.utils.experiment_metadata_utils import load_experiment_metadata\n",
    "\n",
    "try:\n",
    "    grounded_sam_ann.experiment_metadata = load_experiment_metadata(experiment_metadata_path)\n",
    "    print(\"‚úÖ Experiment metadata loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load experiment metadata: {e}\")\n",
    "    grounded_sam_ann.experiment_metadata = None\n",
    "\n",
    "print(\"‚úÖ GroundedSamAnnotations loaded successfully!\")\n",
    "grounded_sam_ann.print_summary()\n",
    "\n",
    "# Cell 3: Video Generation Helper Functions\n",
    "# =========================================\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "\n",
    "def rle_to_mask(rle_data: Dict) -> np.ndarray:\n",
    "    \"\"\"Convert RLE format back to binary mask.\"\"\"\n",
    "    try:\n",
    "        from pycocotools import mask as mask_utils\n",
    "        \n",
    "        if isinstance(rle_data, dict) and 'counts' in rle_data:\n",
    "            # Standard RLE format\n",
    "            return mask_utils.decode(rle_data)\n",
    "        elif isinstance(rle_data, dict) and rle_data.get('format') == 'simple_mask':\n",
    "            # Fallback simple format\n",
    "            size = rle_data['size']\n",
    "            data = np.array(rle_data['data']).reshape(size)\n",
    "            return data.astype(np.uint8)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown RLE format\")\n",
    "            \n",
    "    except ImportError:\n",
    "        # Handle case where pycocotools isn't available\n",
    "        if isinstance(rle_data, dict) and rle_data.get('format') == 'simple_mask':\n",
    "            size = rle_data['size']\n",
    "            data = np.array(rle_data['data']).reshape(size)\n",
    "            return data.astype(np.uint8)\n",
    "        else:\n",
    "            raise ValueError(\"pycocotools required for RLE decoding\")\n",
    "\n",
    "\n",
    "def generate_embryo_colors(num_embryos: int) -> List[Tuple[int, int, int]]:\n",
    "    \"\"\"Generate distinct colors for each embryo.\"\"\"\n",
    "    colors = []\n",
    "    for i in range(num_embryos):\n",
    "        hue = i / max(num_embryos, 1)\n",
    "        saturation = 0.8\n",
    "        value = 0.9\n",
    "        rgb = hsv_to_rgb([hue, saturation, value])\n",
    "        # Convert to BGR for OpenCV (0-255 range)\n",
    "        bgr = (int(rgb[2] * 255), int(rgb[1] * 255), int(rgb[0] * 255))\n",
    "        colors.append(bgr)\n",
    "    return colors\n",
    "\n",
    "\n",
    "def create_mask_overlay(image: np.ndarray, masks: Dict[str, Dict], \n",
    "                       embryo_colors: Dict[str, Tuple[int, int, int]],\n",
    "                       alpha: float = 0.5, \n",
    "                       show_contours: bool = True,\n",
    "                       show_labels: bool = True) -> np.ndarray:\n",
    "    \"\"\"Create an overlay of masks on the original image.\"\"\"\n",
    "    overlay = image.copy()\n",
    "    \n",
    "    for embryo_id, mask_data in masks.items():\n",
    "        if embryo_id not in embryo_colors:\n",
    "            continue\n",
    "            \n",
    "        color = embryo_colors[embryo_id]\n",
    "        \n",
    "        # Convert mask to binary\n",
    "        if mask_data['segmentation_format'] == 'rle':\n",
    "            binary_mask = rle_to_mask(mask_data['segmentation'])\n",
    "        else:\n",
    "            # Handle polygon format if needed\n",
    "            continue\n",
    "        \n",
    "        # Create colored mask\n",
    "        colored_mask = np.zeros_like(image)\n",
    "        colored_mask[binary_mask > 0] = color\n",
    "        \n",
    "        # Blend with original image\n",
    "        mask_area = binary_mask > 0\n",
    "        overlay[mask_area] = cv2.addWeighted(\n",
    "            image[mask_area], 1 - alpha, \n",
    "            colored_mask[mask_area], alpha, 0\n",
    "        )\n",
    "        \n",
    "        if show_contours:\n",
    "            # Draw contour lines\n",
    "            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(overlay, contours, -1, color, 2)\n",
    "        \n",
    "        if show_labels:\n",
    "            # Add embryo label\n",
    "            moments = cv2.moments(binary_mask)\n",
    "            if moments[\"m00\"] != 0:\n",
    "                cx = int(moments[\"m10\"] / moments[\"m00\"])\n",
    "                cy = int(moments[\"m01\"] / moments[\"m00\"])\n",
    "                \n",
    "                # Simplified label (e.g., \"e1\" instead of \"20240411_H10_e1\")\n",
    "                simple_label = embryo_id.split('_')[-1]\n",
    "                \n",
    "                cv2.putText(overlay, simple_label, (cx-10, cy+5), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                cv2.putText(overlay, simple_label, (cx-10, cy+5), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "\n",
    "def generate_sam2_videos(grounded_sam_ann: 'GroundedSamAnnotations',\n",
    "                        output_dir: str,\n",
    "                        video_ids: Optional[List[str]] = None,\n",
    "                        experiment_ids: Optional[List[str]] = None,\n",
    "                        max_videos: Optional[int] = None,\n",
    "                        fps: int = 8,\n",
    "                        video_format: str = 'mp4',\n",
    "                        overlay_alpha: float = 0.5,\n",
    "                        show_contours: bool = True,\n",
    "                        show_labels: bool = True,\n",
    "                        show_frame_numbers: bool = True,\n",
    "                        verbose: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"Generate videos with SAM2 mask overlays from GroundedSamAnnotations results.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üé¨ Generating SAM2 mask videos...\")\n",
    "        print(f\"   Output directory: {output_dir}\")\n",
    "    \n",
    "    # Get videos to process\n",
    "    available_videos = grounded_sam_ann.get_processed_video_ids()\n",
    "    \n",
    "    if experiment_ids:\n",
    "        available_videos = [v for v in available_videos if v.split('_')[0] in experiment_ids]\n",
    "    \n",
    "    if video_ids:\n",
    "        available_videos = [v for v in available_videos if v in video_ids]\n",
    "    \n",
    "    if max_videos:\n",
    "        available_videos = available_videos[:max_videos]\n",
    "    \n",
    "    if not available_videos:\n",
    "        if verbose:\n",
    "            print(\"‚ùå No processed videos found to generate\")\n",
    "        return {}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Processing {len(available_videos)} videos\")\n",
    "    \n",
    "    generated_videos = {}\n",
    "    \n",
    "    for video_idx, video_id in enumerate(available_videos, 1):\n",
    "        if verbose:\n",
    "            print(f\"\\nüé• Video {video_idx}/{len(available_videos)}: {video_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Get video data from results\n",
    "            experiment_id = video_id.split('_')[0]\n",
    "            video_data = grounded_sam_ann.results['experiments'][experiment_id]['videos'][video_id]\n",
    "            \n",
    "            if not video_data.get('sam2_success'):\n",
    "                if verbose:\n",
    "                    print(f\"   ‚è≠Ô∏è  Skipping {video_id} (not successfully processed)\")\n",
    "                continue\n",
    "            \n",
    "            # Get embryo info and colors\n",
    "            embryo_ids = video_data['embryo_ids']\n",
    "            embryo_colors_list = generate_embryo_colors(len(embryo_ids))\n",
    "            embryo_colors = {embryo_id: color for embryo_id, color in zip(embryo_ids, embryo_colors_list)}\n",
    "            \n",
    "            # Get image directory\n",
    "            if grounded_sam_ann.experiment_metadata:\n",
    "                # Get from metadata\n",
    "                video_info = None\n",
    "                for exp_data in grounded_sam_ann.experiment_metadata.get(\"experiments\", {}).values():\n",
    "                    if video_id in exp_data.get(\"videos\", {}):\n",
    "                        video_info = exp_data[\"videos\"][video_id]\n",
    "                        break\n",
    "                \n",
    "                if not video_info:\n",
    "                    if verbose:\n",
    "                        print(f\"   ‚ùå Video metadata not found for {video_id}\")\n",
    "                    continue\n",
    "                \n",
    "                image_dir = Path(video_info[\"processed_jpg_images_dir\"])\n",
    "            else:\n",
    "                # Fallback: try to infer from stored metadata\n",
    "                image_dir = Path(video_data.get(\"processed_jpg_images_dir\", \"\"))\n",
    "            \n",
    "            if not image_dir.exists():\n",
    "                if verbose:\n",
    "                    print(f\"   ‚ùå Image directory not found: {image_dir}\")\n",
    "                continue\n",
    "            \n",
    "            # Get sorted image data\n",
    "            images_data = video_data['images']\n",
    "            sorted_image_ids = sorted(images_data.keys(), \n",
    "                                    key=lambda x: images_data[x].get('frame_index', 0))\n",
    "            \n",
    "            if not sorted_image_ids:\n",
    "                if verbose:\n",
    "                    print(f\"   ‚ùå No images found for {video_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Initialize video writer\n",
    "            output_path = output_dir / f\"{video_id}_sam2_masks.{video_format}\"\n",
    "            \n",
    "            # Read first image to get dimensions\n",
    "            first_image_path = image_dir / f\"{sorted_image_ids[0]}.jpg\"\n",
    "            if not first_image_path.exists():\n",
    "                if verbose:\n",
    "                    print(f\"   ‚ùå First image not found: {first_image_path}\")\n",
    "                continue\n",
    "            \n",
    "            first_image = cv2.imread(str(first_image_path))\n",
    "            height, width = first_image.shape[:2]\n",
    "            \n",
    "            # Setup video writer\n",
    "            fourcc = cv2.VideoWriter_fourcc(*('mp4v' if video_format == 'mp4' else 'XVID'))\n",
    "            video_writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"   üìù Processing {len(sorted_image_ids)} frames\")\n",
    "                print(f\"   üé® Embryos: {', '.join([e.split('_')[-1] for e in embryo_ids])}\")\n",
    "            \n",
    "            frames_written = 0\n",
    "            \n",
    "            for frame_idx, image_id in enumerate(sorted_image_ids):\n",
    "                image_path = image_dir / f\"{image_id}.jpg\"\n",
    "                \n",
    "                if not image_path.exists():\n",
    "                    if verbose:\n",
    "                        print(f\"   ‚ö†Ô∏è  Image not found: {image_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Load original image\n",
    "                image = cv2.imread(str(image_path))\n",
    "                if image is None:\n",
    "                    continue\n",
    "                \n",
    "                # Get masks for this frame\n",
    "                frame_masks = images_data[image_id].get('embryos', {})\n",
    "                \n",
    "                # Create overlay\n",
    "                if frame_masks:\n",
    "                    overlay_image = create_mask_overlay(\n",
    "                        image, frame_masks, embryo_colors,\n",
    "                        alpha=overlay_alpha,\n",
    "                        show_contours=show_contours,\n",
    "                        show_labels=show_labels\n",
    "                    )\n",
    "                else:\n",
    "                    overlay_image = image.copy()\n",
    "                \n",
    "                # Add frame number if requested\n",
    "                if show_frame_numbers:\n",
    "                    cv2.putText(overlay_image, f\"Frame {frame_idx}\", \n",
    "                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "                    cv2.putText(overlay_image, f\"Frame {frame_idx}\", \n",
    "                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 1)\n",
    "                \n",
    "                # Add video info\n",
    "                cv2.putText(overlay_image, video_id, \n",
    "                           (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                cv2.putText(overlay_image, video_id, \n",
    "                           (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "                \n",
    "                # Write frame\n",
    "                video_writer.write(overlay_image)\n",
    "                frames_written += 1\n",
    "            \n",
    "            video_writer.release()\n",
    "            \n",
    "            if frames_written > 0:\n",
    "                generated_videos[video_id] = str(output_path)\n",
    "                if verbose:\n",
    "                    print(f\"   ‚úÖ Generated: {output_path.name} ({frames_written} frames)\")\n",
    "            else:\n",
    "                output_path.unlink(missing_ok=True)  # Delete empty video file\n",
    "                if verbose:\n",
    "                    print(f\"   ‚ùå No frames written for {video_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"   ‚ùå Error generating video for {video_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüéâ Generated {len(generated_videos)} videos in {output_dir}\")\n",
    "        for video_id, path in generated_videos.items():\n",
    "            print(f\"   {video_id}: {Path(path).name}\")\n",
    "    \n",
    "    return generated_videos\n",
    "\n",
    "print(\"‚úÖ Video generation functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9301be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d0e4417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading GroundedSamAnnotations from: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/annotation_and_masks/sam2_annotations/grounded_sam_annotations.json\n",
      "üìÅ Loading experiment metadata from: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/raw_data_organized/experiment_metadata.json\n",
      "üìÅ Loading seed annotations from: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/annotation_and_masks/gdino_annotations/gdino_annotations.json\n",
      "üìÅ Loading existing SAM2 results from: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/annotation_and_masks/sam2_annotations/grounded_sam_annotations.json\n",
      "‚úÖ Loaded 5 videos successfully\n",
      "üé¨ GroundedSamAnnotations initialized\n",
      "   Target prompt: 'individual embryo'\n",
      "   Segmentation format: rle\n",
      "   ‚úÖ High-quality seed annotations: 79 experiments\n",
      "   üîß Seed model: GroundedDINO (groundingdino_swint_ogc.pth)\n",
      "‚úÖ GroundedSamAnnotations loaded successfully!\n",
      "\n",
      "üìä GROUNDED SAM2 SUMMARY\n",
      "===================================\n",
      "üß™ Experiments: 5\n",
      "üé¨ Videos: 5\n",
      "üß¨ Embryos: 5\n",
      "üéØ Target prompt: 'individual embryo'\n",
      "üì¶ Format: rle\n",
      "üå± Seed model: GroundedDINO (groundingdino_swint_ogc.pth)\n",
      "üå± Seed quality: ‚úÖ High-quality\n",
      "üïí Last updated: 2025-07-10T15:42:48.664979\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your saved GroundedSamAnnotations results\n",
    "grounded_sam_results_path = SANDBOX_ROOT / \"data/annotation_and_masks/sam2_annotations/grounded_sam_annotations.json\"\n",
    "seed_annotations_path = SANDBOX_ROOT / \"data/annotation_and_masks/gdino_annotations/gdino_annotations.json\"\n",
    "experiment_metadata_path = SANDBOX_ROOT / \"data/raw_data_organized/experiment_metadata.json\"\n",
    "\n",
    "print(f\"üìÅ Loading GroundedSamAnnotations from: {grounded_sam_results_path}\")\n",
    "\n",
    "# Initialize GroundedSamAnnotations with your data\n",
    "grounded_sam_ann = GroundedSamAnnotations(\n",
    "    filepath=grounded_sam_results_path,\n",
    "    seed_annotations_path=seed_annotations_path,\n",
    "    experiment_metadata_path=experiment_metadata_path,  # Add this parameter\n",
    "    target_prompt=\"individual embryo\",\n",
    "    segmentation_format=\"rle\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GroundedSamAnnotations loaded successfully!\")\n",
    "grounded_sam_ann.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "383a0637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debugging video data structure...\n",
      "\n",
      "üìã Sample video: 20231218_H03\n",
      "   SAM2 success: True\n",
      "   Embryo IDs: ['20231218_H03_e1']\n",
      "   Frames processed: 34\n",
      "   Stored image dir: None\n",
      "   Images in results: 34\n",
      "   First image: 20231218_H03_0000\n",
      "     Frame index: 0\n",
      "     Is seed frame: True\n",
      "     Embryos: ['20231218_H03_e1']\n",
      "\n",
      "üóÇÔ∏è  Experiment metadata available: True\n",
      "   Metadata image dir: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/raw_data_organized/20231218/images/20231218_H03\n",
      "   Metadata image count: 34\n",
      "\n",
      "üìÅ Looking for image directory...\n",
      "   Option 1: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/raw_data_organized/20231218/images/20231218_H03 - Image exists: True\n",
      "     ‚úÖ Found working directory!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: Debug Video Data Structure\n",
    "# ===================================\n",
    "\n",
    "print(\"üîç Debugging video data structure...\")\n",
    "\n",
    "# Check a sample video\n",
    "processed_videos = grounded_sam_ann.get_processed_video_ids()\n",
    "if processed_videos:\n",
    "    sample_video = processed_videos[0]\n",
    "    print(f\"\\nüìã Sample video: {sample_video}\")\n",
    "    \n",
    "    # Get video data\n",
    "    exp_id = sample_video.split('_')[0]\n",
    "    video_data = grounded_sam_ann.results['experiments'][exp_id]['videos'][sample_video]\n",
    "    \n",
    "    print(f\"   SAM2 success: {video_data.get('sam2_success')}\")\n",
    "    print(f\"   Embryo IDs: {video_data.get('embryo_ids', [])}\")\n",
    "    print(f\"   Frames processed: {video_data.get('frames_processed')}\")\n",
    "    print(f\"   Stored image dir: {video_data.get('processed_jpg_images_dir')}\")\n",
    "    \n",
    "    # Check images data\n",
    "    images_data = video_data.get('images', {})\n",
    "    print(f\"   Images in results: {len(images_data)}\")\n",
    "    \n",
    "    if images_data:\n",
    "        # Check first image\n",
    "        first_image_id = list(images_data.keys())[0]\n",
    "        first_image_data = images_data[first_image_id]\n",
    "        print(f\"   First image: {first_image_id}\")\n",
    "        print(f\"     Frame index: {first_image_data.get('frame_index')}\")\n",
    "        print(f\"     Is seed frame: {first_image_data.get('is_seed_frame')}\")\n",
    "        print(f\"     Embryos: {list(first_image_data.get('embryos', {}).keys())}\")\n",
    "    \n",
    "    # Check if experiment metadata is available\n",
    "    print(f\"\\nüóÇÔ∏è  Experiment metadata available: {hasattr(grounded_sam_ann, 'experiment_metadata') and grounded_sam_ann.experiment_metadata is not None}\")\n",
    "    \n",
    "    if hasattr(grounded_sam_ann, 'experiment_metadata') and grounded_sam_ann.experiment_metadata:\n",
    "        # Try to find this video in metadata\n",
    "        found_in_metadata = False\n",
    "        for exp_data in grounded_sam_ann.experiment_metadata.get(\"experiments\", {}).values():\n",
    "            if sample_video in exp_data.get(\"videos\", {}):\n",
    "                video_meta = exp_data[\"videos\"][sample_video]\n",
    "                print(f\"   Metadata image dir: {video_meta.get('processed_jpg_images_dir')}\")\n",
    "                print(f\"   Metadata image count: {len(video_meta.get('image_ids', []))}\")\n",
    "                found_in_metadata = True\n",
    "                break\n",
    "        \n",
    "        if not found_in_metadata:\n",
    "            print(f\"   ‚ö†Ô∏è  Video {sample_video} not found in experiment metadata\")\n",
    "    \n",
    "    # Try to find the actual image directory\n",
    "    print(f\"\\nüìÅ Looking for image directory...\")\n",
    "    possible_dirs = [\n",
    "        Path(f\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/raw_data_organized/{exp_id}/images/{sample_video}\"),\n",
    "        Path(f\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/raw_data_organized/{exp_id}/processed_jpg_images\"),\n",
    "        Path(video_data.get('processed_jpg_images_dir', '')) if video_data.get('processed_jpg_images_dir') else None\n",
    "    ]\n",
    "    \n",
    "    for i, possible_dir in enumerate(possible_dirs):\n",
    "        if possible_dir and possible_dir.exists():\n",
    "            # Check if first image exists\n",
    "            if images_data:\n",
    "                first_image_path = possible_dir / f\"{first_image_id}.jpg\"\n",
    "                exists = first_image_path.exists()\n",
    "                print(f\"   Option {i+1}: {possible_dir} - Image exists: {exists}\")\n",
    "                if exists:\n",
    "                    print(f\"     ‚úÖ Found working directory!\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"   Option {i+1}: {possible_dir} - Directory exists but no images to check\")\n",
    "        else:\n",
    "            print(f\"   Option {i+1}: {possible_dir} - Does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18225fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Available processed videos:\n",
      "Total processed videos: 5\n",
      "\n",
      "üß™ Videos by experiment:\n",
      "  20231218: 1 videos\n",
      "    - 20231218_H03\n",
      "  20240530: 1 videos\n",
      "    - 20240530_G07\n",
      "  20240509: 1 videos\n",
      "    - 20240509_H07\n",
      "  20230615: 1 videos\n",
      "    - 20230615_B05\n",
      "  20241023: 1 videos\n",
      "    - 20241023_D12\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Check Available Videos\n",
    "# =============================\n",
    "\n",
    "print(\"üìä Available processed videos:\")\n",
    "processed_videos = grounded_sam_ann.get_processed_video_ids()\n",
    "print(f\"Total processed videos: {len(processed_videos)}\")\n",
    "\n",
    "# Group by experiment\n",
    "experiments = {}\n",
    "for video_id in processed_videos:\n",
    "    exp_id = video_id.split('_')[0]\n",
    "    if exp_id not in experiments:\n",
    "        experiments[exp_id] = []\n",
    "    experiments[exp_id].append(video_id)\n",
    "\n",
    "print(\"\\nüß™ Videos by experiment:\")\n",
    "for exp_id, videos in experiments.items():\n",
    "    print(f\"  {exp_id}: {len(videos)} videos\")\n",
    "    # Show first few videos as examples\n",
    "    for video in videos[:3]:\n",
    "        print(f\"    - {video}\")\n",
    "    if len(videos) > 3:\n",
    "        print(f\"    ... and {len(videos)-3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31e997c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Generating test videos (first 3 videos)...\n",
      "üé¨ Generating SAM2 mask videos...\n",
      "   Output directory: /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/videos/sam2_test\n",
      "üìä Processing 3 videos\n",
      "\n",
      "üé• Video 1/3: 20231218_H03\n",
      "   üìù Processing 34 frames\n",
      "   üé® Embryos: e1\n",
      "   ‚ùå Error generating video for 20231218_H03: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "üé• Video 2/3: 20240530_G07\n",
      "   üìù Processing 81 frames\n",
      "   üé® Embryos: e1\n",
      "   ‚ùå Error generating video for 20240530_G07: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "üé• Video 3/3: 20240509_H07\n",
      "   üìù Processing 41 frames\n",
      "   üé® Embryos: e1\n",
      "   ‚ùå Error generating video for 20240509_H07: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "\n",
      "üéâ Generated 0 videos in /net/trapnell/vol1/home/mdcolon/proj/morphseq/segmentation_sandbox/data/videos/sam2_test\n",
      "\n",
      "‚úÖ Generated 0 test videos:\n"
     ]
    }
   ],
   "source": [
    "#Cell 6: Generate Test Videos (Small Sample)\n",
    "# ===========================================\n",
    "\n",
    "print(\"üé¨ Generating test videos (first 3 videos)...\")\n",
    "\n",
    "output_dir = SANDBOX_ROOT / \"data/videos/sam2_test\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate videos for first 3 processed videos\n",
    "test_videos = generate_sam2_videos(\n",
    "    grounded_sam_ann=grounded_sam_ann,\n",
    "    output_dir=str(output_dir),\n",
    "    max_videos=3,  # Limit to 3 videos for testing\n",
    "    fps=10,        # 10 FPS for faster playback\n",
    "    overlay_alpha=0.6,  # Slightly more opaque masks\n",
    "    show_contours=True,\n",
    "    show_labels=True,\n",
    "    show_frame_numbers=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(test_videos)} test videos:\")\n",
    "for video_id, path in test_videos.items():\n",
    "    file_size = Path(path).stat().st_size / (1024*1024)  # MB\n",
    "    print(f\"  üìΩÔ∏è  {video_id}: {Path(path).name} ({file_size:.1f} MB)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation_grounded_sam",
   "language": "python",
   "name": "segmentation_grounded_sam"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
