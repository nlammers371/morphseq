{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this notebook is to recapitulate in python the phenotype clustering script that Cole wrote in R.**\n",
    "\n",
    "**The key steps in this process are:**\n",
    "1) Load and filter the raw phenotype data\n",
    "2) Convert string phenotypes to wideform binary array\n",
    "3) Perform UMAP compression and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load phenotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to raw data\n",
    "raw_data_dir = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/zfin/20240326/\"\n",
    "\n",
    "# set output directory\n",
    "built_data_dir =  \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/zfin/20240326/built_data_py/\" \n",
    "if not os.path.isdir(built_data_dir):\n",
    "    os.makedirs(built_data_dir)\n",
    "    \n",
    "# load phenotype data and stage DF\n",
    "phenotype_df_cole = pd.read_csv(os.path.join(raw_data_dir, \"clean_zfin_single-mut_with-ids_phenotype_df.csv\"))\n",
    "# stage_to_hpf_key = pd.read_csv(os.path.join(raw_data_dir, \"stage_to_hpf_key.csv\"))\n",
    "# phenotype_df = phenotype_df_raw.merge(stage_to_hpf_key, how = \"left\", on=\"start_stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ontology info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomy_nodes_df = pd.read_csv(os.path.join(raw_data_dir, \"anatomy_item.txt\"), sep='\\t', header=1)\n",
    "anatomy_edges_df = pd.read_csv(os.path.join(raw_data_dir, \"anatomy_relationship.txt\"), sep='\\t', header=1)\n",
    "anatomy_synonyms_df = pd.read_csv(os.path.join(raw_data_dir, \"anatomy_synonyms.txt\"), sep='\\t', header=1)\n",
    "zfin_pheno_df = pd.read_csv(os.path.join(raw_data_dir, \"phenoGeneCleanData_fish.txt\"), sep='\\t', header=1)\n",
    "stage_df = pd.read_csv(os.path.join(raw_data_dir, \"stage_ontology.txt\"), sep='\\t', header=1)\n",
    "# print(anatomy_edges_df.head())\n",
    "# print(anatomy_nodes_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build cleaned zfin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfin_pheno_df = zfin_pheno_df.rename(columns={\n",
    "            \"Affected Structure or Process 1 superterm ID\": \"structure_1_ID\",\n",
    "            \"Affected Structure or Process 1 superterm Name\": \"structure_1\",\n",
    "            \"Affected Structure or Process 2 superterm ID\": \"structure_2_ID\",\n",
    "            \"Affected Structure or Process 2 superterm name\": \"structure_2\",\n",
    "            \"Gene Symbol\" : \"gene\",\n",
    "            \"Gene ID\": \"gene_ID\",\n",
    "            \"Phenotype Keyword ID\": \"pheno_ID\"\n",
    "}).loc[:, [\"gene\", \"gene_ID\", \"structure_1\", \"structure_1_ID\", \"structure_2\", \"structure_2_ID\", \"pheno_ID\",\n",
    "           \"Start Stage ID\", \"End Stage ID\", \"Figure ID\"]]\n",
    "\n",
    "zfin_pheno_df = zfin_pheno_df.merge(phenotype_df_cole.loc[:, \"gene\"].drop_duplicates(), how=\"inner\", on=\"gene\")\n",
    "zfin_pheno_df = zfin_pheno_df.merge(stage_df.loc[:, [\"Stage ID\", \"Begin Hours\"]], how=\"left\", \n",
    "                                    left_on=\"Start Stage ID\", right_on=\"Stage ID\")\n",
    "\n",
    "zfin_pheno_df = zfin_pheno_df.rename(columns={\"Begin Hours\":\"start_hpf\"})\n",
    "\n",
    "zfin_pheno_df = zfin_pheno_df.merge(stage_df.loc[:, [\"Stage ID\", \"End Hours\"]], how=\"left\", \n",
    "                                    left_on=\"End Stage ID\", right_on=\"Stage ID\")\n",
    "\n",
    "zfin_pheno_df = zfin_pheno_df.rename(columns={\"End Hours\":\"end_hpf\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make phenotype DF longform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zfin_pheno_long = pd.wide_to_long(zfin_pheno_df, stubnames=[\"structure\"])\n",
    "zfin_pheno_temp = zfin_pheno_df.copy()\n",
    "# zfin_pheno_long[\"id\"] = zfin_pheno_long.index\n",
    "# id_key1 = zfin_pheno_long.loc[:, [\"structure_1\", \"structure_1_ID\"]].rename(columns={\"structure_1\":\"structure\", \n",
    "#                                                                                     \"structure_1\":\"ID\"})\n",
    "# id_key2 = zfin_pheno_long.loc[:, [\"structure_2\", \"structure_2_ID\"]].rename(columns={\"structure_2\":\"structure\", \n",
    "#                                                                                     \"structure_2\":\"ID\"})\n",
    "                                                                           \n",
    "# id_key = pd.concat([id_key1, id_key2], axis=0, ignore_index=True).drop_duplicates()\n",
    "\n",
    "zfin_pheno1 = zfin_pheno_temp.drop(labels=[\"structure_2\", \"structure_2_ID\", \"Stage ID_x\", \"Stage ID_y\"], \n",
    "                                   axis=1).rename(columns={\"structure_1\":\"structure\", \n",
    "                                                                                    \"structure_1_ID\":\"ID\"})\n",
    "\n",
    "zfin_pheno2 = zfin_pheno_temp.drop(labels=[\"structure_1\", \"structure_1_ID\", \"Stage ID_x\", \"Stage ID_y\"], \n",
    "                                   axis=1).rename(columns={\"structure_2\":\"structure\", \n",
    "                                                                                    \"structure_2_ID\":\"ID\"})\n",
    "\n",
    "zfin_pheno_long = pd.concat([zfin_pheno1, zfin_pheno2], axis=0, ignore_index=True).dropna(\n",
    "    subset=[\"structure\", \"ID\"]).drop_duplicates()\n",
    "\n",
    "\n",
    "zfin_pheno_long.head()\n",
    "zfin_pheno_long.to_csv(os.path.join(built_data_dir, \"zfin_phenotypes_clean.csv\"), index=False)\n",
    "print(zfin_pheno_long.shape)\n",
    "zfin_pheno_long = zfin_pheno_long.drop_duplicates()\n",
    "print(zfin_pheno_long.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up anatomy data and build an ontology graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, construct full graph\n",
    "edge_vec = anatomy_edges_df[\"Relationship Type ID\"].to_list()\n",
    "keep_edge_types = [\"is_a\", \"part of\"]\n",
    "keep_flags = np.asarray([e in keep_edge_types for e in edge_vec])\n",
    "\n",
    "# filter for only desired edge types\n",
    "edge_df = anatomy_edges_df.loc[keep_flags, [\"Parent Item ID\", \"Child Item ID\", \"Relationship Type ID\"]]\n",
    "edge_df.reset_index(inplace=True, drop=True)\n",
    "node_df = anatomy_nodes_df.loc[:, [\"Anatomy ID\", \"Anatomy Name\"]].drop_duplicates()\n",
    "node_df.reset_index(inplace=True, drop=True)\n",
    "node_df.loc[:, \"node_id\"] = node_df.index\n",
    "# construct node dictionary\n",
    "anatomy_nodes_id_vec = node_df[\"Anatomy ID\"].to_numpy()\n",
    "node_container = []\n",
    "for i, a_term in enumerate(node_df[\"Anatomy Name\"]):\n",
    "    node_container.append(tuple([i, {\"name\": a_term, \"id\": anatomy_nodes_id_vec[i]}]))\n",
    "\n",
    "\n",
    "# join node df to edges to get edge IDs\n",
    "edge_df = edge_df.merge(node_df.loc[:, [\"Anatomy ID\", \"node_id\"]], \n",
    "                        how=\"left\", left_on=\"Parent Item ID\", right_on=\"Anatomy ID\")\n",
    "edge_df = edge_df.rename(columns={\"node_id\":\"from_id\"})\n",
    "\n",
    "edge_df = edge_df.merge(node_df.loc[:, [\"Anatomy ID\", \"node_id\"]], \n",
    "                        how=\"left\", left_on=\"Child Item ID\", right_on=\"Anatomy ID\")\n",
    "edge_df = edge_df.rename(columns={\"node_id\":\"to_id\"})\n",
    "                         \n",
    "edge_df = edge_df.loc[:, [\"Parent Item ID\", \"Child Item ID\", \"Relationship Type ID\", \"from_id\", \"to_id\"]]\n",
    "edge_df = edge_df.dropna(subset=[\"from_id\", \"to_id\"])\n",
    "edge_df.reset_index(inplace=True, drop=True)\n",
    "edge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "a_graph = nx.Graph()\n",
    "a_graph.add_nodes_from(node_container)\n",
    "\n",
    "edge_container = []\n",
    "for i in range(edge_df.shape[0]):\n",
    "    edge_container.append(tuple([edge_df.loc[i, \"from_id\"], edge_df.loc[i, \"to_id\"]]))\n",
    "    \n",
    "a_graph.add_edges_from(edge_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # pos = nx.nx_agraph.graphviz_layout(a_graph, prog=\"twopi\", args=\"\")\n",
    "# # x_vec = [pos[i][0] for i in range(len(pos))]\n",
    "# # y_vec = [pos[i][1] for i in range(len(pos))]\n",
    "\n",
    "# pos = nx.nx_agraph.graphviz_layout(a_graph, prog=\"twopi\", args=\"\")\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# nx.draw(a_graph, pos, node_size=20, alpha=0.5, node_color=\"blue\", with_labels=False)\n",
    "# plt.axis(\"equal\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate graph distance between all genes in the phenotypes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "distance_dict = dict(nx.shortest_path_length(a_graph))\n",
    "\n",
    "# make distance matrix\n",
    "dist_mat = np.zeros((len(distance_dict), len(distance_dict)))\n",
    "for i in range(len(distance_dict)):\n",
    "    for j in range(len(distance_dict)):\n",
    "        try:\n",
    "            dist_mat[i, j] = distance_dict[i][j]\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(dist_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, use the penotype graph to calculate pairwise distances between gene phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove phneotypes that present after 72hpf\n",
    "print(zfin_pheno_long.shape)\n",
    "zfin_pheno_ft = zfin_pheno_long.loc[zfin_pheno_long[\"start_hpf\"]<=72, :]\n",
    "print(zfin_pheno_ft.shape)\n",
    "# remove any remaining structure IDs\n",
    "id_vec = zfin_pheno_ft.loc[:, \"ID\"].tolist()\n",
    "keep_flags = np.asarray([\"ZFA\" in i for i in id_vec])\n",
    "zfin_pheno_ft = zfin_pheno_ft.loc[keep_flags]\n",
    "print(zfin_pheno_ft.shape)\n",
    "zfin_pheno_ft.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get node numbers for phneotypes matched to each gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# add node ID info\n",
    "zfin_pheno_node = zfin_pheno_ft.merge(node_df.loc[:, [\"Anatomy ID\", \"node_id\"]].drop_duplicates(), how=\"left\",\n",
    "                                      left_on=\"ID\", right_on=\"Anatomy ID\").drop(labels=\"pheno_ID\", axis=1)\n",
    "\n",
    "zfin_pheno_node = zfin_pheno_node.loc[:, [\"gene\", \"structure\", \"ID\", \"node_id\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "gene_index = np.unique(zfin_pheno_node[\"gene\"])\n",
    "gene_node_list = []\n",
    "for g, gene in enumerate(gene_index):\n",
    "    gene_nodes = zfin_pheno_node.loc[zfin_pheno_node[\"gene\"]==gene, \"node_id\"].to_numpy()\n",
    "    assert len(gene_nodes) > 0\n",
    "    gene_node_list.append(gene_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iterate through genes an calculate phenotypic distances using the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_dist = 5\n",
    "# generate weighted edges\n",
    "gene_edge_container = []\n",
    "gene_dist_mat = np.zeros((len(gene_index), len(gene_index)))\n",
    "for i in tqdm(range(len(gene_index))):\n",
    "    \n",
    "    for j in range(i+1, len(gene_index)):\n",
    "        # get nodes\n",
    "        i_nodes = gene_node_list[i]\n",
    "        j_nodes = gene_node_list[j]\n",
    "        # calculate the shortest distance to a companion node for i-> and j->i\n",
    "        ij_array = np.reshape(dist_mat[j_nodes, i_nodes[:, np.newaxis]], (len(j_nodes), len(i_nodes)))\n",
    "        i_mean = np.mean(np.min(ij_array, axis=0))\n",
    "        j_mean = np.mean(np.min(ij_array, axis=1))\n",
    "        \n",
    "        dist_avg = np.max([i_mean, j_mean])\n",
    "        gene_dist_mat[i , j] = j_mean\n",
    "        gene_dist_mat[j , i] = i_mean\n",
    "            \n",
    "        if dist_avg <= max_dist:\n",
    "            gene_edge_container.append(tuple([i, j, 1 / (0.1 + dist_avg)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "px.imshow(gene_dist_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "weight_vec = np.asarray([g[2] for g in gene_edge_container])\n",
    "fig = px.histogram(x=weight_vec)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nn_keep = 5\n",
    "\n",
    "edge_container_knn = []\n",
    "included_mat = np.zeros((len(gene_index), len(gene_index)))\n",
    "\n",
    "for i in tqdm(range(len(gene_index))):\n",
    "        \n",
    "        dist_vec = gene_dist_mat[i, :]\n",
    "        dist_vec[i] = np.inf\n",
    "        si =  np.argsort(dist_vec)\n",
    "        \n",
    "        for j in si[:nn_keep]:\n",
    "            if (not included_mat[i, j]) and (not included_mat[j, i]):\n",
    "                edge_container_knn.append(tuple([i, j]))\n",
    "                \n",
    "            included_mat[i, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# generate nodes\n",
    "gene_node_container = []\n",
    "for g, gene in enumerate(gene_index):\n",
    "    gene_node_container.append(tuple([g, {\"name\": gene}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# make the graph\n",
    "gene_graph = nx.Graph()\n",
    "gene_graph.add_nodes_from(gene_node_container)\n",
    "\n",
    "gene_graph.add_weighted_edges_from(gene_edge_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gene_graph_knn = nx.Graph()\n",
    "gene_graph_knn.add_nodes_from(gene_node_container)\n",
    "gene_graph_knn.add_edges_from(edge_container_knn)\n",
    "\n",
    "len(gene_graph_knn.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pos=nx.spring_layout(gene_graph_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pos_array = np.empty((len(pos), 2))\n",
    "for p in range(pos_array.shape[0]):\n",
    "    pos_array[p, :] = pos[p]\n",
    "    \n",
    "fig = px.scatter(x=pos_array[:, 0], y=pos_array[:, 1], color=gene_index, opacity=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_dist = 2\n",
    "\n",
    "edge_container_knn_w = []\n",
    "included_mat = np.zeros((len(gene_index), len(gene_index)))\n",
    "\n",
    "for i in tqdm(range(len(gene_index))):\n",
    "    for j in range(len(gene_index)):\n",
    "        \n",
    "        dist = np.max([gene_dist_mat[i, j], gene_dist_mat[j, i]])\n",
    "        dist = np.max([0.1, dist])\n",
    "        if dist <= max_dist:\n",
    "            if (not included_mat[i, j]) and (not included_mat[j, i]):\n",
    "                edge_container_knn_w.append(tuple([i, j, dist]))\n",
    "                \n",
    "            included_mat[i, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gene_graph_kw = nx.Graph()\n",
    "gene_graph_kw.add_nodes_from(gene_node_container)\n",
    "\n",
    "gene_graph_kw.add_weighted_edges_from(edge_container_knn_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pos_kw = nx.spring_layout(gene_graph_kw, k=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pos_array_kw = np.empty((len(pos), 2))\n",
    "for p in range(pos_array_kw.shape[0]):\n",
    "    pos_array_kw[p, :] = pos_kw[p]\n",
    "    \n",
    "fig = px.scatter(x=pos_array_kw[:, 0], y=pos_array_kw[:, 1], color=gene_index)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pos_spec = nx.spectral_layout(gene_graph_kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pos_array_spec = np.empty((len(pos_spec), 2))\n",
    "for p in range(pos_array_spec.shape[0]):\n",
    "    pos_array_spec[p, :] = pos_spec[p]\n",
    "    \n",
    "fig = px.scatter(x=pos_array_spec[:, 0], y=pos_array_spec[:, 1], color=gene_index)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pos_array_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a matrix that we can use for UMAP compression and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pheno_df_wide = pd.pivot_table(phenotype_df_long.loc[:, [\"gene\", \"start_hpf\", \"aff_struct_super_1\", \"val\"]],\n",
    "                         index=[\"gene\"], values=[\"val\"], columns=[\"aff_struct_super_1\"],\n",
    "                              fill_value=0)\n",
    "\n",
    "pheno_df_wide.reset_index(inplace=True)\n",
    "pheno_df_wide.columns = pheno_df_wide.columns.get_level_values(1)\n",
    "pheno_df_wide = pheno_df_wide.rename(columns={\"\": \"gene\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate UMAP projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "\n",
    "# pull out binary phenotype array\n",
    "phen_mat = pheno_df_wide.iloc[:, 1:].to_numpy()\n",
    "\n",
    "# first, we need to use LSA to obtain lower-dim input for UMAP\n",
    "# n_lsa_comp = 3\n",
    "# svd_model = TruncatedSVD(n_components=n_lsa_comp, \n",
    "#                          algorithm='randomized',\n",
    "#                          n_iter=10, random_state=42)\n",
    "# svd_model.fit(phen_mat.T)\n",
    "\n",
    "transformer = PCA(n_components=5, random_state=0)\n",
    "transformer.fit(phen_mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "transformer.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_umap_comp = 2\n",
    "# fit UMAP\n",
    "svd_components = transformer.components_.T\n",
    "reducer = umap.UMAP(n_components=n_umap_comp)\n",
    "# scaled_svd = StandardScaler().fit_transform(svd_components)\n",
    "embedding = reducer.fit_transform(svd_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=embedding[:, 0], y=embedding[:, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(svd_model.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
