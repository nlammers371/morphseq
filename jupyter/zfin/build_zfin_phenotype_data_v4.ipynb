{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load phenotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to raw data\n",
    "raw_data_dir = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/zfin/20240326/\"\n",
    "\n",
    "# set output directory\n",
    "built_data_dir =  \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/zfin/20240326/built_data_py/\" \n",
    "if not os.path.isdir(built_data_dir):\n",
    "    os.makedirs(built_data_dir)\n",
    "    \n",
    "# load phenotype data and stage DF\n",
    "phenotype_df_cole = pd.read_csv(os.path.join(raw_data_dir, \"clean_zfin_single-mut_with-ids_phenotype_df.csv\"))\n",
    "# stage_to_hpf_key = pd.read_csv(os.path.join(raw_data_dir, \"stage_to_hpf_key.csv\"))\n",
    "# phenotype_df = phenotype_df_raw.merge(stage_to_hpf_key, how = \"left\", on=\"start_stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ontology info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomy_nodes_df = pd.read_csv(os.path.join(raw_data_dir, \"anatomy_item.txt\"), sep='\\t', header=1)\n",
    "anatomy_edges_df = pd.read_csv(os.path.join(raw_data_dir, \"anatomy_relationship.txt\"), sep='\\t', header=1)\n",
    "anatomy_synonyms_df = pd.read_csv(os.path.join(raw_data_dir, \"anatomy_synonyms.txt\"), sep='\\t', header=1)\n",
    "zfin_pheno_df_raw = pd.read_csv(os.path.join(raw_data_dir, \"phenoGeneCleanData_fish.txt\"), sep='\\t', header=1)\n",
    "stage_df = pd.read_csv(os.path.join(raw_data_dir, \"stage_ontology.txt\"), sep='\\t', header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each phenotype, check for morpholinos\n",
    "disp_vec = zfin_pheno_df_raw[\"Fish Display Name\"].tolist()\n",
    "MO_names = [disp for disp in disp_vec if \"MO\" in disp]\n",
    "MO_names_alt = [disp for disp in disp_vec if \"+ mo\" in disp.lower()]\n",
    "\n",
    "print(len(disp_vec))\n",
    "print(len(MO_names))\n",
    "print(len(MO_names_alt))\n",
    "# zfin_pheno_df_raw['Phenotype Tag']\n",
    "# zfin_pheno_df_raw['Phenotype Keyword Name']\n",
    "# zfin_pheno_df_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build cleaned zfin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_vec = zfin_pheno_df_raw[\"Fish Display Name\"].tolist()\n",
    "MO_flags = [1 if \"MO\" in disp else 0 for disp in disp_vec]\n",
    "zfin_pheno_df_raw[\"morpholino_flag\"] = MO_flags\n",
    "zfin_pheno_df = zfin_pheno_df_raw.rename(columns={\n",
    "            \"Affected Structure or Process 1 superterm ID\": \"structure_1_ID\",\n",
    "            \"Affected Structure or Process 1 superterm Name\": \"structure_1\",\n",
    "            \"Affected Structure or Process 2 superterm ID\": \"structure_2_ID\",\n",
    "            \"Affected Structure or Process 2 superterm name\": \"structure_2\",\n",
    "            \"Gene Symbol\" : \"gene\",\n",
    "            \"Gene ID\": \"gene_ID\",\n",
    "            \"Phenotype Keyword ID\": \"pheno_ID\"\n",
    "}).loc[:, [\"gene\", \"gene_ID\", \"structure_1\", \"structure_1_ID\", 'Phenotype Tag', 'Phenotype Keyword Name',\n",
    "           \"morpholino_flag\",\n",
    "           \"structure_2\", \"structure_2_ID\", \"pheno_ID\",\n",
    "           \"Start Stage ID\", \"End Stage ID\", \"Figure ID\"]]\n",
    "\n",
    "# keep only genes considered by Cole (consider dropping this)\n",
    "zfin_pheno_df = zfin_pheno_df.merge(phenotype_df_cole.loc[:, \"gene\"].drop_duplicates(), how=\"inner\", on=\"gene\")\n",
    "\n",
    "# add staging info\n",
    "zfin_pheno_df = zfin_pheno_df.merge(stage_df.loc[:, [\"Stage ID\", \"Begin Hours\"]], how=\"left\", \n",
    "                                    left_on=\"Start Stage ID\", right_on=\"Stage ID\")\n",
    "\n",
    "zfin_pheno_df = zfin_pheno_df.rename(columns={\"Begin Hours\":\"start_hpf\"})\n",
    "\n",
    "zfin_pheno_df = zfin_pheno_df.merge(stage_df.loc[:, [\"Stage ID\", \"End Hours\"]], how=\"left\", \n",
    "                                    left_on=\"End Stage ID\", right_on=\"Stage ID\")\n",
    "\n",
    "zfin_pheno_df = zfin_pheno_df.rename(columns={\"End Hours\":\"end_hpf\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make phenotype DF longform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zfin_pheno_long = pd.wide_to_long(zfin_pheno_df, stubnames=[\"structure\"])\n",
    "zfin_pheno_temp = zfin_pheno_df.copy()\n",
    "# zfin_pheno_long[\"id\"] = zfin_pheno_long.index\n",
    "# id_key1 = zfin_pheno_long.loc[:, [\"structure_1\", \"structure_1_ID\"]].rename(columns={\"structure_1\":\"structure\", \n",
    "#                                                                                     \"structure_1\":\"ID\"})\n",
    "# id_key2 = zfin_pheno_long.loc[:, [\"structure_2\", \"structure_2_ID\"]].rename(columns={\"structure_2\":\"structure\", \n",
    "#                                                                                     \"structure_2\":\"ID\"})\n",
    "                                                                           \n",
    "# id_key = pd.concat([id_key1, id_key2], axis=0, ignore_index=True).drop_duplicates()\n",
    "\n",
    "zfin_pheno1 = zfin_pheno_temp.drop(labels=[\"structure_2\", \"structure_2_ID\", \"Stage ID_x\", \"Stage ID_y\"], \n",
    "                                   axis=1).rename(columns={\"structure_1\":\"structure\", \n",
    "                                                                                    \"structure_1_ID\":\"ID\"})\n",
    "\n",
    "# zfin_pheno2 = zfin_pheno_temp.drop(labels=[\"structure_1\", \"structure_1_ID\", \"Stage ID_x\", \"Stage ID_y\"], \n",
    "#                                    axis=1).rename(columns={\"structure_2\":\"structure\", \n",
    "#                                                                                     \"structure_2_ID\":\"ID\"})\n",
    "\n",
    "zfin_pheno_long = zfin_pheno1.dropna(subset=[\"structure\", \"ID\"]).drop(labels=[\"morpholino_flag\"], axis=1).drop_duplicates()\n",
    "\n",
    "print(zfin_pheno_long.shape)\n",
    "gene_pheno_temp = zfin_pheno1.loc[:, [\"gene\", \"ID\", \"start_hpf\", \"morpholino_flag\"]].drop_duplicates()\n",
    "gene_pheno_temp = gene_pheno_temp.groupby([\"gene\", \"ID\", \"start_hpf\"]).max()\n",
    "gene_pheno_temp = gene_pheno_temp.reset_index(drop=False)\n",
    "\n",
    "zfin_pheno_long = zfin_pheno_long.merge(gene_pheno_temp, how=\"left\", on=[\"gene\", \"ID\", \"start_hpf\"])\n",
    "print(zfin_pheno_long.shape)\n",
    "\n",
    "zfin_pheno_long.to_csv(os.path.join(built_data_dir, \"zfin_phenotypes_clean.csv\"), index=False)\n",
    "zfin_pheno_long = zfin_pheno_long.loc[zfin_pheno_long[\"Phenotype Tag\"]=='abnormal', :].drop_duplicates()\n",
    "zfin_pheno_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add on morpholino info\n",
    "gene_pheno_temp = zfin_pheno1.loc[:, [\"gene\", \"ID\", \"start_hpf\", \"morpholino_flag\"]].drop_duplicates()\n",
    "gene_pheno_temp = gene_pheno_temp.groupby([\"gene\", \"ID\", \"start_hpf\"]).max()\n",
    "gene_pheno_temp.reset_index(drop=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for desired time period and zfa identifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove phneotypes that present after 72hpf\n",
    "print(zfin_pheno_long.shape)\n",
    "zfin_pheno_ft = zfin_pheno_long.loc[zfin_pheno_long[\"start_hpf\"]<=72, :].copy()\n",
    "print(zfin_pheno_ft.shape)\n",
    "# remove any remaining structure IDs\n",
    "id_vec = zfin_pheno_ft.loc[:, \"ID\"].tolist()\n",
    "keep_flags = np.asarray([\"ZFA\" in i for i in id_vec])\n",
    "zfin_pheno_ft = zfin_pheno_ft.loc[keep_flags]\n",
    "print(zfin_pheno_ft.shape)\n",
    "# Keep only structures that are in the anatomy graph\n",
    "zfin_pheno_ft = zfin_pheno_ft.merge(anatomy_nodes_df.loc[:, [\"Anatomy ID\"]].drop_duplicates(), how=\"inner\",\n",
    "                                    left_on=\"ID\", right_on=\"Anatomy ID\")\n",
    "zfin_pheno_ft = zfin_pheno_ft.drop(labels=[\"Anatomy ID\"], axis=1)\n",
    "print(zfin_pheno_ft.shape)\n",
    "zfin_pheno_ft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up anatomy data and build an ontology graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, construct full graph\n",
    "edge_vec = anatomy_edges_df[\"Relationship Type ID\"].to_list()\n",
    "keep_edge_types = [\"part of\"]\n",
    "keep_flags = np.asarray([e in keep_edge_types for e in edge_vec])\n",
    "\n",
    "# filter for only desired edge types\n",
    "edge_df = anatomy_edges_df.loc[keep_flags, [\"Parent Item ID\", \"Child Item ID\", \"Relationship Type ID\"]]\n",
    "edge_df.reset_index(inplace=True, drop=True)\n",
    "node_df = anatomy_nodes_df.loc[:, [\"Anatomy ID\", \"Anatomy Name\"]].drop_duplicates()\n",
    "node_df.reset_index(inplace=True, drop=True)\n",
    "node_df.loc[:, \"node_id\"] = node_df.index\n",
    "\n",
    "# get num observations in the zfin database\n",
    "node_df_temp = node_df.copy().loc[:, [\"Anatomy ID\", \"node_id\"]]\n",
    "node_df_temp = node_df_temp.merge(zfin_pheno_ft.loc[:, \"ID\"], how=\"left\", left_on=\"Anatomy ID\", right_on=\"ID\").loc[:, [\"node_id\", \"ID\"]]\n",
    "count_df = node_df_temp.groupby(\"node_id\").count()\n",
    "count_df.reset_index(inplace=True)\n",
    "\n",
    "node_df = node_df.merge(count_df, how=\"left\", on=\"node_id\")\n",
    "node_df = node_df.rename(columns={\"ID\": \"zfin_counts\"})\n",
    "\n",
    "\n",
    "# construct node dictionary\n",
    "anatomy_nodes_id_vec = node_df[\"Anatomy ID\"].to_numpy()\n",
    "node_container = []\n",
    "for i, a_term in enumerate(node_df[\"Anatomy Name\"]):\n",
    "    node_container.append(tuple([i, {\"name\": a_term, \"id\": anatomy_nodes_id_vec[i]}]))\n",
    "\n",
    "\n",
    "# # join node df to edges to get edge IDs\n",
    "edge_df = edge_df.merge(node_df.loc[:, [\"Anatomy ID\", \"node_id\"]], \n",
    "                        how=\"left\", left_on=\"Parent Item ID\", right_on=\"Anatomy ID\")\n",
    "edge_df = edge_df.rename(columns={\"node_id\":\"from_id\"})\n",
    "\n",
    "edge_df = edge_df.merge(node_df.loc[:, [\"Anatomy ID\", \"node_id\"]], \n",
    "                        how=\"left\", left_on=\"Child Item ID\", right_on=\"Anatomy ID\")\n",
    "edge_df = edge_df.rename(columns={\"node_id\":\"to_id\"})\n",
    "                         \n",
    "edge_df = edge_df.loc[:, [\"Parent Item ID\", \"Child Item ID\", \"Relationship Type ID\", \"from_id\", \"to_id\"]]\n",
    "edge_df = edge_df.dropna(subset=[\"from_id\", \"to_id\"])\n",
    "edge_df.reset_index(inplace=True, drop=True)\n",
    "edge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "anatomy_graph = nx.DiGraph()\n",
    "anatomy_graph.add_nodes_from(node_container)\n",
    "\n",
    "edge_container = []\n",
    "for i in range(edge_df.shape[0]):\n",
    "    edge_container.append(tuple([edge_df.loc[i, \"from_id\"], edge_df.loc[i, \"to_id\"]]))\n",
    "    \n",
    "anatomy_graph.add_edges_from(edge_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify nodes with no parent. \n",
    "\n",
    "If a node has no parent and no children, remove it unless it has reported observations in the zfin database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = anatomy_graph.nodes\n",
    "root_node_list = []\n",
    "rm_node_list = []\n",
    "for node in node_list:\n",
    "    n_successors = len(list(anatomy_graph.successors(node)))\n",
    "    n_predecessors = len(list(anatomy_graph.predecessors(node)))\n",
    "    \n",
    "    if (n_predecessors==0) and (n_successors > 0):\n",
    "        root_node_list.append(node)\n",
    "        \n",
    "    elif n_predecessors==0:\n",
    "        if node_df.loc[node_df[\"node_id\"]==node, \"zfin_counts\"].values[0] > 0:\n",
    "            root_node_list.append(node)\n",
    "        else:\n",
    "            rm_node_list.append(node)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_graph_cleaned = anatomy_graph.copy()\n",
    "\n",
    "# remove flagged nodes\n",
    "for node in rm_node_list:\n",
    "    a_graph_cleaned.remove_node(node)\n",
    "    \n",
    "# add master dummy node to connect the graph\n",
    "dummy_id = np.max(node_list) + 1\n",
    "a_graph_cleaned.add_node(dummy_id, name=\"linker_node\", id=\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add edges\n",
    "link_edge_container = []\n",
    "for i in root_node_list:\n",
    "    link_edge_container.append(tuple([dummy_id, i]))\n",
    "    \n",
    "a_graph_cleaned.add_edges_from(link_edge_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos = nx.nx_agraph.graphviz_layout(a_graph_cleaned, prog=\"twopi\", args=\"\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "nx.draw(a_graph_cleaned, pos, node_size=10, alpha=0.5, node_color=\"blue\", with_labels=False)\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to tree topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ChatGPT\n",
    "from collections import deque\n",
    "\n",
    "def graph_to_tree_bfs(graph, root_node):\n",
    "    tree = nx.Graph()\n",
    "    visited = set()\n",
    "    queue = deque([root_node])\n",
    "\n",
    "    while queue:\n",
    "        current_node = queue.popleft()\n",
    "        visited.add(current_node)\n",
    "        tree.add_node(current_node, **graph.nodes[current_node])\n",
    "\n",
    "        for neighbor in graph.neighbors(current_node):\n",
    "            if neighbor not in visited:\n",
    "                tree.add_edge(current_node, neighbor)\n",
    "                queue.append(neighbor)\n",
    "                visited.add(neighbor)\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "def graph_to_directed_tree_bfs(graph, root_node):\n",
    "    tree = nx.DiGraph()\n",
    "    visited = set()\n",
    "    queue = deque([root_node])\n",
    "\n",
    "    while queue:\n",
    "        current_node = queue.popleft()\n",
    "        visited.add(current_node)\n",
    "        tree.add_node(current_node, **graph.nodes[current_node])\n",
    "\n",
    "        for neighbor in graph.neighbors(current_node):\n",
    "            if neighbor not in visited:\n",
    "                tree.add_edge(current_node, neighbor)\n",
    "                queue.append(neighbor)\n",
    "                visited.add(neighbor)\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose any node as the root\n",
    "root_node = dummy_id # node_df.loc[node_df[\"Anatomy Name\"]==\"whole organism\", \"node_id\"].to_numpy()[0]\n",
    "\n",
    "# Convert undirected graph to directed tree\n",
    "a_tree = graph_to_tree_bfs(a_graph_cleaned, root_node)\n",
    "a_tree_dir = graph_to_directed_tree_bfs(a_graph_cleaned, root_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos = nx.nx_agraph.graphviz_layout(a_tree, prog=\"twopi\", args=\"\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "nx.draw(a_tree, pos, node_size=10, alpha=0.5, node_color=\"blue\", with_labels=False)\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate aggregate observations that include the node AND its children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_index = np.asarray(list(a_tree_dir.nodes))\n",
    "node_index = node_index[np.asarray(node_index) != dummy_id]\n",
    "# generate count dict\n",
    "node_count_dict = dict({})\n",
    "for node in node_index:\n",
    "    z_counts = node_df.loc[node_df[\"node_id\"]==node, \"zfin_counts\"].values[0]\n",
    "    node_count_dict[node] = z_counts\n",
    "    \n",
    "# get DF that contains only the nodes we kept\n",
    "node_df_tree = node_df.copy()\n",
    "keep_indices = np.asarray([i for i in node_df_tree[\"node_id\"] if i in node_index])\n",
    "node_df_tree = node_df_tree.loc[keep_indices, :]\n",
    "node_df_tree.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# get counts that include children\n",
    "for node in node_index:\n",
    "    \n",
    "    # initialize\n",
    "    z_counts = 0\n",
    "    \n",
    "    # get counts from successor nodes\n",
    "    d_nodes = list(nx.descendants(a_tree_dir, node))\n",
    "    nz_counts = 0\n",
    "    for d in d_nodes:\n",
    "        z_counts += node_count_dict[d]\n",
    "        if node_count_dict[d] > 0:\n",
    "            nz_counts += 1\n",
    "        \n",
    "    node_df_tree.loc[node_df_tree[\"node_id\"]==node, \"d_counts\"] = z_counts\n",
    "    node_df_tree.loc[node_df_tree[\"node_id\"]==node, \"n_counts\"] = len(d_nodes)\n",
    "    node_df_tree.loc[node_df_tree[\"node_id\"]==node, \"nz_counts\"] = nz_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_limit = np.percentile(node_df_tree[\"zfin_counts\"], 95)\n",
    "y_limit = np.percentile(node_df_tree[\"d_counts\"], 95)\n",
    "\n",
    "node_df_tree.loc[:, \"importance_flag\"] = 0\n",
    "i_filter = (node_df_tree[\"zfin_counts\"] >= x_limit) & (node_df_tree[\"d_counts\"] >= y_limit)\n",
    "node_df_tree.loc[i_filter, \"importance_flag\"] = 1\n",
    "node_df_tree.loc[:, \"importance_flag\"] = node_df_tree.loc[:, \"importance_flag\"].astype(str)\n",
    "\n",
    "node_df_tree[\"log_n_counts\"] = 25 + node_df_tree.loc[:, \"n_counts\"].to_numpy() #np.log(node_df_tree.loc[:, \"n_counts\"].to_numpy() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/zfin/figures/\"\n",
    "\n",
    "fig = px.scatter(node_df_tree, x=\"zfin_counts\", y=\"d_counts\", color=\"importance_flag\", size=\"log_n_counts\",\n",
    "                 log_x=True, log_y=True,\n",
    "                 labels={\"zfin_counts\": \"direct zfin observations\", \n",
    "                         \"d_counts\": \"descendant zfin observations\",\n",
    "                         \"importance_flag\": \"importance flag\",\n",
    "                         \"n_counts\": \"number of descendant phenotypes\"},\n",
    "                hover_data={\"Anatomy Name\":True, \"Anatomy ID\":True, \"log_n_counts\":False, \n",
    "                            \"n_counts\":True, \"importance_flag\": False})\n",
    "\n",
    "fig.update_layout(showlegend=False,\n",
    "    yaxis_title=\"# descendant reports\", xaxis_title=\"# direct reports on zfin\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(fig_path, \"zfin_importance_scatter.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, iterate through genes and calculate the following:\n",
    "\n",
    "1) Total importance across all phenotypes\n",
    "2) TF flag\n",
    "3) Top 3 (?) reported phenotypes by importance\n",
    "4) Effects for those 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gene ID table\n",
    "gene_info_path = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/zfin/20240326/built_data/gene_set_mf.csv\"\n",
    "gene_info_df = pd.read_csv(gene_info_path)\n",
    "gene_info_df = gene_info_df.loc[:, [\"gene_symbol\", \"gs_name\", \"gs_description\"]].rename(columns={\"gene_symbol\": \"gene\"})\n",
    "tf_flag_vec = [1 if \"transcription\" in name.lower() else 0 for name in list(gene_info_df[\"gs_name\"]) ]\n",
    "gene_info_df[\"TF_flag\"] = tf_flag_vec\n",
    "\n",
    "# Make gene phenotype DF and join on the above info\n",
    "gene_pheno_df = zfin_pheno_ft.loc[:, [\"gene\", \"gene_ID\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Note, about 15% of genes are not in this reference table and so may be incorrectly labeled as non-TF\n",
    "gene_pheno_df = gene_pheno_df.merge(gene_info_df.loc[gene_info_df[\"TF_flag\"]==1, [\"gene\", \"TF_flag\"]].drop_duplicates(),\n",
    "                                    how=\"left\", on=\"gene\")\n",
    "\n",
    "gene_pheno_df.loc[np.isnan(gene_pheno_df[\"TF_flag\"]), \"TF_flag\"] = 0\n",
    "\n",
    "gene_pheno_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through each gene and calculate an aggregate importance score, as well as its top 3 \"most important\" phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "zfin_pheno_node = zfin_pheno_ft.merge(node_df.loc[:, [\"Anatomy ID\", \"node_id\"]].drop_duplicates(), how=\"left\",\n",
    "                                      left_on=\"ID\", right_on=\"Anatomy ID\").drop(labels=\"Anatomy ID\", axis=1)\n",
    "\n",
    "zfin_pheno_node = zfin_pheno_node.loc[zfin_pheno_node[\"morpholino_flag\"]==0\n",
    "                                        , [\"gene\", \"structure\", \"ID\", \"Phenotype Keyword Name\",\n",
    "                                          \"start_hpf\", \"node_id\"]].drop_duplicates().reset_index(drop=True).rename(\n",
    "                                        columns={\"Phenotype Keyword Name\":\"keyword\"})\n",
    "\n",
    "zfin_pheno_node = zfin_pheno_node.loc[zfin_pheno_node[\"start_hpf\"]>0, :]\n",
    "# filter out evidence from mopholino experiment\n",
    "\n",
    "gene_index = list(gene_pheno_df[\"gene\"])\n",
    "gene_node_list = []\n",
    "for g, gene in enumerate(tqdm(gene_index)):\n",
    "    \n",
    "    # get nodes for each phenotype\n",
    "    pheno_nodes = zfin_pheno_node.loc[zfin_pheno_node[\"gene\"]==gene, \"node_id\"].to_numpy()\n",
    "    \n",
    "    # get de-duped list of all descendants\n",
    "    d_list = list(np.unique(pheno_nodes))\n",
    "    for d in d_list:\n",
    "        d_list += list(nx.descendants(a_tree_dir, d))\n",
    "    \n",
    "    d_index = np.unique(d_list)\n",
    "    nz_count = 0\n",
    "    for d in d_index:\n",
    "        if node_count_dict[d] > 0:\n",
    "            nz_count += 1\n",
    "            \n",
    "    # add to DF\n",
    "    gene_pheno_df.loc[g, \"importance_score\"] = nz_count\n",
    "    \n",
    "    # get importance ranking for each phenotype\n",
    "    nz_count_list = []\n",
    "    dd_nodes = np.unique(pheno_nodes)\n",
    "    for node in dd_nodes:\n",
    "        nz = node_df_tree.loc[node_df_tree[\"node_id\"]==node, \"nz_counts\"].values[0] + 1\n",
    "        nz_count_list.append(nz)\n",
    "        \n",
    "    si = np.argsort(nz_count_list)\n",
    "    \n",
    "    # add info to table\n",
    "    gene_table = zfin_pheno_node.loc[zfin_pheno_node[\"gene\"]==gene, :].reset_index(drop=True)\n",
    "    ranked_node_ids = dd_nodes[si[::-1]]\n",
    "    if len(ranked_node_ids) > 0:\n",
    "        filter0 = gene_table[\"node_id\"]==ranked_node_ids[0]\n",
    "        gene_pheno_df.loc[g, \"phenotype_1\"] = gene_table.loc[filter0, \"structure\"].values[0]\n",
    "        gene_pheno_df.loc[g, \"effect_1\"] = gene_table.loc[filter0, \"keyword\"].values[0]\n",
    "        gene_pheno_df.loc[g, \"start_hpf_1\"] = gene_table.loc[filter0, \"start_hpf\"].values[0]\n",
    "    else:\n",
    "        gene_pheno_df.loc[g, \"phenotype_1\"] = \"\"\n",
    "        gene_pheno_df.loc[g, \"effect_1\"] = \"\"\n",
    "        gene_pheno_df.loc[g, \"start_hpf_1\"] = np.nan\n",
    "    \n",
    "    if len(ranked_node_ids) > 1:\n",
    "        filter1 = gene_table[\"node_id\"]==ranked_node_ids[1]\n",
    "        gene_pheno_df.loc[g, \"phenotype_2\"] = gene_table.loc[filter1, \"structure\"].values[0]\n",
    "        gene_pheno_df.loc[g, \"effect_2\"] = gene_table.loc[filter1, \"keyword\"].values[0]\n",
    "        gene_pheno_df.loc[g, \"start_hpf_2\"] = gene_table.loc[filter1, \"start_hpf\"].values[0]\n",
    "    else:\n",
    "        gene_pheno_df.loc[g, \"phenotype_2\"] = \"\"\n",
    "        gene_pheno_df.loc[g, \"effect_2\"] = \"\"\n",
    "        gene_pheno_df.loc[g, \"start_hpf_2\"] = np.nan\n",
    "        \n",
    "    if len(ranked_node_ids) > 2:\n",
    "        filter2 = gene_table[\"node_id\"]==ranked_node_ids[2]\n",
    "        gene_pheno_df.loc[g, \"phenotype_3\"] = gene_table.loc[filter2, \"structure\"].values[0]\n",
    "        gene_pheno_df.loc[g, \"effect_3\"] = gene_table.loc[filter2, \"keyword\"].values[0]\n",
    "        gene_pheno_df.loc[g, \"start_hpf_3\"] = gene_table.loc[filter2, \"start_hpf\"].values[0]\n",
    "    else:\n",
    "        gene_pheno_df.loc[g, \"phenotype_3\"] = \"\"\n",
    "        gene_pheno_df.loc[g, \"effect_3\"] = \"\"\n",
    "        gene_pheno_df.loc[g, \"start_hpf_3\"] = np.nan\n",
    "        \n",
    "        \n",
    "# remove genes with no phenotype\n",
    "print(gene_pheno_df.shape)\n",
    "gene_pheno_df = gene_pheno_df.loc[gene_pheno_df[\"phenotype_1\"] != \"\", :]\n",
    "print(gene_pheno_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_pheno_df = gene_pheno_df.sort_values(by=[\"TF_flag\", \"importance_score\"], axis=0, ascending=False)\n",
    "gene_pheno_df.reset_index(inplace=True, drop=True)\n",
    "gene_pheno_df.to_csv(os.path.join(built_data_dir, \"zfin_gene_KO_candidates.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfin_pheno_node.loc[zfin_pheno_node[\"gene\"]==\"otx2b\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the phenotype table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_counts = 1\n",
    "node_df_tree[\"agg_counts\"] = node_df_tree[\"zfin_counts\"] + node_df_tree[\"d_counts\"]\n",
    "count_filter = (node_df_tree[\"zfin_counts\"] >= min_counts) & (node_df_tree[\"agg_counts\"] >= 10)\n",
    "nodes_filtered = node_df_tree.loc[count_filter, \"node_id\"].to_numpy()\n",
    "\n",
    "zfin_pheno_node2 = zfin_pheno_ft.merge(node_df.loc[:, [\"Anatomy ID\", \"node_id\"]].drop_duplicates(), how=\"left\",\n",
    "                                      left_on=\"ID\", right_on=\"Anatomy ID\").drop(labels=\"Anatomy ID\", axis=1)\n",
    "\n",
    "\n",
    "gene_index = np.unique(zfin_pheno_node2[\"gene\"])\n",
    "phenotype_array = np.zeros((len(gene_index), len(nodes_filtered)), dtype=np.uint8)\n",
    "\n",
    "for g, gene in enumerate(tqdm(gene_index)):\n",
    "    p_nodes = np.unique(zfin_pheno_node2.loc[zfin_pheno_node2[\"gene\"]==gene, \"node_id\"])\n",
    "    for node in p_nodes:\n",
    "        ft = node == nodes_filtered\n",
    "        if np.sum(ft) == 1:\n",
    "            phenotype_array[g, ft] = 1\n",
    "            \n",
    "phenotype_array = phenotype_array[np.max(phenotype_array, axis=1) > 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "\n",
    "# gene_dist_mat_fit = gene_dist_mat.copy()\n",
    "# gene_dist_mat_fit[gene_dist_mat_fit > 6] = 6\n",
    "# pull out binary phenotype array\n",
    "n_lsa_comp = 25\n",
    "svd_model = TruncatedSVD(n_components=n_lsa_comp, \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10, random_state=42)\n",
    "svd_model.fit(phenotype_array.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "keep_indices = np.where(svd_model.explained_variance_ratio_ >= 0.005)[0]\n",
    "print(keep_indices)\n",
    "\n",
    "n_umap_comp = 2\n",
    "# fit UMAP\n",
    "svd_components = svd_model.components_.T[:, keep_indices]\n",
    "reducer = umap.UMAP(n_components=n_umap_comp, n_neighbors=15)\n",
    "scaled_svd = StandardScaler().fit_transform(svd_components)\n",
    "embedding = reducer.fit_transform(scaled_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=embedding[:, 0], y=embedding[:, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=svd_components[:, 0], y=svd_components[:, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try using a weighted graph to calculate distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a weighted version of the graph\n",
    "node_key = np.asarray(list(a_tree)).astype(int)\n",
    "\n",
    "# calculate node depths\n",
    "depth_vec = np.empty((np.max(node_key)+1,))\n",
    "depth_vec[:] = np.nan\n",
    "for n in node_key:\n",
    "    depth_vec[n] = nx.shortest_path_length(a_tree, source=root_node, target=n)\n",
    "    \n",
    "# get unique list of depths\n",
    "depth_index = np.unique(depth_vec)\n",
    "wt_edge_container = []\n",
    "\n",
    "# generate new edge list\n",
    "for node_i in tqdm(a_tree.nodes):\n",
    "    for node_j in a_tree.nodes:\n",
    "        if a_tree.has_edge(node_i, node_j):\n",
    "            level = np.min([depth_vec[node_i], depth_vec[node_j]])\n",
    "            wt = 2**(-level)\n",
    "            wt_edge_container.append(tuple([node_i, node_j, wt]))\n",
    "            \n",
    "# make tree\n",
    "a_tree_weighted = nx.Graph()\n",
    "a_tree_weighted.nodes = a_tree.nodes\n",
    "a_tree_weighted.add_weighted_edges_from(wt_edge_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = nx.get_edge_attributes(a_tree_weighted, \"weight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat[865, 1031]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_index[1031]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat[int(node_index[1031]), int(node_index[865])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# a_tree_uni = anatomy_tree.to_undirected()\n",
    "\n",
    "distance_dict = dict(nx.shortest_path_length(a_tree_weighted, weight=\"weight\"))\n",
    "node_list = list(a_tree_weighted.nodes)\n",
    "# make distance matrix\n",
    "dist_mat = np.zeros((len(distance_dict), len(distance_dict)))\n",
    "for i in range(len(distance_dict)):\n",
    "    for j in range(len(distance_dict)):\n",
    "        try:\n",
    "            dist_mat[i, j] = distance_dict[node_list[i]][node_list[j]]\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_index = np.zeros((np.max(node_list)+1,))-1\n",
    "for i, node in enumerate(node_list):\n",
    "    node_index[node] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(dist_mat)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, construct a gene-level graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node ID info\n",
    "zfin_pheno_node2 = zfin_pheno_ft.merge(node_df.loc[:, [\"Anatomy ID\", \"node_id\"]].drop_duplicates(), how=\"left\",\n",
    "                                      left_on=\"ID\", right_on=\"Anatomy ID\").drop(labels=\"pheno_ID\", axis=1)\n",
    "\n",
    "zfin_pheno_node2 = zfin_pheno_node2.loc[:, [\"gene\", \"structure\", \"ID\", \"node_id\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "gene_index = np.unique(zfin_pheno_node2[\"gene\"])\n",
    "gene_node_list = []\n",
    "for g, gene in enumerate(gene_index):\n",
    "    gene_nodes = zfin_pheno_node2.loc[zfin_pheno_node2[\"gene\"]==gene, \"node_id\"].to_numpy()\n",
    "    assert len(gene_nodes) > 0\n",
    "    gene_node_list.append(gene_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_dist = 3\n",
    "# generate weighted edges\n",
    "gene_edge_container = []\n",
    "gene_dist_mat = np.zeros((len(gene_index), len(gene_index)))\n",
    "for i in tqdm(range(len(gene_index))):\n",
    "    \n",
    "    for j in range(i+1, len(gene_index)):\n",
    "        # get nodes\n",
    "        i_nodes = node_index[gene_node_list[i]].astype(int)\n",
    "        j_nodes = node_index[gene_node_list[j]].astype(int)\n",
    "        # convert to indices\n",
    "        \n",
    "        # calculate the shortest distance to a companion node for i-> and j->i\n",
    "        ij_array = np.reshape(dist_mat[j_nodes, i_nodes[:, np.newaxis]], (len(j_nodes), len(i_nodes)))\n",
    "        i_mean = np.mean(np.min(ij_array, axis=0))\n",
    "        j_mean = np.mean(np.min(ij_array, axis=1))\n",
    "        \n",
    "        dist_avg = np.max([i_mean, j_mean])\n",
    "        gene_dist_mat[i , j] = i_mean\n",
    "        gene_dist_mat[j , i] = j_mean\n",
    "            \n",
    "        if dist_avg <= max_dist:\n",
    "            gene_edge_container.append(tuple([i, j, 1 / (0.1 + dist_avg)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.imshow(gene_dist_mat)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "\n",
    "gene_dist_mat_fit = gene_dist_mat.copy()\n",
    "# gene_dist_mat_fit[gene_dist_mat_fit > 6] = 6\n",
    "# pull out binary phenotype array\n",
    "n_lsa_comp = 100\n",
    "svd_model = TruncatedSVD(n_components=n_lsa_comp, \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10, random_state=42)\n",
    "svd_model.fit(gene_dist_mat_fit.T)\n",
    "# keep_indices = np.where(svd_model.explained_variance_ratio_ >= 0.01)[0]\n",
    "# print(keep_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "keep_indices = np.where(svd_model.explained_variance_ratio_ >= 0.001)[0]\n",
    "print(keep_indices)\n",
    "\n",
    "n_umap_comp = 2\n",
    "# fit UMAP\n",
    "# svd_components = svd_model.components_.T[:, keep_indices]\n",
    "svd_components = gene_dist_mat\n",
    "reducer = umap.UMAP(n_components=n_umap_comp, n_neighbors=5)\n",
    "scaled_svd = StandardScaler().fit_transform(svd_components)\n",
    "embedding = reducer.fit_transform(scaled_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=embedding[:, 0], y=embedding[:, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try rolling up tree graph to a fixed level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "node_key = np.asarray(list(anatomy_tree)).astype(int)\n",
    "# calculate node depths\n",
    "depth_vec = np.empty((np.max(node_key)+1,))\n",
    "depth_vec[:] = np.nan\n",
    "for n in node_key:\n",
    "    depth_vec[n] = nx.shortest_path_length(anatomy_tree, source=root_node, target=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_index, d_counts = np.unique(depth_vec[~np.isnan(depth_vec)], return_counts=True)\n",
    "print(d_counts)\n",
    "level_dict = dict({})\n",
    "for d in range(len(d_counts)):\n",
    "    d_nodes = np.where(depth_vec==d)[0]\n",
    "    d_id_vec = np.arange(len(d_nodes))\n",
    "    lvl_dict = dict([(d_nodes[i], d_id_vec[i]) for i in range(len(d_nodes))])\n",
    "    level_dict[d] = lvl_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that traverses up or down the tree to find the parent/child nodes at the desired description level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phenotype_vector(root_id, target_id, level, level_dict, phenotype_vec, target_depth):\n",
    "        \n",
    "    lvl_dict = level_dict[level]\n",
    "\n",
    "    if target_depth == level:\n",
    "        phenotype_vec[lvl_dict[target_id]] += 1\n",
    "\n",
    "    elif target_depth > level:\n",
    "        n_iters = target_depth - level\n",
    "        query_nodes = [target_id]\n",
    "        for n in range(int(n_iters)):\n",
    "            p_nodes = []\n",
    "            for q in query_nodes:\n",
    "                p_nodes += list(anatomy_tree_dir.predecessors(q))\n",
    "            query_nodes = p_nodes\n",
    "\n",
    "        for p in p_nodes:\n",
    "            phenotype_vec[lvl_dict[p]] += 1\n",
    "\n",
    "    elif target_depth < level:\n",
    "        n_iters = level - target_depth\n",
    "        query_nodes = [target_id]\n",
    "        for n in range(int(n_iters)):\n",
    "            s_nodes = []\n",
    "            for q in query_nodes:\n",
    "                s_nodes += list(anatomy_tree_dir.successors(q))\n",
    "            query_nodes = s_nodes\n",
    "\n",
    "        for s in s_nodes:\n",
    "            phenotype_vec[lvl_dict[s]] += 1\n",
    "            \n",
    "    return phenotype_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_id = 3080\n",
    "target_id = 2376\n",
    "level = 3\n",
    "\n",
    "# initialize vector to store results\n",
    "phenotype_vec = np.zeros((d_counts[level], 1))\n",
    "target_depth = depth_vec[target_id]\n",
    "\n",
    "phenotype_vec = update_phenotype_vector(root_id, target_id, level, level_dict, phenotype_vec, target_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = anatomy_tree_dir.nodes\n",
    "\n",
    "level = 1\n",
    "node_list = list(level_dict[level].keys())\n",
    "node_info = np.asarray([all_nodes[n] for n in node_list])\n",
    "print(node_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_nodes[1031])\n",
    "list(anatomy_tree_dir.successors(1031))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes[531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(anatomy_tree_dir.successors(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_vec = np.zeros((np.max(node_key)+1,))\n",
    "for n in node_key:\n",
    "    ns_vec[n] = len(list(anatomy_tree_dir.successors(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = np.argsort(ns_vec)\n",
    "top_nodes = si[::-1][:25]\n",
    "\n",
    "node_info = np.asarray([all_nodes[n] for n in top_nodes])\n",
    "print(node_info)\n",
    "print(ns_vec[top_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of nodes/phenotypes that correspond to each gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add node ID info\n",
    "zfin_pheno_node = zfin_pheno_ft.merge(node_df.loc[:, [\"Anatomy ID\", \"node_id\"]].drop_duplicates(), how=\"left\",\n",
    "                                      left_on=\"ID\", right_on=\"Anatomy ID\").drop(labels=\"pheno_ID\", axis=1)\n",
    "\n",
    "zfin_pheno_node = zfin_pheno_node.loc[:, [\"gene\", \"structure\", \"ID\", \"node_id\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "gene_index = np.unique(zfin_pheno_node[\"gene\"])\n",
    "gene_node_list = []\n",
    "for g, gene in enumerate(gene_index):\n",
    "    gene_nodes = zfin_pheno_node.loc[zfin_pheno_node[\"gene\"]==gene, \"node_id\"].to_numpy()\n",
    "    assert len(gene_nodes) > 0\n",
    "    gene_node_list.append(gene_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "root_id = 3080\n",
    "level = 1\n",
    "\n",
    "# generate array to store phenotype_vectors\n",
    "phenotype_array = np.zeros((len(gene_index), d_counts[level]))\n",
    "\n",
    "# iterate through each gene to build a phenotype vector\n",
    "for g, gi in tqdm(enumerate(gene_index)):\n",
    "    # initialize vector\n",
    "    phenotype_vec = np.zeros((d_counts[level],))\n",
    "    # get list of phenotypes\n",
    "    gene_nodes = gene_node_list[g]\n",
    "    \n",
    "    for target_id in gene_nodes:\n",
    "        \n",
    "        if target_id != root_id:\n",
    "            # get depth\n",
    "            target_depth = depth_vec[target_id]\n",
    "            # update phenotype vector\n",
    "            phenotype_vec = update_phenotype_vector(root_id, target_id, level, level_dict, phenotype_vec, target_depth)\n",
    "        \n",
    "    # add to main array\n",
    "    phenotype_array[g, :] = phenotype_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_flags = np.sum(phenotype_array, axis=1) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(phenotype_array[0:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate UMAP projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "\n",
    "# pull out binary phenotype array\n",
    "n_lsa_comp = 25\n",
    "svd_model = TruncatedSVD(n_components=n_lsa_comp, \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=100, random_state=42)\n",
    "svd_model.fit(phenotype_array[keep_flags, :].T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_umap_comp = 2\n",
    "# fit UMAP\n",
    "svd_components = svd_model.components_.T[:, :14]\n",
    "reducer = umap.UMAP(n_components=n_umap_comp)\n",
    "# scaled_svd = StandardScaler().fit_transform(svd_components)\n",
    "embedding = reducer.fit_transform(svd_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=embedding[:, 0], y=embedding[:, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "n_umap_comp = 2\n",
    "# fit UMAP\n",
    "# svd_components = svd_model.components_.T\n",
    "reducer = umap.UMAP(n_components=n_umap_comp)\n",
    "# scaled_svd = StandardScaler().fit_transform(svd_components)\n",
    "embedding_full = reducer.fit_transform(phenotype_array[keep_flags, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=embedding_full[:, 0], y=embedding_full[:, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svd_model.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
