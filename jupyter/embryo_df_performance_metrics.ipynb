{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    auc\n",
    ")\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.chdir(\"/net/trapnell/vol1/home/mdcolon/proj/fishcaster/\")\n",
    "\n",
    "#this is the function that is used to save the paths, they are used the same way in all functions\n",
    "#change this function so you can save them in the way you want. \n",
    "def results_path(sub_directory):\n",
    "    \"\"\"Creates a directory path with the current date and sub-directory.\"\"\"\n",
    "    today = datetime.now()\n",
    "    month_year = today.strftime(\"%m_%Y\")\n",
    "    date = today.strftime(\"%m_%d_%Y\")\n",
    "    path = os.path.join('results', month_year, date, sub_directory)\n",
    "    os.makedirs(path, exist_ok=True)  # Create directory if not exists\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing two Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Data\n",
    "df_all_path = \"./data/embryo_morph_df.csv\"\n",
    "df_hld_path = \"./data/embryo_stats_no_lmx1b_gdf3_df.csv\"\n",
    "\n",
    "df_all = pd.read_csv(df_all_path)\n",
    "df_hld = pd.read_csv(df_hld_path)\n",
    "\n",
    "# Update 'master_perturbation' values to 'Shh' where it contains 'Shh'\n",
    "for df in [df_all, df_hld]:\n",
    "    df.loc[df[\"master_perturbation\"].str.contains(\"Shh\"), \"master_perturbation\"] = \"Shh\"\n",
    "\n",
    "# Define the comparisons (Multiclass) and obtain coloumns for data\n",
    "pert_comparisons = [\"Wnt-i\", \"TGFB-i\", \"wik\", \"lmx1b\", \"gdf3\"]\n",
    "z_mu_columns = [col for col in df_all.columns if 'z_mu' in col]    \n",
    "z_mu_biological_columns = [col for col in z_mu_columns if \"b\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_embryo_id_column(df):\n",
    "    \"\"\"\n",
    "    Ensures the 'embryo_id' column exists in the dataframe by extracting it from the 'snip_id' column.\n",
    "    If 'embryo_id' already exists, no changes are made.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe that may contain the 'snip_id' and 'embryo_id' columns.\n",
    "    \n",
    "    Returns:\n",
    "    df (pd.DataFrame): The dataframe with the 'embryo_id' column ensured.\n",
    "    \"\"\"\n",
    "    if 'embryo_id' not in df.columns:\n",
    "        try:\n",
    "            df['embryo_id'] = df['snip_id'].str.replace(r'_[^_]*$', '', regex=True)\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\"'snip_id' column not found in the dataframe\") from e\n",
    "    return df\n",
    "\n",
    "def split_train_test(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into training and test sets based on unique 'embryo_id'.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing the 'embryo_id' column.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): The random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    train_df (pd.DataFrame): The training dataframe.\n",
    "    test_df (pd.DataFrame): The test dataframe.\n",
    "    \"\"\"\n",
    "    df = ensure_embryo_id_column(df)\n",
    "    unique_embryo_ids = df[\"embryo_id\"].unique()\n",
    "    train_ids, test_ids = train_test_split(unique_embryo_ids, test_size=test_size, random_state=random_state)\n",
    "    train_df = df[df[\"embryo_id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    test_df = df[df[\"embryo_id\"].isin(test_ids)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, test_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "df_all_train, df_all_test, df_all = split_train_test(df_all)\n",
    "df_hld_train, df_hld_test, df_hld = split_train_test(df_hld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Multiclass Regression, and balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logistic_regression_multiclass(train_df, test_df, z_mu_biological_columns, pert_comparisons, tol=1e-3, balanced=False):\n",
    "    \"\"\"\n",
    "    Perform logistic regression for a multiclass classification problem.\n",
    "\n",
    "    Parameters:\n",
    "    - train_df (pd.DataFrame): Training DataFrame.\n",
    "    - test_df (pd.DataFrame): Test DataFrame.\n",
    "    - z_mu_biological_columns (list): List of feature column names.\n",
    "    - pert_comparisons (list): List of perturbation class names.\n",
    "    - tol (float): Tolerance for stopping criteria. Defaults to 1e-3.\n",
    "    - balanced (bool): Whether to balance the classes in training and test sets. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - y_test (np.ndarray): True labels for the test set.\n",
    "    - y_pred_proba (np.ndarray): Predicted probabilities for the test set.\n",
    "    - log_reg (LogisticRegression): Trained Logistic Regression model.\n",
    "    - train_df (pd.DataFrame): Modified training DataFrame with 'class_num'.\n",
    "    - test_df (pd.DataFrame): Modified test DataFrame with 'class_num'.\n",
    "    \"\"\"\n",
    "    # Create a mapping for the perturbations to integer labels\n",
    "    perturbation_to_label = {pert: int(i) for i, pert in enumerate(pert_comparisons)}\n",
    "\n",
    "    # Add 'class_num' column to both train_df and test_df\n",
    "    train_df['class_num'] = train_df['master_perturbation'].map(perturbation_to_label)\n",
    "    test_df['class_num'] = test_df['master_perturbation'].map(perturbation_to_label)\n",
    "\n",
    "    # Remove any rows where class_num is NaN\n",
    "    train_df = train_df.dropna(subset=['class_num'])\n",
    "    test_df = test_df.dropna(subset=['class_num'])\n",
    "\n",
    "    # After mapping 'class_num' and dropping NaNs\n",
    "    train_df['class_num'] = train_df['class_num'].astype(int)\n",
    "    test_df['class_num'] = test_df['class_num'].astype(int)\n",
    "\n",
    "    # Balance the classes in the training and test sets if balanced=True\n",
    "    if balanced:\n",
    "        # Balance the classes in the training set\n",
    "        train_class_counts = train_df['class_num'].value_counts()\n",
    "        min_train_class_size = train_class_counts.min()\n",
    "\n",
    "        train_df_balanced = train_df.groupby('class_num').apply(\n",
    "            lambda x: x.sample(n=min_train_class_size, random_state=42)\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # Balance the classes in the test set\n",
    "        test_class_counts = test_df['class_num'].value_counts()\n",
    "        min_test_class_size = test_class_counts.min()\n",
    "\n",
    "        test_df_balanced = test_df.groupby('class_num').apply(\n",
    "            lambda x: x.sample(n=min_test_class_size, random_state=42)\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # Update train_df and test_df with balanced data\n",
    "        train_df = train_df_balanced\n",
    "        test_df = test_df_balanced\n",
    "\n",
    "    # Extract features and labels\n",
    "    X_train = train_df[z_mu_biological_columns].values\n",
    "    y_train = train_df['class_num'].values\n",
    "\n",
    "    X_test = test_df[z_mu_biological_columns].values\n",
    "    y_test = test_df['class_num'].values\n",
    "\n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize Logistic Regression for multiclass classification\n",
    "    log_reg = LogisticRegression(\n",
    "        C=10,\n",
    "        l1_ratio=0.2,\n",
    "        penalty='elasticnet',\n",
    "        solver='saga',\n",
    "        max_iter=250,  # Increased max_iter for convergence\n",
    "        multi_class='multinomial',  # Important for multiclass problems\n",
    "        random_state=42,\n",
    "        tol=tol\n",
    "    )\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict probabilities on the test set\n",
    "    y_pred_proba = log_reg.predict_proba(X_test_scaled)\n",
    "\n",
    "    # Return results and modified dataframes\n",
    "    return y_test, y_pred_proba, log_reg, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage with logistic_regression_multiclass (all suffix) ---\n",
    "y_test_all, y_pred_proba_all, log_reg_all, train_df_all, test_df_all = logistic_regression_multiclass(\n",
    "    df_all_train, df_all_test, z_mu_biological_columns, pert_comparisons)\n",
    "\n",
    "# --- Example usage with logistic_regression_multiclass (hld suffix) ---\n",
    "y_test_hld, y_pred_proba_hld, log_reg_hld, train_df_hld, test_df_hld = logistic_regression_multiclass(\n",
    "    df_hld_train, df_hld_test, z_mu_biological_columns, pert_comparisons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall functions and outputs\n",
    "\n",
    "IGNORE THESE FUNCTIONS, we arent using them, I have them here for future purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Compute PR AUC Over Time Bins ---\n",
    "def compute_pr_auc_over_time_bins(test_results_df, cls, num_bins=20, max_hpf=40):\n",
    "    \"\"\"\n",
    "    Compute PR AUC over time bins for a specific class.\n",
    "\n",
    "    Parameters:\n",
    "    - test_results_df (pd.DataFrame): DataFrame containing 'predicted_stage_hpf', 'class_num', and 'y_pred_proba'.\n",
    "    - cls (int): The class number to compute PR-AUC for.\n",
    "    - num_bins (int): Number of time bins to divide the data into.\n",
    "    - max_hpf (float or None): Maximum hpf value to include in the analysis. If None, no filtering is applied.\n",
    "\n",
    "    Returns:\n",
    "    - bin_centers (np.ndarray): The centers of the time bins.\n",
    "    - pr_auc_list (list): The PR-AUC values for each time bin.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter data based on max_hpf\n",
    "    if max_hpf is not None:\n",
    "        data = test_results_df[test_results_df['predicted_stage_hpf'] <= max_hpf]\n",
    "    else:\n",
    "        data = test_results_df\n",
    "\n",
    "    # Define time bins\n",
    "    time_min = data['predicted_stage_hpf'].min()\n",
    "    time_max = data['predicted_stage_hpf'].max()\n",
    "    bins = np.linspace(time_min, time_max, num_bins + 1)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    pr_auc_list = []\n",
    "\n",
    "    # Compute PR-AUC for each time bin\n",
    "    for i in range(num_bins):\n",
    "        # Get the data in the current time bin\n",
    "        bin_mask = (data['predicted_stage_hpf'] >= bins[i]) & (data['predicted_stage_hpf'] < bins[i + 1])\n",
    "        bin_data = data[bin_mask]\n",
    "\n",
    "        if not bin_data.empty:\n",
    "            y_true = bin_data['class_num'] == cls\n",
    "            y_scores = bin_data['y_pred_proba']\n",
    "\n",
    "            # Compute number of positive and negative samples\n",
    "            num_positive = np.sum(y_true)\n",
    "            num_negative = np.sum(~y_true)\n",
    "\n",
    "            # Append NaN if either num_positive or num_negative is zero\n",
    "            if num_positive == 0 or num_negative == 0:\n",
    "                pr_auc_list.append(np.nan)\n",
    "            else:\n",
    "                precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "                pr_auc = auc(recall, precision)\n",
    "                pr_auc_list.append(pr_auc)\n",
    "        else:\n",
    "            pr_auc_list.append(np.nan)\n",
    "\n",
    "    return bin_centers, pr_auc_list\n",
    "\n",
    "def pr_auc_over_time_multiclass(y_test, y_pred_proba, test_df, perturbations, num_bins=20, max_hpf=40):\n",
    "    \"\"\"\n",
    "    Compute PR AUC over time bins for the multiclass problem without plotting.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test (np.ndarray): True class labels.\n",
    "    - y_pred_proba (np.ndarray): Predicted probabilities from the classifier.\n",
    "    - test_df (pd.DataFrame): Test DataFrame containing 'predicted_stage_hpf' and 'class_num'.\n",
    "    - perturbations (list): List of class names corresponding to the classes.\n",
    "    - num_bins (int): Number of time bins to divide the data into.\n",
    "    - max_hpf (float or None): Maximum hpf value to include in the analysis. If None, no filtering is applied.\n",
    "\n",
    "    Returns:\n",
    "    - results_dict (dict): Dictionary containing bin centers and PR-AUC lists for each perturbation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the test results dataframe with predicted probabilities and the true class labels\n",
    "    test_results_df = test_df[['predicted_stage_hpf', 'class_num']].copy()\n",
    "\n",
    "    # Add predicted probabilities to the DataFrame for each class\n",
    "    for i, pert in enumerate(perturbations):\n",
    "        test_results_df[f'y_pred_proba_{pert}'] = y_pred_proba[:, i]\n",
    "\n",
    "    # Dictionary to store results\n",
    "    results_dict = {}\n",
    "\n",
    "    # Compute PR AUC for each perturbation\n",
    "    for i, pert in enumerate(perturbations):\n",
    "        # Prepare data for this perturbation\n",
    "        data = test_results_df[['predicted_stage_hpf', 'class_num', f'y_pred_proba_{pert}']].copy()\n",
    "        data.rename(columns={f'y_pred_proba_{pert}': 'y_pred_proba'}, inplace=True)\n",
    "\n",
    "        # Compute PR AUC over time bins\n",
    "        bin_centers, pr_auc_list = compute_pr_auc_over_time_bins(\n",
    "            data, cls=i, num_bins=num_bins, max_hpf=max_hpf)\n",
    "\n",
    "        # Store results\n",
    "        results_dict[pert] = {\n",
    "            'bin_centers': bin_centers,\n",
    "            'pr_auc_list': pr_auc_list\n",
    "        }\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "def plot_pr_auc_over_time(results_dict, perturbations, dataset_label='', title=\"PR-AUC Over Time for Perturbations\", plot=True, save=False):\n",
    "    \"\"\"\n",
    "    Plot PR AUC over time for the multiclass problem using the results from pr_auc_over_time_multiclass.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dict (dict): Dictionary containing bin centers and PR-AUC lists for each perturbation.\n",
    "    - perturbations (list): List of perturbation names.\n",
    "    - dataset_label (str): Label to differentiate datasets (e.g., 'all', 'hld'). Default is ''.\n",
    "    - title (str): Title of the plot.\n",
    "    - plot (bool): Whether to display the plot. Defaults to True.\n",
    "    - save (bool): Whether to save the plot. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot PR AUC for each perturbation\n",
    "    for pert in perturbations:\n",
    "        bin_centers = results_dict[pert]['bin_centers']\n",
    "        pr_auc_list = results_dict[pert]['pr_auc_list']\n",
    "        plt.plot(bin_centers, pr_auc_list, label=f'{pert} PR AUC', marker='o')\n",
    "\n",
    "    plt.xlabel(\"Time (hpf)\")\n",
    "    plt.ylabel(\"PR-AUC\")\n",
    "    plt.title(f\"{title} ({dataset_label})\" if dataset_label else title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the PR-AUC plot if save=True\n",
    "    if save:\n",
    "        filename = f\"pr_auc_over_time_multiclass_{dataset_label}.png\" if dataset_label else \"pr_auc_over_time_multiclass.png\"\n",
    "        pr_auc_plot_path = os.path.join(results_path(\"multiclass_classification_test\"), filename)\n",
    "        plt.savefig(pr_auc_plot_path)\n",
    "        print(\"PR-AUC plot saved to:\", pr_auc_plot_path)\n",
    "\n",
    "    # Display the plot if plot=True\n",
    "    if plot:\n",
    "        plt.show()\n",
    "\n",
    "    # Close the plot to free memory\n",
    "    plt.close()\n",
    "\n",
    "def plot_pr_auc_comparison(results_dict1, results_dict2, perturbation, labels, title, filename_suffix=''):\n",
    "    \"\"\"\n",
    "    Plot PR AUC over time for a specific perturbation from two different results_dicts.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dict1 (dict): First results dictionary.\n",
    "    - results_dict2 (dict): Second results dictionary.\n",
    "    - perturbation (str): The perturbation to plot.\n",
    "    - labels (tuple): A tuple of labels for the two datasets (e.g., ('All Data', 'Held-out Data')).\n",
    "    - title (str): Title of the plot.\n",
    "    - filename_suffix (str): Suffix to differentiate the filename.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Dataset 1\n",
    "    bin_centers1 = results_dict1[perturbation]['bin_centers']\n",
    "    pr_auc_list1 = results_dict1[perturbation]['pr_auc_list']\n",
    "    plt.plot(bin_centers1, pr_auc_list1, label=f'{labels[0]} - {perturbation}', marker='o')\n",
    "\n",
    "    # Dataset 2\n",
    "    bin_centers2 = results_dict2[perturbation]['bin_centers']\n",
    "    pr_auc_list2 = results_dict2[perturbation]['pr_auc_list']\n",
    "    plt.plot(bin_centers2, pr_auc_list2, label=f'{labels[1]} - {perturbation}', marker='x')\n",
    "\n",
    "    plt.xlabel(\"Time (hpf)\")\n",
    "    plt.ylabel(\"PR-AUC\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the comparison plot\n",
    "    filename = f\"pr_auc_comparison_{perturbation}_{filename_suffix}.png\" if filename_suffix else f\"pr_auc_comparison_{perturbation}.png\"\n",
    "    pr_auc_plot_path = os.path.join(results_path(\"multiclass_classification_test\"), filename)\n",
    "    plt.savefig(pr_auc_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Comparison plot saved to: {pr_auc_plot_path}\")\n",
    "\n",
    "def create_pr_auc_dataframe(results_dict_all, results_dict_hld, perturbations):\n",
    "    \"\"\"\n",
    "    Create a dataframe containing per-bin PR-AUC values for each perturbation and each dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_dict_all (dict): Results dictionary from 'all' dataset.\n",
    "    - results_dict_hld (dict): Results dictionary from 'hld' dataset.\n",
    "    - perturbations (list): List of perturbation names.\n",
    "    \n",
    "    Returns:\n",
    "    - pr_auc_df (pd.DataFrame): Dataframe containing per-bin PR-AUC values.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for pert in perturbations:\n",
    "        # Get bin centers and PR-AUC values for both datasets\n",
    "        bin_centers_all = results_dict_all[pert]['bin_centers']\n",
    "        pr_auc_all = results_dict_all[pert]['pr_auc_list']\n",
    "        \n",
    "        bin_centers_hld = results_dict_hld[pert]['bin_centers']\n",
    "        pr_auc_hld = results_dict_hld[pert]['pr_auc_list']\n",
    "\n",
    "        # Round bin centers to the nearest integer\n",
    "        bin_centers_all_rounded = np.round(bin_centers_all).astype(int)\n",
    "        bin_centers_hld_rounded = np.round(bin_centers_hld).astype(int)\n",
    "\n",
    "        # Ensure that the rounded bin centers match between datasets\n",
    "        if not np.array_equal(bin_centers_all_rounded, bin_centers_hld_rounded):\n",
    "            raise ValueError(f\"Rounded bin centers for perturbation '{pert}' do not match between datasets.\")\n",
    "        \n",
    "        # For each bin, record the PR-AUC values\n",
    "        for i in range(len(bin_centers_all)):\n",
    "            data.append({\n",
    "                'perturbation': pert,\n",
    "                'bin': i + 1,  # Bins numbered from 1 to num_bins\n",
    "                'bin_center': bin_centers_all[i],\n",
    "                'pr_auc_all': pr_auc_all[i],\n",
    "                'pr_auc_hld': pr_auc_hld[i]\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    pr_auc_df = pd.DataFrame(data)\n",
    "    \n",
    "    return pr_auc_df\n",
    "\n",
    "def compute_average_pr_auc(pr_auc_df):\n",
    "    \"\"\"\n",
    "    Compute the average PR-AUC across bins for each perturbation and dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - pr_auc_df (pd.DataFrame): Dataframe containing per-bin PR-AUC values.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_pr_auc_df (pd.DataFrame): Dataframe containing average PR-AUC and differences.\n",
    "    \"\"\"\n",
    "    # Group by perturbation and compute mean PR-AUC, ignoring NaN values\n",
    "    avg_pr_auc = pr_auc_df.groupby('perturbation').agg({\n",
    "        'pr_auc_all': 'mean',\n",
    "        'pr_auc_hld': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate the difference\n",
    "    avg_pr_auc['difference'] = avg_pr_auc['pr_auc_all'] - avg_pr_auc['pr_auc_hld']\n",
    "    \n",
    "    return avg_pr_auc\n",
    "\n",
    "def plot_average_pr_auc_difference(avg_pr_auc_df, plot=True, save=False):\n",
    "    \"\"\"\n",
    "    Plot the differences in average PR-AUC between the two datasets for each perturbation.\n",
    "    \n",
    "    Parameters:\n",
    "    - avg_pr_auc_df (pd.DataFrame): DataFrame containing 'perturbation' and 'difference' columns.\n",
    "    - plot (bool): Whether to display the plot interactively. Defaults to True.\n",
    "    - save (bool): Whether to save the plot to a file. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    perturbations = avg_pr_auc_df['perturbation']\n",
    "    differences = avg_pr_auc_df['difference']\n",
    "    \n",
    "    bars = plt.bar(perturbations, differences, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Perturbation', fontsize=14)\n",
    "    plt.ylabel('Difference in Average PR-AUC (All - Held-out)', fontsize=14)\n",
    "    plt.title('Difference in Average PR-AUC Between Datasets', fontsize=16)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Annotate bars with difference values\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            plt.annotate(f'{height:.2f}',\n",
    "                         xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                         xytext=(0, 3),  # 3 points vertical offset\n",
    "                         textcoords=\"offset points\",\n",
    "                         ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Save the plot if save=True\n",
    "    if save:\n",
    "        filename = \"average_pr_auc_difference.png\"\n",
    "        plot_path = os.path.join(results_path(\"multiclass_classification_test\"), filename)\n",
    "        plt.savefig(plot_path, bbox_inches='tight')\n",
    "        print(\"Average PR-AUC difference plot saved to:\", plot_path)\n",
    "    \n",
    "    # Display the plot if plot=True\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    \n",
    "    # Close the plot to free memory\n",
    "    plt.close()\n",
    "\n",
    "def save_dataframes(pr_auc_df, avg_pr_auc_df):\n",
    "    \"\"\"\n",
    "    Save the dataframes as CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    - pr_auc_df (pd.DataFrame): Dataframe containing per-bin PR-AUC values.\n",
    "    - avg_pr_auc_df (pd.DataFrame): Dataframe containing average PR-AUC and differences.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Define paths\n",
    "    results_dir = results_path(\"multiclass_classification_test\")\n",
    "    per_bin_csv_path = os.path.join(results_dir, \"per_bin_pr_auc_values.csv\")\n",
    "    avg_pr_auc_csv_path = os.path.join(results_dir, \"average_pr_auc_differences.csv\")\n",
    "    \n",
    "    # Save dataframes\n",
    "    pr_auc_df.to_csv(per_bin_csv_path, index=False)\n",
    "    avg_pr_auc_df.to_csv(avg_pr_auc_csv_path, index=False)\n",
    "    \n",
    "    print(\"Per-bin PR-AUC values saved to:\", per_bin_csv_path)\n",
    "    print(\"Average PR-AUC differences saved to:\", avg_pr_auc_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage with logistic_regression_multiclass ---\n",
    "results_dict_all = pr_auc_over_time_multiclass(y_test_all, y_pred_proba_all, test_df_all, pert_comparisons)\n",
    "\n",
    "# Now is available globally in test_df, and you can use it\n",
    "results_dict_hld = pr_auc_over_time_multiclass(y_test_hld, y_pred_proba_hld, test_df_hld, pert_comparisons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_auc_over_time(results_dict_all, pert_comparisons,dataset_label=f'all_test_{pert_comparisons}' , title=\"PR-AUC Over Time for All Data\")\n",
    "\n",
    "plot_pr_auc_over_time(results_dict_hld, pert_comparisons, dataset_label= f'hld_test_{pert_comparisons}', title=\"PR-AUC Over Time for Held-out Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Processing ---\n",
    "# Step 1: Create per-bin PR-AUC dataframe\n",
    "pr_auc_df = create_pr_auc_dataframe(results_dict_all, results_dict_hld, pert_comparisons)\n",
    "\n",
    "# Step 2: Compute average PR-AUC and differences\n",
    "avg_pr_auc_df = compute_average_pr_auc(pr_auc_df)\n",
    "\n",
    "# Step 3: Plot the differences in average PR-AUC\n",
    "plot_average_pr_auc_difference(avg_pr_auc_df)\n",
    "\n",
    "# Step 4: Save dataframes as CSV files\n",
    "# save_dataframes(pr_auc_df, avg_pr_auc_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Scores functions and outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1_score_over_time_bins(data, cls, num_bins=20, max_hpf=40):\n",
    "    \"\"\"\n",
    "    Compute F1 scores over time bins for a specific class.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing 'predicted_stage_hpf', 'y_true', and 'y_pred'.\n",
    "    - num_bins (int): Number of time bins to divide the data into.\n",
    "    - max_hpf (float or None): Maximum hpf value to include in the analysis. If None, no filtering is applied.\n",
    "\n",
    "    Returns:\n",
    "    - bin_centers (np.ndarray): The centers of the time bins.\n",
    "    - f1_score_list (list): The F1 scores for each time bin.\n",
    "    \"\"\"\n",
    "    # Filter data based on max_hpf\n",
    "    if max_hpf is not None:\n",
    "        data = data[data['predicted_stage_hpf'] <= max_hpf]\n",
    "\n",
    "    # Check if data is empty after filtering\n",
    "    if data.empty:\n",
    "        print(\"No data available after filtering by max_hpf.\")\n",
    "        return np.array([]), []\n",
    "\n",
    "    # Define time bins\n",
    "    time_min = data['predicted_stage_hpf'].min()\n",
    "    time_max = data['predicted_stage_hpf'].max()\n",
    "    bins = np.linspace(time_min, time_max, num_bins + 1)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    f1_score_list = []\n",
    "\n",
    "    # Compute F1 score for each time bin\n",
    "    for i in range(num_bins):\n",
    "        # Get the data in the current time bin\n",
    "        bin_mask = (data['predicted_stage_hpf'] >= bins[i]) & (data['predicted_stage_hpf'] < bins[i + 1])\n",
    "        bin_data = data[bin_mask]\n",
    "\n",
    "        if not bin_data.empty:\n",
    "            y_true  = bin_data['class_num'] == cls\n",
    "            y_pred  = bin_data['y_pred']    == cls\n",
    "\n",
    "            # Compute number of positive and negative samples\n",
    "            num_positive = np.sum(y_true)\n",
    "            num_negative = np.sum(~y_true)\n",
    "\n",
    "            # Append NaN if either num_positive or num_negative is zero\n",
    "            if num_positive == 0 or num_negative == 0:\n",
    "                f1_score_list.append(np.nan)\n",
    "            else:\n",
    "                f1 = f1_score(y_true, y_pred)\n",
    "                f1_score_list.append(f1)\n",
    "        else:\n",
    "            f1_score_list.append(np.nan)\n",
    "\n",
    "    return bin_centers, f1_score_list\n",
    "\n",
    "def f1_score_over_time_multiclass(y_test, y_pred_proba, test_df, perturbations, num_bins=20, max_hpf=40):\n",
    "    \"\"\"\n",
    "    Compute F1 scores over time bins for the multiclass problem without plotting.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test (np.ndarray): True class labels.\n",
    "    - y_pred_proba (np.ndarray): Predicted probabilities from the classifier.\n",
    "    - test_df (pd.DataFrame): Test DataFrame containing 'predicted_stage_hpf' and 'class_num'.\n",
    "    - perturbations (list): List of class names corresponding to the classes.\n",
    "    - num_bins (int): Number of time bins to divide the data into.\n",
    "    - max_hpf (float or None): Maximum hpf value to include in the analysis. If None, no filtering is applied.\n",
    "\n",
    "    Returns:\n",
    "    - results_dict (dict): Dictionary containing bin centers and F1 score lists for each perturbation.\n",
    "    \"\"\"\n",
    "    # Prepare the test results dataframe with predicted probabilities and the true class labels\n",
    "    test_results_df = test_df[['predicted_stage_hpf', 'class_num']].copy()\n",
    "\n",
    "    # Add predicted probabilities to the DataFrame for each class\n",
    "    for i, pert in enumerate(perturbations):\n",
    "        test_results_df[f'y_pred_proba_{pert}'] = y_pred_proba[:, i]\n",
    "\n",
    "    # Add predicted class labels (argmax over predicted probabilities)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    test_results_df['y_pred'] = y_pred.astype(int)\n",
    "    test_results_df['y_true'] = y_test.astype(int)\n",
    "\n",
    "    # Dictionary to store results\n",
    "    results_dict = {}\n",
    "\n",
    "    # Compute F1 scores for each perturbation\n",
    "    for i, pert in enumerate(perturbations):\n",
    "        # Prepare data for this perturbation\n",
    "        data = test_results_df[['predicted_stage_hpf', 'class_num', f'y_pred_proba_{pert}', 'y_true', 'y_pred']].copy()\n",
    "        data.rename(columns={f'y_pred_proba_{pert}': 'y_pred_proba'}, inplace=True)\n",
    "\n",
    "        # Compute F1 score over time bins\n",
    "        bin_centers, f1_score_list = compute_f1_score_over_time_bins(\n",
    "            data, cls=i, num_bins=num_bins, max_hpf=max_hpf)\n",
    "\n",
    "        # Store results\n",
    "        results_dict[pert] = {\n",
    "            'bin_centers': bin_centers,\n",
    "            'f1_score_list': f1_score_list\n",
    "        }\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "def plot_f1_score_over_time(results_dict, perturbations, dataset_label='', title=\"F1 Score Over Time for Perturbations\", plot=True, save=False):\n",
    "    \"\"\"\n",
    "    Plot F1 scores over time for the multiclass problem using the results from f1_score_over_time_multiclass.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dict (dict): Dictionary containing bin centers and F1 score lists for each perturbation.\n",
    "    - perturbations (list): List of perturbation names.\n",
    "    - dataset_label (str): Label to differentiate datasets (e.g., 'all', 'hld'). Default is ''.\n",
    "    - title (str): Title of the plot.\n",
    "    - plot (bool): Whether to display the plot. Defaults to True.\n",
    "    - save (bool): Whether to save the plot. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot F1 score for each perturbation\n",
    "    for pert in perturbations:\n",
    "        bin_centers = results_dict[pert]['bin_centers']\n",
    "        f1_score_list = results_dict[pert]['f1_score_list']\n",
    "        plt.plot(bin_centers, f1_score_list, label=f'{pert} F1 Score', marker='o')\n",
    "\n",
    "    plt.xlabel(\"Time (hpf)\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(f\"{title} ({dataset_label})\" if dataset_label else title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the F1 score plot if save=True\n",
    "    if save:\n",
    "        filename = f\"f1_score_over_time_multiclass_{dataset_label}.png\" if dataset_label else \"f1_score_over_time_multiclass.png\"\n",
    "        f1_score_plot_path = os.path.join(results_path(\"multiclass_classification_test\"), filename)\n",
    "        plt.savefig(f1_score_plot_path)\n",
    "        print(\"F1 score plot saved to:\", f1_score_plot_path)\n",
    "\n",
    "    # Display the plot if plot=True\n",
    "    if plot:\n",
    "        plt.show()\n",
    "\n",
    "    # Close the plot to free memory\n",
    "    plt.close()\n",
    "\n",
    "def plot_f1_score_comparison(results_dict1, results_dict2, perturbation, labels, title, filename_suffix=''):\n",
    "    \"\"\"\n",
    "    Plot F1 scores over time for a specific perturbation from two different results_dicts.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dict1 (dict): First results dictionary.\n",
    "    - results_dict2 (dict): Second results dictionary.\n",
    "    - perturbation (str): The perturbation to plot.\n",
    "    - labels (tuple): A tuple of labels for the two datasets (e.g., ('All Data', 'Held-out Data')).\n",
    "    - title (str): Title of the plot.\n",
    "    - filename_suffix (str): Suffix to differentiate the filename.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Dataset 1\n",
    "    bin_centers1 = results_dict1[perturbation]['bin_centers']\n",
    "    f1_score_list1 = results_dict1[perturbation]['f1_score_list']\n",
    "    plt.plot(bin_centers1, f1_score_list1, labels=f'{labels[0]} - {perturbation}', marker='o')\n",
    "\n",
    "    # Dataset 2\n",
    "    bin_centers2 = results_dict2[perturbation]['bin_centers']\n",
    "    f1_score_list2 = results_dict2[perturbation]['f1_score_list']\n",
    "    plt.plot(bin_centers2, f1_score_list2, label=f'{labels[1]} - {perturbation}', marker='x')\n",
    "\n",
    "    plt.xlabel(\"Time (hpf)\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the comparison plot\n",
    "    filename = f\"f1_score_comparison_{perturbation}_{filename_suffix}.png\" if filename_suffix else f\"f1_score_comparison_{perturbation}.png\"\n",
    "    f1_score_plot_path = os.path.join(results_path(\"multiclass_classification_test\"), filename)\n",
    "    plt.savefig(f1_score_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Comparison plot saved to: {f1_score_plot_path}\")\n",
    "\n",
    "def create_f1_score_dataframe(results_dict_all, results_dict_hld, perturbations):\n",
    "    \"\"\"\n",
    "    Create a dataframe containing per-bin F1 scores and their differences for each perturbation and each dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - results_dict_all (dict): Results dictionary from 'all' dataset.\n",
    "    - results_dict_hld (dict): Results dictionary from 'hld' dataset.\n",
    "    - perturbations (list): List of perturbation names.\n",
    "    \n",
    "    Returns:\n",
    "    - f1_score_df (pd.DataFrame): Dataframe containing per-bin F1 scores and differences.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for pert in perturbations:\n",
    "        # Get bin centers and F1 scores for both datasets\n",
    "        bin_centers_all = results_dict_all[pert]['bin_centers']\n",
    "        f1_score_all = results_dict_all[pert]['f1_score_list']\n",
    "        \n",
    "        bin_centers_hld = results_dict_hld[pert]['bin_centers']\n",
    "        f1_score_hld = results_dict_hld[pert]['f1_score_list']\n",
    "\n",
    "        # Round bin centers to the nearest integer\n",
    "        bin_centers_all_rounded = np.round(bin_centers_all).astype(int)\n",
    "        bin_centers_hld_rounded = np.round(bin_centers_hld).astype(int)\n",
    "\n",
    "        # Ensure that the rounded bin centers match between datasets\n",
    "        if not np.array_equal(bin_centers_all_rounded, bin_centers_hld_rounded):\n",
    "            raise ValueError(f\"Bin centers for perturbation '{pert}' do not match between datasets.\")\n",
    "        \n",
    "        # For each bin, record the F1 scores and their difference\n",
    "        for i in range(len(bin_centers_all)):\n",
    "            difference = f1_score_all[i] - f1_score_hld[i]\n",
    "            data.append({\n",
    "                'Perturbation': pert,\n",
    "                'bin': i + 1,  # Bins numbered from 1 to num_bins\n",
    "                'bin_center': bin_centers_all_rounded[i],\n",
    "                'F1_score_all': f1_score_all[i],\n",
    "                'F1_score_hld': f1_score_hld[i],\n",
    "                'F1_all_hld_diff': difference\n",
    "            })\n",
    "\n",
    "    # Create DataFrame\n",
    "    f1_score_df = pd.DataFrame(data)\n",
    "    \n",
    "    return f1_score_df\n",
    "\n",
    "def compute_average_f1_score(f1_score_df):\n",
    "    \"\"\"\n",
    "    Compute the average F1 score across bins for each perturbation and dataset, and calculate the differences.\n",
    "    \n",
    "    Parameters:\n",
    "    - f1_score_df (pd.DataFrame): Dataframe containing per-bin F1 scores.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_f1_score_df (pd.DataFrame): Dataframe containing average F1 scores and differences.\n",
    "    \"\"\"\n",
    "    # Group by perturbation and compute mean F1 score, ignoring NaN values\n",
    "    avg_f1_score = f1_score_df.groupby('Perturbation').agg({\n",
    "        'F1_score_all': 'mean',\n",
    "        'F1_score_hld': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate the difference\n",
    "    avg_f1_score['F1_all_hld_diff'] = avg_f1_score['F1_score_all'] - avg_f1_score['F1_score_hld']\n",
    "    \n",
    "    return avg_f1_score\n",
    "\n",
    "def plot_average_f1_score_difference(avg_f1_score_df, plot=True, save=False):\n",
    "    \"\"\"\n",
    "    Plot the differences in average F1 scores between the two datasets for each perturbation.\n",
    "    \n",
    "    Parameters:\n",
    "    - avg_f1_score_df (pd.DataFrame): DataFrame containing 'perturbation' and 'difference' columns.\n",
    "    - plot (bool): Whether to display the plot interactively. Defaults to True.\n",
    "    - save (bool): Whether to save the plot to a file. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    perturbations = avg_f1_score_df['Perturbation']\n",
    "    differences = avg_f1_score_df['F1_all_hld_diff']\n",
    "    \n",
    "    bars = plt.bar(perturbations, differences, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Perturbation', fontsize=14)\n",
    "    plt.ylabel('Difference in Average F1 Score (All - Held-out)', fontsize=14)\n",
    "    plt.title('Difference in Average F1 Score Between Datasets', fontsize=16)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Annotate bars with difference values\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            plt.annotate(f'{height:.2f}',\n",
    "                         xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                         xytext=(0, 3),  # 3 points vertical offset\n",
    "                         textcoords=\"offset points\",\n",
    "                         ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Save the plot if save=True\n",
    "    if save:\n",
    "        filename = \"average_f1_score_difference.png\"\n",
    "        plot_path = os.path.join(results_path(\"multiclass_classification_test\"), filename)\n",
    "        plt.savefig(plot_path, bbox_inches='tight')\n",
    "        print(\"Average F1 Score difference plot saved to:\", plot_path)\n",
    "    \n",
    "    # Display the plot if plot=True\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    \n",
    "    # Close the plot to free memory\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have y_test, y_pred, test_df, and perturbations defined\n",
    "\n",
    "# Compute F1 scores over time for the multiclass problem\n",
    "\n",
    "##all\n",
    "results_dict_all = f1_score_over_time_multiclass(y_test_all, y_pred_proba_all, test_df_all, pert_comparisons, num_bins=20, max_hpf=40)\n",
    "dataset_label = 'all_perts_F1'\n",
    "plot_f1_score_over_time(results_dict_all, pert_comparisons, dataset_label=dataset_label, title=\"F1 Score Over Time for Perturbations\")\n",
    "\n",
    "#hld\n",
    "results_dict_hld = f1_score_over_time_multiclass(y_test_hld, y_pred_proba_hld, test_df_hld, pert_comparisons, num_bins=20, max_hpf=40)\n",
    "dataset_label = 'hld_gdf3_lmx1b_perts_F1'\n",
    "plot_f1_score_over_time(results_dict_hld, pert_comparisons, dataset_label=dataset_label, title=\"F1 Score Over Time for Perturbations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Processing ---\n",
    "# Step 1: Create per-bin F1 score dataframe\n",
    "f1_score_df = create_f1_score_dataframe(results_dict_all, results_dict_hld, pert_comparisons)\n",
    "\n",
    "# # Step 2: Compute average F1 scores and differences\n",
    "avg_f1_score_df = compute_average_f1_score(f1_score_df)\n",
    "\n",
    "# # Step 3: Plot the differences in average F1 scores\n",
    "plot_average_f1_score_difference(avg_f1_score_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_f1_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "\n",
    "# List of DataFrames and titles for processing\n",
    "dataframes = [\n",
    "    (df_all, 'Original Data'),\n",
    "    (df_hld, 'No lmx1b gdf3')\n",
    "]\n",
    "\n",
    "random_state = 100\n",
    "\n",
    "# List of perturbations to process\n",
    "pert_comparisons\n",
    "\n",
    "# Subsample fraction\n",
    "subsample_fraction = 0.05  # Adjust as needed\n",
    "\n",
    "# Functions to compute metrics\n",
    "def compute_graph_metrics(z_mu_data):\n",
    "    \"\"\"\n",
    "    Compute graph metrics for the given data.\n",
    "    \"\"\"\n",
    "    k_neighbors = 10\n",
    "    knn = NearestNeighbors(n_neighbors=min(k_neighbors, len(z_mu_data)))\n",
    "    knn.fit(z_mu_data)\n",
    "    knn_graph = knn.kneighbors_graph(z_mu_data, mode='connectivity')\n",
    "    G = nx.Graph(knn_graph)\n",
    "\n",
    "    metrics = {}\n",
    "    if nx.is_connected(G):\n",
    "        metrics['avg_path_length'] = nx.average_shortest_path_length(G)\n",
    "    else:\n",
    "        # Compute metrics on the largest connected component\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        subgraph = G.subgraph(largest_cc).copy()\n",
    "        metrics['avg_path_length'] = nx.average_shortest_path_length(subgraph)\n",
    "    metrics['clustering_coeff'] = nx.average_clustering(G)\n",
    "    return metrics\n",
    "\n",
    "def compute_histogram(distances, bins=30):\n",
    "    \"\"\"\n",
    "    Compute histogram counts and bin edges for the given distances.\n",
    "    \"\"\"\n",
    "    counts, bin_edges = np.histogram(distances, bins=bins, density=True)\n",
    "    return counts, bin_edges\n",
    "\n",
    "def compute_kl_divergence(ref_counts, counts):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between the reference histogram counts and current counts.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-10  # To prevent division by zero\n",
    "    P = ref_counts + epsilon\n",
    "    Q = counts + epsilon\n",
    "    P /= np.sum(P)\n",
    "    Q /= np.sum(Q)\n",
    "    KL_div = np.sum(P * np.log(P / Q))\n",
    "    return KL_div\n",
    "\n",
    "def compute_metrics_for_dataframes(dataframes, comparisons, subsample_fraction=0.05, random_state=100):\n",
    "    \"\"\"\n",
    "    Compute metrics for given dataframes and comparisons.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes: List of tuples (DataFrame, title)\n",
    "    - comparisons: List of perturbations to process\n",
    "    - subsample_fraction: Fraction of data to subsample\n",
    "    - random_state: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - metrics_intra_df: DataFrame containing intra-perturbation metrics\n",
    "    - metrics_inter_df: DataFrame containing inter-perturbation metrics\n",
    "    \"\"\"\n",
    "    # Prepare dictionaries to store results\n",
    "    reference_histograms = {'intra': {}, 'inter': {}}\n",
    "    metrics_intra_list = []\n",
    "    metrics_inter_list = []\n",
    "\n",
    "    # Loop over intra- and inter-perturbation distances\n",
    "    for distance_type in ['intra', 'inter']:\n",
    "        # Loop over each DataFrame and its title\n",
    "        for df_idx, (df, df_title) in enumerate(dataframes):\n",
    "            # Sample data for all perturbations\n",
    "            subsampled_data = {}\n",
    "            for perturbation in comparisons:\n",
    "                df_pert = df[df['master_perturbation'] == perturbation].copy()\n",
    "                if df_pert.empty:\n",
    "                    # Skip perturbations not present in the DataFrame\n",
    "                    continue\n",
    "                embryo_ids = df_pert['embryo_id'].unique()\n",
    "\n",
    "                # Initialize a list to store samples from each embryo_id\n",
    "                samples_per_embryo = []\n",
    "\n",
    "                for embryo_id in embryo_ids:\n",
    "                    df_embryo = df_pert[df_pert['embryo_id'] == embryo_id]\n",
    "                    # Ensure at least one data point is sampled from each embryo_id\n",
    "                    n_samples = max(int(len(df_embryo) * subsample_fraction), 1)\n",
    "                    df_embryo_sampled = df_embryo.sample(n=n_samples, random_state=random_state)\n",
    "                    samples_per_embryo.append(df_embryo_sampled)\n",
    "\n",
    "                # Combine samples from all embryo_ids for this perturbation\n",
    "                df_pert_sampled = pd.concat(samples_per_embryo, ignore_index=True)\n",
    "                subsampled_data[perturbation] = df_pert_sampled\n",
    "\n",
    "            # Combine all subsampled data into a global dataset\n",
    "            df_sampled = pd.concat(subsampled_data.values(), ignore_index=True)\n",
    "            z_mu_data_sampled = df_sampled[z_mu_biological_columns].values\n",
    "\n",
    "            # Compute global mean and std for z-scoring\n",
    "            full_distances_sampled = pdist(z_mu_data_sampled, metric='euclidean')\n",
    "            mean_dist = np.mean(full_distances_sampled)\n",
    "            std_dist = np.std(full_distances_sampled)\n",
    "\n",
    "            # Loop over the perturbations to compare\n",
    "            for perturbation in comparisons:\n",
    "                if perturbation not in subsampled_data:\n",
    "                    continue  # Skip if perturbation data is missing\n",
    "\n",
    "                df_pert_sampled = subsampled_data[perturbation]\n",
    "                z_mu_data_pert = df_pert_sampled[z_mu_biological_columns].values\n",
    "\n",
    "                # Ensure there are enough points\n",
    "                if len(z_mu_data_pert) < 2:\n",
    "                    continue\n",
    "\n",
    "                if distance_type == 'intra':\n",
    "                    # Intra-perturbation distances\n",
    "                    distances = pdist(z_mu_data_pert, metric='euclidean')\n",
    "                    z_mu_data_for_metrics = z_mu_data_pert  # For graph metrics\n",
    "                else:\n",
    "                    # Inter-perturbation distances\n",
    "                    # Sample points from other perturbations\n",
    "                    other_perturbations = [p for p in comparisons if p != perturbation and p in subsampled_data]\n",
    "                    if not other_perturbations:\n",
    "                        continue\n",
    "                    df_other = pd.concat([subsampled_data[p] for p in other_perturbations], ignore_index=True)\n",
    "                    z_mu_data_other = df_other[z_mu_biological_columns].values\n",
    "\n",
    "                    # Ensure same number of points\n",
    "                    n_samples = len(z_mu_data_pert)\n",
    "                    if len(z_mu_data_other) > n_samples:\n",
    "                        indices = np.random.choice(len(z_mu_data_other), n_samples, replace=False)\n",
    "                        z_mu_data_other = z_mu_data_other[indices]\n",
    "                    elif len(z_mu_data_other) < 2:\n",
    "                        continue\n",
    "\n",
    "                    # Compute distances between z_mu_data_pert and z_mu_data_other\n",
    "                    distances = cdist(z_mu_data_pert, z_mu_data_other, metric='euclidean').flatten()\n",
    "                    # For graph metrics, combine both datasets\n",
    "                    z_mu_data_for_metrics = np.vstack((z_mu_data_pert, z_mu_data_other))\n",
    "\n",
    "                # Z-score distances\n",
    "                distances_z = (distances - mean_dist) / std_dist\n",
    "\n",
    "                # Compute metrics\n",
    "                metrics = compute_graph_metrics(z_mu_data_for_metrics)\n",
    "\n",
    "                # Compute histogram and KL divergence\n",
    "                # Use the same bin edges across all datasets for consistency\n",
    "                if df_title == 'Original Data':\n",
    "                    # Compute histogram counts and bin edges for the reference dataset\n",
    "                    counts, bin_edges = compute_histogram(distances_z, bins=30)\n",
    "                    # Store the reference histogram\n",
    "                    reference_histograms[distance_type][perturbation] = {'counts': counts, 'bin_edges': bin_edges}\n",
    "                    KL_div = 0.0  # KL divergence is zero for the reference\n",
    "                else:\n",
    "                    # Use the reference histogram's bin edges\n",
    "                    ref_hist = reference_histograms[distance_type].get(perturbation)\n",
    "                    if ref_hist is None:\n",
    "                        continue\n",
    "\n",
    "                    bin_edges = ref_hist['bin_edges']\n",
    "\n",
    "                    # Compute histogram counts using the same bin edges\n",
    "                    counts, _ = np.histogram(distances_z, bins=bin_edges, density=True)\n",
    "\n",
    "                    # Compute KL divergence\n",
    "                    KL_div = compute_kl_divergence(ref_hist['counts'], counts)\n",
    "\n",
    "                # Store metrics in the appropriate list\n",
    "                metrics_entry = {\n",
    "                    'DataFrame': df_title,\n",
    "                    'Perturbation': perturbation,\n",
    "                    'Avg_Path_Length': metrics['avg_path_length'],\n",
    "                    'Clustering_Coeff': metrics['clustering_coeff'],\n",
    "                    'KL_Divergence': KL_div\n",
    "                }\n",
    "\n",
    "                if distance_type == 'intra':\n",
    "                    metrics_intra_list.append(metrics_entry)\n",
    "                else:\n",
    "                    metrics_inter_list.append(metrics_entry)\n",
    "\n",
    "    # Convert the lists of metrics to DataFrames\n",
    "    metrics_intra_df = pd.DataFrame(metrics_intra_list)\n",
    "    metrics_inter_df = pd.DataFrame(metrics_inter_list)\n",
    "\n",
    "    return metrics_intra_df, metrics_inter_df\n",
    "\n",
    "def compute_differences(metrics_intra_df, metrics_inter_df, reference_title, comparison_title):\n",
    "    \"\"\"\n",
    "    Compute the differences between the metrics of the comparison DataFrame and the reference DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics_intra_df: DataFrame containing intra-perturbation metrics\n",
    "    - metrics_inter_df: DataFrame containing inter-perturbation metrics\n",
    "    - reference_title: Title of the reference DataFrame (e.g., 'Original Data')\n",
    "    - comparison_title: Title of the comparison DataFrame (e.g., 'No lmx1b gdf3')\n",
    "\n",
    "    Returns:\n",
    "    - diff_intra: DataFrame containing differences in intra-perturbation metrics\n",
    "    - diff_inter: DataFrame containing differences in inter-perturbation metrics\n",
    "    \"\"\"\n",
    "    # Intra-Perturbation Differences\n",
    "    metrics_intra_reference = metrics_intra_df[metrics_intra_df['DataFrame'] == reference_title]\n",
    "    metrics_intra_comparison = metrics_intra_df[metrics_intra_df['DataFrame'] == comparison_title]\n",
    "\n",
    "    # Merge on 'Perturbation'\n",
    "    merged_intra = pd.merge(\n",
    "        metrics_intra_reference,\n",
    "        metrics_intra_comparison,\n",
    "        on='Perturbation',\n",
    "        suffixes=('_reference', '_comparison')\n",
    "    )\n",
    "\n",
    "    # Compute differences\n",
    "    merged_intra['Avg_Path_Length_Diff'] = merged_intra['Avg_Path_Length_comparison'] - merged_intra['Avg_Path_Length_reference']\n",
    "    merged_intra['Clustering_Coeff_Diff'] = merged_intra['Clustering_Coeff_comparison'] - merged_intra['Clustering_Coeff_reference']\n",
    "    merged_intra['KL_Divergence_Diff'] = merged_intra['KL_Divergence_comparison'] - merged_intra['KL_Divergence_reference']\n",
    "\n",
    "    # Select relevant columns\n",
    "    diff_intra = merged_intra[['Perturbation', 'Avg_Path_Length_Diff', 'Clustering_Coeff_Diff', 'KL_Divergence_Diff']]\n",
    "\n",
    "    # Inter-Perturbation Differences\n",
    "    metrics_inter_reference = metrics_inter_df[metrics_inter_df['DataFrame'] == reference_title]\n",
    "    metrics_inter_comparison = metrics_inter_df[metrics_inter_df['DataFrame'] == comparison_title]\n",
    "\n",
    "    # Merge on 'Perturbation'\n",
    "    merged_inter = pd.merge(\n",
    "        metrics_inter_reference,\n",
    "        metrics_inter_comparison,\n",
    "        on='Perturbation',\n",
    "        suffixes=('_reference', '_comparison')\n",
    "    )\n",
    "\n",
    "    # Compute differences\n",
    "    merged_inter['Avg_Path_Length_Diff'] = merged_inter['Avg_Path_Length_comparison'] - merged_inter['Avg_Path_Length_reference']\n",
    "    merged_inter['Clustering_Coeff_Diff'] = merged_inter['Clustering_Coeff_comparison'] - merged_inter['Clustering_Coeff_reference']\n",
    "    merged_inter['KL_Divergence_Diff'] = merged_inter['KL_Divergence_comparison'] - merged_inter['KL_Divergence_reference']\n",
    "\n",
    "    # Select relevant columns\n",
    "    diff_inter = merged_inter[['Perturbation', 'Avg_Path_Length_Diff', 'Clustering_Coeff_Diff', 'KL_Divergence_Diff']]\n",
    "\n",
    "    return diff_intra, diff_inter\n",
    "\n",
    "def plot_differences_together(diff_df, distance_type, plot=True, save=False):\n",
    "    \"\"\"\n",
    "    Plot differences in metrics together in a grid for a given distance type (Intra or Inter).\n",
    "    \n",
    "    Parameters:\n",
    "    - diff_df (pd.DataFrame): DataFrame containing the differences in metrics (diff_intra or diff_inter).\n",
    "    - distance_type (str): String indicating the type ('Intra' or 'Inter').\n",
    "    - plot (bool): Whether to display the plots interactively. Defaults to True.\n",
    "    - save (bool): Whether to save the plots to files. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Metrics to plot\n",
    "    metric_names = ['Avg_Path_Length_Diff', 'Clustering_Coeff_Diff', 'KL_Divergence_Diff']\n",
    "    metric_labels = {\n",
    "        'Avg_Path_Length_Diff': 'Average Path Length',\n",
    "        'Clustering_Coeff_Diff': 'Clustering Coefficient',\n",
    "        'KL_Divergence_Diff': 'KL Divergence'\n",
    "    }\n",
    "    \n",
    "    num_metrics = len(metric_names)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(6*num_metrics, 6))\n",
    "    \n",
    "    for ax, metric in zip(axes, metric_names):\n",
    "        # Extract relevant data\n",
    "        data = diff_df[['Perturbation', metric]].copy()\n",
    "        data = data.rename(columns={metric: 'Difference'})\n",
    "        \n",
    "        # Create the bar plot with a single color\n",
    "        sns.barplot(\n",
    "            data=data,\n",
    "            x='Perturbation',\n",
    "            y='Difference',\n",
    "            color='skyblue',\n",
    "            edgecolor='black',\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        ax.set_xlabel('Perturbation', fontsize=14)\n",
    "        ax.set_ylabel('Difference', fontsize=14)\n",
    "        ax.set_title(f\"{metric_labels[metric]}\", fontsize=16)  # Condensed title\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Annotate bars with difference values\n",
    "        for index, row in data.iterrows():\n",
    "            ax.text(\n",
    "                index, \n",
    "                row['Difference'], \n",
    "                f\"{row['Difference']:.2f}\", \n",
    "                color='black', \n",
    "                ha=\"center\", \n",
    "                va='bottom', \n",
    "                fontsize=12\n",
    "            )\n",
    "    \n",
    "    # Add a global title\n",
    "    fig.suptitle(f'Differences in Metrics Between Datasets ({distance_type}-Perturbation)', fontsize=18)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to accommodate suptitle\n",
    "    \n",
    "    # Save the plot if save=True\n",
    "    if save:\n",
    "        filename = f\"metrics_difference_{distance_type.lower()}.png\"\n",
    "        plot_path = os.path.join(results_path(\"multiclass_classification_test\"), filename)\n",
    "        plt.savefig(plot_path, bbox_inches='tight')\n",
    "        print(f\"Metrics difference plot saved to:\", plot_path)\n",
    "    \n",
    "    # Display the plot if plot=True\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    \n",
    "    # Close the plot to free memory\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics\n",
    "metrics_intra_df, metrics_inter_df = compute_metrics_for_dataframes(\n",
    "    dataframes=dataframes,\n",
    "    comparisons=pert_comparisons,\n",
    "    subsample_fraction=subsample_fraction,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Compute the differences\n",
    "diff_intra, diff_inter = compute_differences(\n",
    "    metrics_intra_df,\n",
    "    metrics_inter_df,\n",
    "    reference_title='Original Data',\n",
    "    comparison_title='No lmx1b gdf3'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot differences together for intra-perturbation\n",
    "plot_differences_together(diff_intra, 'Intra')\n",
    "\n",
    "# Plot differences together for inter-perturbation\n",
    "plot_differences_together(diff_inter, 'Inter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffix \"_intra\" to columns in metrics_intra_df if they don't already have it\n",
    "metrics_intra_df.columns = [\n",
    "    f\"{col}_intra\" if col not in ['DataFrame', 'Perturbation'] and not col.endswith(\"_intra\") else col\n",
    "    for col in metrics_intra_df.columns\n",
    "]\n",
    "\n",
    "# Add suffix \"_inter\" to columns in metrics_inter_df if they don't already have it\n",
    "metrics_inter_df.columns = [\n",
    "    f\"{col}_inter\" if col not in ['DataFrame', 'Perturbation'] and not col.endswith(\"_inter\") else col\n",
    "    for col in metrics_inter_df.columns\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding suffix to distinguish intra, and inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_intra.columns = [\n",
    "    f\"{col}_intra\" if col not in ['DataFrame', 'Perturbation'] and not col.endswith(\"_intra\") else col\n",
    "    for col in diff_intra.columns\n",
    "]\n",
    "\n",
    "# Add suffix \"_inter\" to columns in metrics_inter_df if they don't already have it\n",
    "diff_inter.columns = [\n",
    "    f\"{col}_inter\" if col not in ['DataFrame', 'Perturbation'] and not col.endswith(\"_inter\") else col\n",
    "    for col in diff_inter.columns\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of neighbors (k) and subsample fraction\n",
    "k_neighbors = 20\n",
    "subsample_fraction = 0.1  # Adjust as needed\n",
    "\n",
    "# List of DataFrames and titles for comparison\n",
    "dataframes = [\n",
    "    (df_hld, 'No lmx1b gdf3')\n",
    "]\n",
    "\n",
    "# Function to randomly subsample\n",
    "def random_subsample(df, fraction, random_state=42):\n",
    "    return df.sample(frac=fraction, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Function to compute Jaccard similarities and return separate DataFrames\n",
    "def compute_jaccard_similarities(orig_df, dataframes, comparisons, z_mu_biological_columns, k_neighbors=20, subsample_fraction=0.1):\n",
    "    # Initialize lists to store results\n",
    "    jaccard_results_global = []\n",
    "    jaccard_results_inter = []\n",
    "    jaccard_results_intra = []\n",
    "    \n",
    "    # Subsample original DataFrame once\n",
    "    orig_subsampled = random_subsample(orig_df, subsample_fraction)\n",
    "    \n",
    "    # Fit NearestNeighbors model on original data (Global Structure)\n",
    "    knn_orig = NearestNeighbors(n_neighbors=100)  # Use a large number to ensure enough neighbors after masking\n",
    "    knn_orig.fit(orig_subsampled[z_mu_biological_columns].values)\n",
    "    \n",
    "    # Loop over each DataFrame in dataframes\n",
    "    for df, df_label in dataframes:\n",
    "        print(f\"Processing DataFrame: {df_label}\")\n",
    "        \n",
    "        # Subsample comparison DataFrame\n",
    "        comp_subsampled = random_subsample(df, subsample_fraction)\n",
    "        \n",
    "        # Fit NearestNeighbors model on comparison data (Global Structure)\n",
    "        knn_comp = NearestNeighbors(n_neighbors=100)  # Use a large number to ensure enough neighbors after masking\n",
    "        knn_comp.fit(comp_subsampled[z_mu_biological_columns].values)\n",
    "        \n",
    "        # Loop over each perturbation\n",
    "        for perturbation in comparisons:\n",
    "            print(f\"  Processing perturbation: {perturbation}\")\n",
    "            \n",
    "            # Get subset of data for the current perturbation\n",
    "            orig_subset = orig_subsampled[orig_subsampled['master_perturbation'] == perturbation]\n",
    "            comp_subset = comp_subsampled[comp_subsampled['master_perturbation'] == perturbation]\n",
    "            \n",
    "            if orig_subset.empty or comp_subset.empty:\n",
    "                print(f\"    No data for perturbation {perturbation}\")\n",
    "                continue\n",
    "            \n",
    "            # Identify common snip_ids in both subsets\n",
    "            common_snip_ids = set(orig_subset['snip_id']).intersection(set(comp_subset['snip_id']))\n",
    "            if not common_snip_ids:\n",
    "                print(f\"    No common snip_ids for perturbation {perturbation}\")\n",
    "                continue\n",
    "            \n",
    "            # Initialize lists to store the similarity scores\n",
    "            jaccard_similarities_all = []\n",
    "            jaccard_similarities_inter = []\n",
    "            jaccard_similarities_intra = []\n",
    "            \n",
    "            # Loop over the common snip_ids to compare nearest neighbors\n",
    "            for snip_id in common_snip_ids:\n",
    "                # Get the data points for this snip_id\n",
    "                point_orig = orig_subset[orig_subset['snip_id'] == snip_id][z_mu_biological_columns].values\n",
    "                point_comp = comp_subset[comp_subset['snip_id'] == snip_id][z_mu_biological_columns].values\n",
    "                \n",
    "                if point_orig.size == 0 or point_comp.size == 0:\n",
    "                    continue  # Skip if no data for this snip_id\n",
    "                \n",
    "                # -- Global Structure --\n",
    "                neighbors_orig_all = knn_orig.kneighbors(point_orig, return_distance=False)[0]\n",
    "                neighbors_comp_all = knn_comp.kneighbors(point_comp, return_distance=False)[0]\n",
    "                \n",
    "                orig_neighbors_snip_ids_all = orig_subsampled.iloc[neighbors_orig_all]['snip_id'].values\n",
    "                comp_neighbors_snip_ids_all = comp_subsampled.iloc[neighbors_comp_all]['snip_id'].values\n",
    "                \n",
    "                # Take the first k_neighbors\n",
    "                orig_neighbors_snip_ids_all = orig_neighbors_snip_ids_all[:k_neighbors]\n",
    "                comp_neighbors_snip_ids_all = comp_neighbors_snip_ids_all[:k_neighbors]\n",
    "                \n",
    "                # Calculate Jaccard similarity using the union of neighbor sets\n",
    "                intersection_all = set(orig_neighbors_snip_ids_all).intersection(set(comp_neighbors_snip_ids_all))\n",
    "                union_all = set(orig_neighbors_snip_ids_all).union(set(comp_neighbors_snip_ids_all))\n",
    "                jaccard_similarity_all = len(intersection_all) / len(union_all)\n",
    "                jaccard_similarities_all.append(jaccard_similarity_all)\n",
    "                \n",
    "                # -- Inter-Class Structure (exclude same perturbation) --\n",
    "                # Filter neighbors to exclude same perturbation\n",
    "                orig_neighbors_inter = orig_subsampled.iloc[neighbors_orig_all]\n",
    "                comp_neighbors_inter = comp_subsampled.iloc[neighbors_comp_all]\n",
    "                \n",
    "                orig_neighbors_snip_ids_inter = orig_neighbors_inter[orig_neighbors_inter['master_perturbation'] != perturbation]['snip_id'].values\n",
    "                comp_neighbors_snip_ids_inter = comp_neighbors_inter[comp_neighbors_inter['master_perturbation'] != perturbation]['snip_id'].values\n",
    "                \n",
    "                # Ensure we have enough neighbors\n",
    "                orig_neighbors_snip_ids_inter = orig_neighbors_snip_ids_inter[:k_neighbors]\n",
    "                comp_neighbors_snip_ids_inter = comp_neighbors_snip_ids_inter[:k_neighbors]\n",
    "                \n",
    "                if len(orig_neighbors_snip_ids_inter) > 0 and len(comp_neighbors_snip_ids_inter) > 0:\n",
    "                    intersection_inter = set(orig_neighbors_snip_ids_inter).intersection(set(comp_neighbors_snip_ids_inter))\n",
    "                    union_inter = set(orig_neighbors_snip_ids_inter).union(set(comp_neighbors_snip_ids_inter))\n",
    "                    jaccard_similarity_inter = len(intersection_inter) / len(union_inter)\n",
    "                    jaccard_similarities_inter.append(jaccard_similarity_inter)\n",
    "                \n",
    "                # -- Intra-Class Structure (only same perturbation) --\n",
    "                # Filter neighbors to include only same perturbation\n",
    "                orig_neighbors_intra = orig_subsampled.iloc[neighbors_orig_all]\n",
    "                comp_neighbors_intra = comp_subsampled.iloc[neighbors_comp_all]\n",
    "                \n",
    "                orig_neighbors_snip_ids_intra = orig_neighbors_intra[orig_neighbors_intra['master_perturbation'] == perturbation]['snip_id'].values\n",
    "                comp_neighbors_snip_ids_intra = comp_neighbors_intra[comp_neighbors_intra['master_perturbation'] == perturbation]['snip_id'].values\n",
    "                \n",
    "                # Ensure we have enough neighbors\n",
    "                orig_neighbors_snip_ids_intra = orig_neighbors_snip_ids_intra[:k_neighbors]\n",
    "                comp_neighbors_snip_ids_intra = comp_neighbors_snip_ids_intra[:k_neighbors]\n",
    "                \n",
    "                if len(orig_neighbors_snip_ids_intra) > 0 and len(comp_neighbors_snip_ids_intra) > 0:\n",
    "                    intersection_intra = set(orig_neighbors_snip_ids_intra).intersection(set(comp_neighbors_snip_ids_intra))\n",
    "                    union_intra = set(orig_neighbors_snip_ids_intra).union(set(comp_neighbors_snip_ids_intra))\n",
    "                    jaccard_similarity_intra = len(intersection_intra) / len(union_intra)\n",
    "                    jaccard_similarities_intra.append(jaccard_similarity_intra)\n",
    "            \n",
    "            # Store the average Jaccard similarities in the respective results lists\n",
    "            if jaccard_similarities_all:\n",
    "                avg_similarity_all = np.mean(jaccard_similarities_all)\n",
    "                jaccard_results_global.append({\n",
    "                    'DataFrame': df_label,\n",
    "                    'Perturbation': perturbation,\n",
    "                    'Jaccard Similarity': avg_similarity_all\n",
    "                })\n",
    "            if jaccard_similarities_inter:\n",
    "                avg_similarity_inter = np.mean(jaccard_similarities_inter)\n",
    "                jaccard_results_inter.append({\n",
    "                    'DataFrame': df_label,\n",
    "                    'Perturbation': perturbation,\n",
    "                    'Jaccard Similarity': avg_similarity_inter\n",
    "                })\n",
    "            if jaccard_similarities_intra:\n",
    "                avg_similarity_intra = np.mean(jaccard_similarities_intra)\n",
    "                jaccard_results_intra.append({\n",
    "                    'DataFrame': df_label,\n",
    "                    'Perturbation': perturbation,\n",
    "                    'Jaccard Similarity': avg_similarity_intra\n",
    "                })\n",
    "    \n",
    "    # Convert results to DataFrames\n",
    "    results_df_global = pd.DataFrame(jaccard_results_global)\n",
    "    results_df_inter = pd.DataFrame(jaccard_results_inter)\n",
    "    results_df_intra = pd.DataFrame(jaccard_results_intra)\n",
    "    \n",
    "    return results_df_global, results_df_inter, results_df_intra\n",
    "\n",
    "# Function to plot the results\n",
    "def plot_jaccard_results(results_df_global, results_df_inter, results_df_intra):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Plotting the bar charts\n",
    "    # Set the seaborn style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Create three subplots that share the same y-axis\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=True)\n",
    "    \n",
    "    # Define the order of the datasets for consistent coloring\n",
    "    dataset_order = results_df_global['DataFrame'].unique()\n",
    "    \n",
    "    # Create a color palette\n",
    "    palette = sns.color_palette('tab10', n_colors=len(dataset_order))\n",
    "    color_dict = dict(zip(dataset_order, palette))\n",
    "    \n",
    "    # Plot for 'Global Structure'\n",
    "    sns.barplot(\n",
    "        data=results_df_global,\n",
    "        x='Perturbation',\n",
    "        y='Jaccard Similarity',\n",
    "        hue='DataFrame',\n",
    "        hue_order=dataset_order,\n",
    "        ax=axes[0],\n",
    "        ci=None,\n",
    "        palette=color_dict\n",
    "    )\n",
    "    axes[0].set_title('Point-Similarity; Global Structure')\n",
    "    axes[0].set_xlabel('Perturbations')\n",
    "    axes[0].set_ylabel('Average Jaccard Similarity')\n",
    "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Remove legend from the first plot if it exists\n",
    "    legend = axes[0].get_legend()\n",
    "    if legend is not None:\n",
    "        legend.remove()\n",
    "    \n",
    "    # Plot for 'Inter-Class Structure'\n",
    "    sns.barplot(\n",
    "        data=results_df_inter,\n",
    "        x='Perturbation',\n",
    "        y='Jaccard Similarity',\n",
    "        hue='DataFrame',\n",
    "        hue_order=dataset_order,\n",
    "        ax=axes[1],\n",
    "        ci=None,\n",
    "        palette=color_dict\n",
    "    )\n",
    "    axes[1].set_title('Point-Similarity; Inter-Class Structure')\n",
    "    axes[1].set_xlabel('Perturbations')\n",
    "    axes[1].set_ylabel('')\n",
    "    axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Remove legend from the second plot if it exists\n",
    "    legend = axes[1].get_legend()\n",
    "    if legend is not None:\n",
    "        legend.remove()\n",
    "    \n",
    "    # Plot for 'Intra-Class Structure'\n",
    "    sns.barplot(\n",
    "        data=results_df_intra,\n",
    "        x='Perturbation',\n",
    "        y='Jaccard Similarity',\n",
    "        hue='DataFrame',\n",
    "        hue_order=dataset_order,\n",
    "        ax=axes[2],\n",
    "        ci=None,\n",
    "        palette=color_dict\n",
    "    )\n",
    "    axes[2].set_title('Point-Similarity; Intra-Class Structure')\n",
    "    axes[2].set_xlabel('Perturbations')\n",
    "    axes[2].set_ylabel('')\n",
    "    axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Remove legend from the third plot if it exists\n",
    "    legend = axes[2].get_legend()\n",
    "    if legend is not None:\n",
    "        legend.remove()\n",
    "    \n",
    "    # Add a single legend for the entire figure\n",
    "    handles, labels = axes[2].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, title='Dataset', loc='upper right')\n",
    "    \n",
    "    # Adjust layout to reduce space between legend and plots\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Adjust the space to accommodate the legend\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage example:\n",
    "\n",
    "# Compute the Jaccard similarities and get the separate results DataFrames\n",
    "results_df_global, results_df_inter, results_df_intra = compute_jaccard_similarities(\n",
    "    orig_df=df_all,\n",
    "    dataframes=dataframes,\n",
    "    comparisons=pert_comparisons,\n",
    "    z_mu_biological_columns=z_mu_biological_columns,\n",
    "    k_neighbors=k_neighbors,\n",
    "    subsample_fraction=subsample_fraction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot_jaccard_results(results_df_global, results_df_inter, results_df_intra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding suffix to distinguish global, intra, and inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add suffix \"_inter\" to columns in metrics_inter_df if they don't already have it\n",
    "results_df_global.columns = [\n",
    "    f\"{col}_global\" if col not in ['DataFrame', 'Perturbation'] and not col.endswith(\"_global\") else col\n",
    "    for col in results_df_global.columns\n",
    "]\n",
    "\n",
    "results_df_intra.columns = [\n",
    "    f\"{col}_intra\" if col not in ['DataFrame', 'Perturbation'] and not col.endswith(\"_intra\") else col\n",
    "    for col in results_df_intra.columns\n",
    "]\n",
    "\n",
    "# Add suffix \"_inter\" to columns in metrics_inter_df if they don't already have it\n",
    "results_df_inter.columns = [\n",
    "    f\"{col}_inter\" if col not in ['DataFrame', 'Perturbation'] and not col.endswith(\"_inter\") else col\n",
    "    for col in results_df_inter.columns\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining and Saving Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we save all of the key metrics, they can all be lined up by the \"Perturbations\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes to merge\n",
    "dfs_to_merge = [avg_f1_score_df, diff_intra, diff_inter, results_df_global, results_df_intra, results_df_inter]\n",
    "\n",
    "# Set 'Perturbation' as the index and drop 'DataFrame' columns\n",
    "dfs_to_merge = [df.set_index('Perturbation').drop(columns='DataFrame', errors='ignore') for df in dfs_to_merge]\n",
    "\n",
    "# Merge all dataframes on the 'Perturbation' index\n",
    "core_performance_metrics = pd.concat(dfs_to_merge, axis=1)\n",
    "\n",
    "# Reset index if needed\n",
    "core_performance_metrics.reset_index(inplace=True)\n",
    "\n",
    "core_performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this has the time \"bin_centers\" so that one can calculate scores over time if ther wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this has the raw distance metric values, not the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'DataFrame' and 'Perturbation' columns from metrics_inter_df\n",
    "metrics_inter_df_dropped = metrics_inter_df.drop(columns=['DataFrame', 'Perturbation'])\n",
    "\n",
    "# Append 'DataFrame' and 'Perturbation' columns from metrics_intra_df to metrics_inter_df_dropped\n",
    "distance_metrics_intra_inter = pd.concat([metrics_intra_df, metrics_inter_df_dropped], axis=1)\n",
    "\n",
    "distance_metrics_intra_inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save these three files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 \n",
    "core_performance_metrics\n",
    "# 2\n",
    "f1_score_df\n",
    "# 3\n",
    "distance_metrics_intra_inter\n",
    "\n",
    "#Note: there are various plotting functions, they have two options Saving, and Plotting, \n",
    "## If you dont want to plot them ignore them\n",
    "## If you want to save the plots, then you need to manipulate the \"results_path\" function so that they are saved to the desired path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zfish_spatial",
   "language": "python",
   "name": "zfish_spatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
