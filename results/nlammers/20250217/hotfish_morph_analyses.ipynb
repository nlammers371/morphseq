{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#### This notebook looks at temperature-dependent changes to embryo morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from glob2 import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embryo_df for our current best model\n",
    "# root = \"/media/nick/hdd02/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/\"\n",
    "\n",
    "root = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/\"\n",
    "train_name = \"20241107_ds\"\n",
    "model_name = \"SeqVAE_z100_ne150_sweep_01_block01_iter030\" \n",
    "train_dir = os.path.join(root, \"training_data\", train_name, \"\")\n",
    "output_dir = os.path.join(train_dir, model_name) \n",
    "\n",
    "# get path to model\n",
    "training_path = sorted(glob(os.path.join(output_dir, \"*\")))[-1]\n",
    "training_name = os.path.dirname(training_path)\n",
    "read_path = os.path.join(training_path, \"figures\", \"\")\n",
    "\n",
    "# path to figures and data\n",
    "fig_path = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/slides/morphseq/20250312/morph_metrics/_archive/\"\n",
    "os.makedirs(fig_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_df = pd.read_csv(read_path + \"embryo_stats_df.csv\", index_col=0)\n",
    "umap_df = pd.read_csv(read_path + \"umap_df.csv\", index_col=0)\n",
    "print(umap_df.shape)\n",
    "umap_df = umap_df.merge(morph_df.loc[:, [\"snip_id\", \"embryo_id\", \"experiment_time\"]], how=\"left\", on=[\"snip_id\"])\n",
    "print(umap_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Make 3D UMAP and PCA for hotfish experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_experiments = np.asarray(['20240813_24hpf', '20240813_30hpf', '20240813_36hpf']) #, '20240813_extras'])\n",
    "hf_morph_df = morph_df.loc[np.isin(morph_df[\"experiment_date\"], HF_experiments), :].reset_index()\n",
    "hf_umap_df = umap_df.loc[np.isin(umap_df[\"experiment_date\"], HF_experiments), :].reset_index()\n",
    "hf_outlier_snips = np.asarray([\"20240813_24hpf_F06_e00_t0000\", \"20240813_36hpf_D03_e00_t0000\", \"20240813_36hpf_C03_e00_t0000\"]) \n",
    "hf_umap_df = hf_umap_df.loc[~np.isin(hf_umap_df[\"snip_id\"], hf_outlier_snips), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make umap scatter\n",
    "fig = px.scatter_3d(hf_umap_df, x=\"UMAP_00_bio_3\", y=\"UMAP_01_bio_3\", z=\"UMAP_02_bio_3\", \n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\", \"snip_id\"})\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_umap.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_umap.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(hf_umap_df, x=\"PCA_00_bio\", y=\"PCA_01_bio\", z=\"PCA_02_bio\", \n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\", \"snip_id\"})\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_pca.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_pca.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Problem: 28C is our control group, but we don't have stage-matching due to stage shifting\n",
    "**Potential solution:** search for reference embryos from timelapse data that closely overlap with 28C, but which also extend out into later timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pert_name = \"wt_ab\" # genotype\n",
    "target_stage = 44 # alive through at least this point\n",
    "start_stage = 18\n",
    "\n",
    "embryo_df = morph_df.loc[:, [\"experiment_date\", \"embryo_id\", \"predicted_stage_hpf\", \"short_pert_name\"]].groupby(\n",
    "                        [\"experiment_date\", \"embryo_id\", \"short_pert_name\"])[\"predicted_stage_hpf\"].agg([\"min\", \"max\"]).reset_index()\n",
    "\n",
    "pert_filter = embryo_df[\"short_pert_name\"] == short_pert_name\n",
    "stage_filter = (embryo_df[\"min\"] <= start_stage) & (embryo_df[\"max\"] >= target_stage)\n",
    "\n",
    "embryo_df = embryo_df.loc[stage_filter & pert_filter, :]\n",
    "# embryo_df.shape\n",
    "\n",
    "ref_umap_df = umap_df.merge(embryo_df.loc[:, [\"embryo_id\"]], how=\"inner\", on=\"embryo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(hf_umap_df, x=\"UMAP_00_bio_3\", y=\"UMAP_01_bio_3\", z=\"UMAP_02_bio_3\", \n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\"})\n",
    "\n",
    "embryo_index = np.unique(ref_umap_df[\"embryo_id\"])\n",
    "for eid in embryo_index:\n",
    "    e_filter = ref_umap_df[\"embryo_id\"]==eid\n",
    "    fig.add_traces(go.Scatter3d(x=ref_umap_df.loc[e_filter, \"UMAP_00_bio_3\"], \n",
    "                                y=ref_umap_df.loc[e_filter, \"UMAP_01_bio_3\"], \n",
    "                                z=ref_umap_df.loc[e_filter, \"UMAP_02_bio_3\"], mode=\"lines\", \n",
    "                                line=dict(color='rgba(0, 0, 0, 0.2)'), showlegend=False ))\n",
    "\n",
    "fig.update_traces(marker=dict(size=4))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_umap_ref.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_umap_ref.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(hf_umap_df, x=\"PCA_00_bio\", y=\"PCA_01_bio\", z=\"PCA_02_bio\", \n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\"})\n",
    "\n",
    "embryo_index = np.unique(ref_umap_df[\"embryo_id\"])\n",
    "for eid in embryo_index:\n",
    "    e_filter = ref_umap_df[\"embryo_id\"]==eid\n",
    "    fig.add_traces(go.Scatter3d(x=ref_umap_df.loc[e_filter, \"PCA_00_bio\"], \n",
    "                                y=ref_umap_df.loc[e_filter, \"PCA_01_bio\"], \n",
    "                                z=ref_umap_df.loc[e_filter, \"PCA_02_bio\"], mode=\"lines\", \n",
    "                                line=dict(color='rgba(0, 0, 0, 0.2)'), showlegend=False ))\n",
    "\n",
    "fig.update_traces(marker=dict(size=4))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_pca_ref.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_pca_ref.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Look at variability by time cohort and temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_early_timepoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Experiment with fitting 3D spline to re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functions.spline_fitting_v2 import LocalPrincipalCurve\n",
    "import time\n",
    "import re \n",
    "from tqdm import tqdm \n",
    "\n",
    "pattern = r\"PCA_.*_bio\"\n",
    "pca_cols = [col for col in ref_umap_df.columns if re.search(pattern, col)]\n",
    "# pca_cols = [col for col in ref_umap_df.columns.tolist() if \"PCA\" in col] #[\"PCA_00_bio\", \"PCA_01_bio\", \"PCA_02_bio\"]\n",
    "bandwidth = .5\n",
    "max_iter = 2500\n",
    "tol = 1e-5\n",
    "angle_penalty_exp = 0.5\n",
    "n_boots = 50\n",
    "boot_size = np.min([ref_umap_df.shape[0], 2500])\n",
    "num_points = 2500\n",
    "\n",
    "# Extract PCA coordinates\n",
    "pert_array = ref_umap_df[pca_cols].values\n",
    "\n",
    "# Compute average early stage point\n",
    "min_time = ref_umap_df[\"predicted_stage_hpf\"].min()\n",
    "early_mask = (ref_umap_df[\"predicted_stage_hpf\"] >= min_time) & \\\n",
    "             (ref_umap_df[\"predicted_stage_hpf\"] < min_time + 2)\n",
    "early_points = ref_umap_df.loc[early_mask, pca_cols].values\n",
    "\n",
    "early_options = np.arange(early_points.shape[0])\n",
    "\n",
    "# Compute average late stage point\n",
    "max_time = ref_umap_df[\"predicted_stage_hpf\"].max()\n",
    "late_mask = (ref_umap_df[\"predicted_stage_hpf\"] >= (max_time - 2))\n",
    "late_points = ref_umap_df.loc[late_mask, pca_cols].values\n",
    "late_options = np.arange(late_points.shape[0])\n",
    "# generate array to store spline fits\n",
    "spline_boot_array = np.zeros((num_points, len(pca_cols), n_boots))\n",
    "\n",
    "# Randomly select a subset of points for fitting\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "for n in tqdm(range(n_boots)):\n",
    "    subset_indices = rng.choice(len(pert_array), size=boot_size, replace=True)\n",
    "    pert_array_subset = pert_array[subset_indices, :]\n",
    "\n",
    "    start_ind = np.random.choice(early_options,1)[0]\n",
    "    stop_ind = np.random.choice(late_options,1)[0]\n",
    "    start_point = early_points[start_ind, :]\n",
    "    stop_point = late_points[stop_ind, :]\n",
    "    \n",
    "    # Fit LocalPrincipalCurve\n",
    "    lpc = LocalPrincipalCurve(\n",
    "        bandwidth=bandwidth,\n",
    "        max_iter=max_iter,\n",
    "        tol=tol,\n",
    "        angle_penalty_exp=angle_penalty_exp\n",
    "    )\n",
    "    \n",
    "    # Fit with the optional start_points/end_point to anchor the spline\n",
    "    lpc.fit(\n",
    "        pert_array_subset,\n",
    "        start_points=start_point[None, :],\n",
    "        end_point=stop_point[None, :],\n",
    "        num_points=num_points\n",
    "    )\n",
    "\n",
    "    spline_boot_array[:, :, n] = lpc.cubic_splines[0]\n",
    "# stop = time.time()\n",
    "\n",
    "\n",
    "# spline_points = None\n",
    "# if len(lpc.cubic_splines) > 0:\n",
    "#     # If your local principal curve class stores the final spline\n",
    "#     spline_points = lpc.cubic_splines[0]\n",
    "# else:\n",
    "#     # If no spline was built, skip\n",
    "#     continue\n",
    "\n",
    "# # Create a temporary DataFrame for the current spline\n",
    "# spline_df = pd.DataFrame(spline_points, columns=[\"PCA_1\", \"PCA_2\", \"PCA_3\"])\n",
    "# spline_df[\"phenotype\"] = pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def compute_tube_points(spline, std_err, num_circle_points=20):\n",
    "    \"\"\"\n",
    "    Given a spline (N x 3) and corresponding radius (N,) at each point,\n",
    "    compute tube mesh coordinates.\n",
    "    \"\"\"\n",
    "    N, d = spline.shape\n",
    "    assert d == 3, \"This function assumes 3D data.\"\n",
    "\n",
    "    # Compute tangent vectors by differentiating the spline\n",
    "    tangents = np.gradient(spline, axis=0)\n",
    "    tangents = np.array([t / np.linalg.norm(t) if np.linalg.norm(t) > 0 else np.array([1,0,0]) for t in tangents])\n",
    "    \n",
    "    # For each tangent, compute two orthogonal vectors:\n",
    "    tube_x, tube_y, tube_z = [], [], []\n",
    "    for i in range(N):\n",
    "        t = tangents[i]\n",
    "        # Find an arbitrary vector not parallel to t\n",
    "        arbitrary = np.array([1, 0, 0]) if abs(t[0]) < 0.9 else np.array([0, 1, 0])\n",
    "        # Compute a vector perpendicular to t\n",
    "        n1 = np.cross(t, arbitrary)\n",
    "        n1 /= np.linalg.norm(n1)\n",
    "        # Compute the second perpendicular vector\n",
    "        n2 = np.cross(t, n1)\n",
    "        n2 /= np.linalg.norm(n2)\n",
    "        \n",
    "        # Build circle points around the spline point:\n",
    "        angles = np.linspace(0, 2*np.pi, num_circle_points, endpoint=False)\n",
    "        for angle in angles:\n",
    "            offset = std_err[i] * (np.cos(angle) * n1 + np.sin(angle) * n2)\n",
    "            tube_x.append(spline[i, 0] + offset[0])\n",
    "            tube_y.append(spline[i, 1] + offset[1])\n",
    "            tube_z.append(spline[i, 2] + offset[2])\n",
    "    \n",
    "    # Reshape to (N, num_circle_points)\n",
    "    tube_x = np.array(tube_x).reshape(N, num_circle_points)\n",
    "    tube_y = np.array(tube_y).reshape(N, num_circle_points)\n",
    "    tube_z = np.array(tube_z).reshape(N, num_circle_points)\n",
    "    \n",
    "    return tube_x, tube_y, tube_z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and se\n",
    "mean_spline = np.mean(spline_boot_array, axis=2)\n",
    "se_spline = np.std(spline_boot_array, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = np.asarray([0, 1, 2])\n",
    "\n",
    "# get se mesh for spline\n",
    "tube_x, tube_y, tube_z = compute_tube_points(mean_spline[:, plot_dims], se_spline[:, plot_dims])\n",
    "\n",
    "se_mesh = go.Mesh3d(\n",
    "    x=tube_x.flatten(),\n",
    "    y=tube_y.flatten(),\n",
    "    z=tube_z.flatten(),\n",
    "    i=[], j=[], k=[],  # You would need to compute triangle indices based on the grid structure\n",
    "    color='lightblue',\n",
    "    opacity=0.2,\n",
    "    name='Uncertainty'\n",
    ")\n",
    "\n",
    "\n",
    "plot_strings = [pca_cols[p] for p in plot_dims]\n",
    "\n",
    "fig = px.scatter_3d(hf_umap_df, x=plot_strings[0], y=plot_strings[1], z=plot_strings[2], opacity=1,\n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\", \"snip_id\"})\n",
    "\n",
    "fig.update_traces(marker=dict(size=5, showscale=False))\n",
    "\n",
    "fig.add_traces(go.Scatter3d(x=mean_spline[:, plot_dims[0]], y=mean_spline[:, plot_dims[1]], \n",
    "                            z=mean_spline[:, plot_dims[2]],\n",
    "                           mode=\"lines\", line=dict(color=\"darkblue\", width=4), name=\"reference curve\"))\n",
    "\n",
    "# fig.add_traces(go.Scatter3d(x=[P2[0]], y=[P2[1]], z=[P2[2]], mode=\"markers\"))\n",
    "\n",
    "# fig.add_traces(se_mesh)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_pca_with_spline.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_pca_with_spline.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We can use this spline to stage embryos. The first step is to calculate elapsed experimental time along each segment. This, plus an estimated starting time will give us a calibration curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_m_distance(ref_data, mean_spline, se_spline):\n",
    "#     n, d = ref_data.shape\n",
    "#     m, _ = mean_spline.shape\n",
    "#     dist_matrix = np.empty((n, m))\n",
    "#     # Compute the Mahalanobis distance for each pair (i, j)\n",
    "#     for j in tqdm(range(m)):\n",
    "#         # Construct the diagonal inverse covariance matrix for spline point j\n",
    "#         # inv_var = 1.0 / (se_spline[j] ** 2)  # shape: (d,)\n",
    "#         se_total = np.sum(se_spline[j, :]**2)\n",
    "#         diff = np.sum((ref_data - mean_spline[j])**2, axis=1)  # shape: (n, d)\n",
    "#         # Compute squared Mahalanobis distances: sum over dimensions of diff**2 * inv_var\n",
    "#         dist_matrix[:, j] = np.sqrt(diff / se_total) #np.sqrt(np.sum(diff**2 * inv_var, axis=1))\n",
    "        \n",
    "#     closest_indices = np.argmin(dist_matrix, axis=1)\n",
    "#     closest_dist = np.min(dist_matrix, axis=1)\n",
    "\n",
    "#     return closest_dist, closest_indices, dist_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Use reference data to get calibration curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "ref_data = ref_umap_df[pca_cols].values\n",
    "ref_dist = distance_matrix(ref_data, mean_spline)\n",
    "sigma = np.sqrt(np.mean(np.sum(se_spline**2, axis=1)))\n",
    "ref_dist_z = ref_dist / sigma\n",
    "ref_dist_z.shape\n",
    "\n",
    "ref_weights = np.exp(-0.5 * ref_dist_z**2)\n",
    "ref_weights[ref_dist_z > (2 * np.sqrt(len(pca_cols)))] = 0\n",
    "\n",
    "# calculate weighted average spline index for each obs\n",
    "knot_i_vec = np.arange(num_points)[None, :]\n",
    "ref_ci_vec = np.argmin(ref_dist, axis=1) #np.divide(np.sum(np.multiply(knot_i_vec, ref_weights), axis=1), np.sum(ref_weights, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_umap_df[\"knot_index\"] = ref_ci_vec\n",
    "# ref_umap_df[\"knot_dist\"] = ref_cd \n",
    "\n",
    "diff_cols = pca_cols + [\"experiment_time\", \"knot_index\"]\n",
    "diff_cols_lb = [col + \"_diff\" for col in diff_cols]\n",
    "for col in diff_cols_lb:\n",
    "    if col in ref_umap_df.columns.tolist():\n",
    "        ref_umap_df = ref_umap_df.drop(labels=col, axis=1)\n",
    "\n",
    "# calculate morphological flux for each embryo\n",
    "ref_umap_df[diff_cols_lb] = ref_umap_df.groupby('embryo_id')[diff_cols].diff()\n",
    "ref_umap_df = ref_umap_df.fillna(method='bfill') \n",
    "# diff_df = diff_df.rename(columns={col: f\"{col}_diff\" for col in pca_cols + [\"experiment_time\"]})\n",
    "# ref_umap_df = ref_umap_df.join(diff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(ref_umap_df[\"knot_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now get average flux for each knot point\n",
    "ref_umap_df[\"dtds\"] = np.divide(ref_umap_df[\"experiment_time_diff\"], ref_umap_df[\"knot_index_diff\"])\n",
    "m_flux_array = np.zeros(mean_spline.shape)\n",
    "t_flux_array = np.zeros((mean_spline.shape[0], 1))\n",
    "pd_time_array = np.zeros((mean_spline.shape[0], 1))\n",
    "# inlier_filter = ref_cd <= (2 * np.sqrt(len(pca_cols)))\n",
    "\n",
    "for t in range(m_flux_array.shape[0]):\n",
    "    knot_indices = np.where(ref_weights[:, t] > 0)[0]\n",
    "    if len(knot_indices) > 0:\n",
    "        # m_flux_array[t, :] = np.mean(ref_umap_df.loc[knot_ref_filter, diff_cols[:-2]], axis=0)\n",
    "        pdt_vals = ref_umap_df.loc[knot_indices, \"predicted_stage_hpf\"]\n",
    "        dt_vals = ref_umap_df.loc[knot_indices, \"experiment_time_diff\"]\n",
    "        ds_vals = ref_umap_df.loc[knot_indices, \"knot_index_diff\"]\n",
    "        # dtds_vals = ref_umap_df.loc[knot_indices, \"dtds\"]\n",
    "        # inf_filter = ~np.isinf(dtds_vals)\n",
    "        wt_vec = ref_weights[knot_indices, t]\n",
    "        dt_avg = np.sum(np.multiply(dt_vals, wt_vec)) / np.sum(wt_vec)\n",
    "        ds_avg = np.sum(np.multiply(ds_vals, wt_vec)) / np.sum(wt_vec)\n",
    "        t_flux_array[t, :] = dt_avg / ds_avg\n",
    "        # t_flux_array[t, :] = np.sum(np.multiply(dtds_vals[inf_filter], wt_vec[inf_filter])) / np.sum(wt_vec[inf_filter])\n",
    "        pd_time_array[t, :] = np.sum(np.multiply(pdt_vals, wt_vec)) / np.sum(wt_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "check_cols = [\"spline_trend_hpf\", \"spline_stage_hpf\", \"spline_flux_hpf\", \"pd_time_hpf\"]\n",
    "for col in check_cols:\n",
    "    if col in ref_umap_df.columns.tolist():\n",
    "        ref_umap_df = ref_umap_df.drop(labels=[col], axis=1)\n",
    "        \n",
    "# calculate predicted stage as a function of knot position\n",
    "start_stage = pd_time_array[0, 0]\n",
    "knot_trend = np.cumsum(t_flux_array) / 3600\n",
    "\n",
    "# fit a linear model to estimate starting stage\n",
    "# stage_mdl = reg = LinearRegression().fit(knot_slope, y) #knot_stage_hpf = start_stage + \n",
    "\n",
    "spline_stage_df = pd.DataFrame(np.arange(num_points), columns=[\"knot_index\"])\n",
    "spline_stage_df[\"spline_trend_hpf\"] = knot_trend\n",
    "spline_stage_df[\"spline_flux_hpf\"] = t_flux_array / 3600\n",
    "spline_stage_df[\"pd_time_hpf\"] = pd_time_array\n",
    "spline_stage_df[pca_cols] = mean_spline\n",
    "\n",
    "# join on fields\n",
    "ref_umap_df = ref_umap_df.merge(spline_stage_df.loc[:, [\"knot_index\", \"spline_trend_hpf\", \"spline_flux_hpf\"]], how=\"left\", on=\"knot_index\")\n",
    "\n",
    "# run regression to get offset\n",
    "reg = LinearRegression().fit(ref_umap_df[\"spline_trend_hpf\"].to_numpy()[:, None], ref_umap_df[\"predicted_stage_hpf\"].to_numpy()[:, None])\n",
    "reg_inv = LinearRegression().fit(ref_umap_df[\"predicted_stage_hpf\"].to_numpy()[:, None], \n",
    "                                 ref_umap_df[\"spline_trend_hpf\"].to_numpy()[:, None] + reg.intercept_)\n",
    "\n",
    "# use offset to get trend\n",
    "spline_stage_df[\"spline_stage_hpf\"] = spline_stage_df[\"spline_trend_hpf\"].copy() + reg.intercept_\n",
    "ref_umap_df[\"spline_stage_hpf\"] = ref_umap_df[\"spline_trend_hpf\"].copy() + reg.intercept_\n",
    "\n",
    "# assign stages to ref embryos (sanity check)\n",
    "fig = px.line(spline_stage_df, x=\"knot_index\", y=\"spline_stage_hpf\")\n",
    "# fig = px.scatter(spline_stage_df, x=\"pd_time_hpf\", y=\"spline_stage_hpf\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "inlier_filter = np.max(ref_weights, axis=1) > 0\n",
    "\n",
    "fig = px.scatter(ref_umap_df, x=\"predicted_stage_hpf\", y=\"spline_stage_hpf\", color=inlier_filter)\n",
    "\n",
    "fig.update_layout(width=800, height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Now use spline to calibrate hotfish embryos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cols = [\"spline_trend_hpf\", \"spline_stage_hpf\", \"spline_flux_hpf\", \"pd_time_hpf\"]\n",
    "for col in check_cols:\n",
    "    if col in hf_umap_df.columns.tolist():\n",
    "        hf_umap_df = ref_umap_df.drop(labels=[col], axis=1)\n",
    "        \n",
    "hf_data = hf_umap_df[pca_cols].values\n",
    "hf_dist = distance_matrix(hf_data, mean_spline)\n",
    "\n",
    "hf_ci_vec = np.argmin(hf_dist, axis=1)\n",
    "hf_umap_df[\"knot_index\"] = hf_ci_vec\n",
    "spl_transfer_cols = [col for col in spline_stage_df.columns.tolist() if col not in pca_cols]\n",
    "hf_umap_df = hf_umap_df.merge(spline_stage_df.loc[:, spl_transfer_cols], how=\"left\", on=\"knot_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_umap_df, x=\"predicted_stage_hpf\", y=\"spline_stage_hpf\", color=\"temperature\")\n",
    "\n",
    "fig.update_layout(width=800, height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Clearly we run into some issues with 35C...they are too far diverged to accurately register\n",
    "what if we try NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nn = 5\n",
    "hf_ref_dist = distance_matrix(hf_data, ref_data)\n",
    "nn_indices = np.argpartition(hf_ref_dist, kth=k_nn, axis=1)[:, :k_nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_stage_vec = ref_umap_df[\"predicted_stage_hpf\"].to_numpy()\n",
    "hf_nn_stage_vec = np.mean(pd_stage_vec[nn_indices], axis=1)\n",
    "hf_umap_df[\"nn_stage_hpf\"] = hf_nn_stage_vec\n",
    "hf_umap_df[\"nn_spline_stage_hpf\"] = reg_inv.predict(hf_nn_stage_vec[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_umap_df, x=\"predicted_stage_hpf\", y=\"nn_spline_stage_hpf\", color=\"temperature\")\n",
    "\n",
    "fig.update_layout(width=800, height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_umap_df, x=\"nn_spline_stage_hpf\", y=\"spline_stage_hpf\", color=\"temperature\")\n",
    "\n",
    "fig.update_layout(width=800, height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Now...let's use stages to estimate phenotypic severity and variability as a function of temperature and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, calculate mean divergence from the WT trajectory\n",
    "# We have to first re-find the most appropriate index, since this will NOT always be closest spline index\n",
    "stage_dist_mat = distance_matrix(hf_umap_df[\"nn_spline_stage_hpf\"].to_numpy()[:, None], \n",
    "                                spline_stage_df[\"spline_stage_hpf\"].to_numpy()[:, None])\n",
    "hf_umap_df[\"spline_index_adjusted\"] = np.argmin(stage_dist_mat, axis=1)[:, None]\n",
    "hf_umap_df[\"wt_spline_dist\"] = hf_dist[np.arange(hf_dist.shape[0]), hf_umap_df[\"spline_index_adjusted\"].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_umap_df[\"timepoint\"] = np.round(hf_umap_df[\"predicted_stage_hpf\"].to_numpy()).astype(int)\n",
    "cohort_dist_df = hf_umap_df.loc[:, [\"timepoint\", \"temperature\", \"wt_spline_dist\"]].groupby(\n",
    "    [\"timepoint\", \"temperature\"]).agg([\"mean\", \"std\"]).reset_index()\n",
    "cohort_dist_df.columns = ['_'.join(map(str, col)).strip() for col in cohort_dist_df.columns.values]\n",
    "cohort_dist_df.head()                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_umap_df, x=\"timepoint\", y=\"wt_spline_dist\", color=\"temperature\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### There is some subtlety here...\n",
    "Quick tangent: what if we fit a surface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define a pipeline that first transforms the input and then fits a linear model.\n",
    "degree = 2  # or any degree you choose\n",
    "model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=degree, include_bias=True)),\n",
    "    ('linear', LinearRegression())\n",
    "])\n",
    "\n",
    "X = ref_umap_df[pca_cols].values\n",
    "y = ref_umap_df[\"predicted_stage_hpf\"].values\n",
    "# Assume X is your (n_samples x N) input array and y is your (n_samples,) target (time).\n",
    "model.fit(X, y)\n",
    "\n",
    "X_new = hf_umap_df[pca_cols].values\n",
    "# You can then use the model to predict or analyze the polynomial surface.\n",
    "hf_surf_predictions = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_umap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_umap_df, x=\"timepoint\", y=hf_surf_predictions, color=\"temperature\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assume X is your (n_samples x N) input data and model is your trained polynomial regression model.\n",
    "N = X.shape[1]\n",
    "\n",
    "# Determine the grid range for the first two dimensions\n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "\n",
    "# Create a grid for the first two dimensions\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Fix remaining dimensions (e.g., at their mean value)\n",
    "fixed_values = np.mean(X[:, 2:], axis=0) if N > 2 else []\n",
    "\n",
    "# Prepare grid data for prediction: reshape xx and yy to columns and append fixed values\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "if N > 2:\n",
    "    # Repeat fixed values for each grid point\n",
    "    fixed_values_repeated = np.tile(fixed_values, (grid_points.shape[0], 1))\n",
    "    grid_points = np.hstack([grid_points, fixed_values_repeated])\n",
    "\n",
    "# Predict the time values for the grid points\n",
    "predictions = model.predict(grid_points).reshape(xx.shape)\n",
    "\n",
    "# Create a Plotly surface plot\n",
    "fig = fig = px.scatter_3d(ref_umap_df, x=\"PCA_00_bio\", y=\"PCA_01_bio\", z=\"predicted_stage_hpf\", \n",
    "                    color=\"predicted_stage_hpf\", hover_data={\"predicted_stage_hpf\", \"experiment_date\"}, \n",
    "                          color_continuous_scale=\"magma\")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.add_traces(go.Surface(x=xx, y=yy, z=predictions, colorscale='magma', opacity=0.8))\n",
    "fig.update_layout(\n",
    "    title='Visualization of the Fitted Polynomial Surface',\n",
    "    scene=dict(\n",
    "        xaxis_title='Dimension 1',\n",
    "        yaxis_title='Dimension 2',\n",
    "        zaxis_title='Predicted Time'\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy import interpolate\n",
    "\n",
    "# hf_umap_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "embryo_i = 94\n",
    "T = hf_umap_df.loc[embryo_i, \"nn_spline_stage_hpf\"]\n",
    "P1 = hf_umap_df.loc[embryo_i, pca_cols].to_numpy().astype(float)\n",
    "spline_i = np.argmin(np.abs(spline_stage_df[\"pd_time_hpf\"] - T))\n",
    "P2 = spline_stage_df.loc[spline_i, pca_cols].to_numpy().astype(float)\n",
    "M = 25\n",
    "# Assume F(D) is your differentiable polynomial function\n",
    "# t is the fixed level set value (F(P1)=F(P2)=t)\n",
    "# P1 and P2 are numpy arrays of shape (N,)\n",
    "# M is the number of segments (M+1 points total)\n",
    "\n",
    "def total_length(points):\n",
    "    # points is a flattened array representing the intermediate points\n",
    "    points = points.reshape(-1, N)\n",
    "    # Prepend P1 and append P2\n",
    "    all_points = np.vstack([P1, points, P2])\n",
    "    # Compute differences between consecutive points\n",
    "    diffs = np.diff(all_points, axis=0)\n",
    "    # Compute Euclidean distances for each segment\n",
    "    distances = np.sqrt(np.sum(diffs**2, axis=1))\n",
    "    return np.sum(distances)\n",
    "\n",
    "def constraint_func(points, model=model):\n",
    "    # For each intermediate point, enforce F(P) - t = 0\n",
    "    points = points.reshape(-1, N)\n",
    "    pd = model.predict(points)\n",
    "    return pd - T\n",
    "\n",
    "# Number of free points\n",
    "num_free = M - 1\n",
    "\n",
    "# Initial guess: linear interpolation between P1 and P2\n",
    "init_points = np.linspace(P1, P2, M+1)[1:-1].flatten()\n",
    "\n",
    "# Define constraints for scipy.optimize.minimize\n",
    "constraints = {'type': 'eq', 'fun': constraint_func}\n",
    "\n",
    "result = minimize(total_length, init_points, constraints=constraints, options={\"maxiter\":2500})\n",
    "\n",
    "# Reshape the result into M-1 points\n",
    "optimal_points = result.x.reshape(-1, N)\n",
    "\n",
    "# Combine with endpoints for the full curve\n",
    "optimal_curve = np.vstack([P1, optimal_points, P2])\n",
    "geodesic_length = total_length(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line_3d(x=optimal_curve[:, 0], y=optimal_curve[:, 1], z=optimal_curve[:, 2])\n",
    "fig.add_trace(go.Scatter3d(x=[P1[0]], y=[P1[1]], z=[P1[2]], mode=\"markers\"))\n",
    "fig.add_trace(go.Scatter3d(x=[P2[0]], y=[P2[1]], z=[P2[2]], mode=\"markers\"))\n",
    "fig.add_traces(go.Scatter3d(x=mean_spline[:, plot_dims[0]], y=mean_spline[:, plot_dims[1]], \n",
    "                            z=mean_spline[:, plot_dims[2]],\n",
    "                           mode=\"lines\", line=dict(color=\"darkblue\", width=4), name=\"reference curve\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((P1-P2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T)\n",
    "model.predict(optimal_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(hf_umap_df[\"snip_id\"]==\"20240813_30hpf_H07_e00_t0000\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_umap_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
