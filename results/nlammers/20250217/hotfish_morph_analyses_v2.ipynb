{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#### This notebook looks at temperature-dependent changes to embryo morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from glob2 import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embryo_df for our current best model\n",
    "# root = \"/media/nick/hdd02/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/\"\n",
    "\n",
    "root = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/\"\n",
    "train_name = \"20241107_ds\"\n",
    "model_name = \"SeqVAE_z100_ne150_sweep_01_block01_iter030\" \n",
    "train_dir = os.path.join(root, \"training_data\", train_name, \"\")\n",
    "output_dir = os.path.join(train_dir, model_name) \n",
    "\n",
    "# get path to model\n",
    "training_path = sorted(glob(os.path.join(output_dir, \"*\")))[-1]\n",
    "training_name = os.path.dirname(training_path)\n",
    "read_path = os.path.join(training_path, \"figures\", \"\")\n",
    "\n",
    "# path to save data\n",
    "out_path = os.path.join(root, \"results\", \"20240303\", \"\")\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# path to figures and data\n",
    "fig_path = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/slides/morphseq/20250312/morph_metrics/\"\n",
    "os.makedirs(fig_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_df = pd.read_csv(read_path + \"embryo_stats_df.csv\", index_col=0)\n",
    "umap_df = pd.read_csv(read_path + \"umap_df.csv\", index_col=0)\n",
    "print(umap_df.shape)\n",
    "umap_df = umap_df.merge(morph_df.loc[:, [\"snip_id\", \"embryo_id\", \"experiment_time\"]], how=\"left\", on=[\"snip_id\"])\n",
    "print(umap_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Make 3D UMAP and PCA for hotfish experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_experiments = np.asarray(['20240813_24hpf', '20240813_30hpf', '20240813_36hpf']) #, '20240813_extras'])\n",
    "hf_morph_df = morph_df.loc[np.isin(morph_df[\"experiment_date\"], HF_experiments), :].reset_index()\n",
    "hf_umap_df = umap_df.loc[np.isin(umap_df[\"experiment_date\"], HF_experiments), :].reset_index()\n",
    "hf_outlier_snips = np.asarray([\"20240813_24hpf_F06_e00_t0000\", \"20240813_36hpf_D03_e00_t0000\", \"20240813_36hpf_C03_e00_t0000\"]) \n",
    "hf_umap_df = hf_umap_df.loc[~np.isin(hf_umap_df[\"snip_id\"], hf_outlier_snips), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make umap scatter\n",
    "fig = px.scatter_3d(hf_umap_df, x=\"UMAP_00_bio_3\", y=\"UMAP_01_bio_3\", z=\"UMAP_02_bio_3\", \n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\", \"snip_id\"})\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_umap.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_umap.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(hf_umap_df, x=\"PCA_00_bio\", y=\"PCA_01_bio\", z=\"PCA_02_bio\", \n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\", \"snip_id\"})\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_pca.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_pca.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Problem: 28C is our control group, but we don't have stage-matching due to stage shifting\n",
    "**Potential solution:** search for reference embryos from timelapse data that closely overlap with 28C, but which also extend out into later timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pert_name = \"wt_ab\" # genotype\n",
    "target_stage = 44 # alive through at least this point\n",
    "start_stage = 18\n",
    "\n",
    "embryo_df = morph_df.loc[:, [\"experiment_date\", \"embryo_id\", \"predicted_stage_hpf\", \"short_pert_name\"]].groupby(\n",
    "                        [\"experiment_date\", \"embryo_id\", \"short_pert_name\"])[\"predicted_stage_hpf\"].agg([\"min\", \"max\"]).reset_index()\n",
    "\n",
    "pert_filter = embryo_df[\"short_pert_name\"] == short_pert_name\n",
    "stage_filter = (embryo_df[\"min\"] <= start_stage) & (embryo_df[\"max\"] >= target_stage)\n",
    "\n",
    "embryo_df = embryo_df.loc[stage_filter & pert_filter, :]\n",
    "# embryo_df.shape\n",
    "\n",
    "ref_umap_df = umap_df.merge(embryo_df.loc[:, [\"embryo_id\"]], how=\"inner\", on=\"embryo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(hf_umap_df, x=\"UMAP_00_bio_3\", y=\"UMAP_01_bio_3\", z=\"UMAP_02_bio_3\", \n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\"})\n",
    "\n",
    "embryo_index = np.unique(ref_umap_df[\"embryo_id\"])\n",
    "for eid in embryo_index:\n",
    "    e_filter = ref_umap_df[\"embryo_id\"]==eid\n",
    "    fig.add_traces(go.Scatter3d(x=ref_umap_df.loc[e_filter, \"UMAP_00_bio_3\"], \n",
    "                                y=ref_umap_df.loc[e_filter, \"UMAP_01_bio_3\"], \n",
    "                                z=ref_umap_df.loc[e_filter, \"UMAP_02_bio_3\"], mode=\"lines\", \n",
    "                                line=dict(color='rgba(0, 0, 0, 0.2)'), showlegend=False ))\n",
    "\n",
    "fig.update_traces(marker=dict(size=4))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_umap_ref.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_umap_ref.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(hf_umap_df, x=\"PCA_00_bio\", y=\"PCA_01_bio\", z=\"PCA_02_bio\", \n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\"})\n",
    "\n",
    "embryo_index = np.unique(ref_umap_df[\"embryo_id\"])\n",
    "for eid in embryo_index:\n",
    "    e_filter = ref_umap_df[\"embryo_id\"]==eid\n",
    "    fig.add_traces(go.Scatter3d(x=ref_umap_df.loc[e_filter, \"PCA_00_bio\"], \n",
    "                                y=ref_umap_df.loc[e_filter, \"PCA_01_bio\"], \n",
    "                                z=ref_umap_df.loc[e_filter, \"PCA_02_bio\"], mode=\"lines\", \n",
    "                                line=dict(color='rgba(0, 0, 0, 0.2)'), showlegend=False ))\n",
    "\n",
    "fig.update_traces(marker=dict(size=4))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_pca_ref.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_pca_ref.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Experiment with fitting 3D spline to re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functions.spline_fitting_v2 import LocalPrincipalCurve\n",
    "import time\n",
    "import re \n",
    "from tqdm import tqdm \n",
    "\n",
    "pattern = r\"PCA_.*_bio\"\n",
    "pca_cols = [col for col in ref_umap_df.columns if re.search(pattern, col)]\n",
    "# pca_cols = [col for col in ref_umap_df.columns.tolist() if \"PCA\" in col] #[\"PCA_00_bio\", \"PCA_01_bio\", \"PCA_02_bio\"]\n",
    "bandwidth = .5\n",
    "max_iter = 2500\n",
    "tol = 1e-5\n",
    "angle_penalty_exp = 0.5\n",
    "n_boots = 50\n",
    "boot_size = np.min([ref_umap_df.shape[0], 2500])\n",
    "num_points = 5000\n",
    "\n",
    "# Extract PCA coordinates\n",
    "pert_array = ref_umap_df[pca_cols].values\n",
    "\n",
    "# Compute average early stage point\n",
    "min_time = ref_umap_df[\"predicted_stage_hpf\"].min()\n",
    "early_mask = (ref_umap_df[\"predicted_stage_hpf\"] >= min_time) & \\\n",
    "             (ref_umap_df[\"predicted_stage_hpf\"] < min_time + 2)\n",
    "early_points = ref_umap_df.loc[early_mask, pca_cols].values\n",
    "\n",
    "early_options = np.arange(early_points.shape[0])\n",
    "\n",
    "# Compute average late stage point\n",
    "max_time = ref_umap_df[\"predicted_stage_hpf\"].max()\n",
    "late_mask = (ref_umap_df[\"predicted_stage_hpf\"] >= (max_time - 2))\n",
    "late_points = ref_umap_df.loc[late_mask, pca_cols].values\n",
    "late_options = np.arange(late_points.shape[0])\n",
    "# generate array to store spline fits\n",
    "spline_boot_array = np.zeros((num_points, len(pca_cols), n_boots))\n",
    "\n",
    "# Randomly select a subset of points for fitting\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "for n in tqdm(range(n_boots)):\n",
    "    subset_indices = rng.choice(len(pert_array), size=boot_size, replace=True)\n",
    "    pert_array_subset = pert_array[subset_indices, :]\n",
    "\n",
    "    start_ind = np.random.choice(early_options,1)[0]\n",
    "    stop_ind = np.random.choice(late_options,1)[0]\n",
    "    start_point = early_points[start_ind, :]\n",
    "    stop_point = late_points[stop_ind, :]\n",
    "    \n",
    "    # Fit LocalPrincipalCurve\n",
    "    lpc = LocalPrincipalCurve(\n",
    "        bandwidth=bandwidth,\n",
    "        max_iter=max_iter,\n",
    "        tol=tol,\n",
    "        angle_penalty_exp=angle_penalty_exp\n",
    "    )\n",
    "    \n",
    "    # Fit with the optional start_points/end_point to anchor the spline\n",
    "    lpc.fit(\n",
    "        pert_array_subset,\n",
    "        start_points=start_point[None, :],\n",
    "        end_point=stop_point[None, :],\n",
    "        num_points=num_points\n",
    "    )\n",
    "\n",
    "    spline_boot_array[:, :, n] = lpc.cubic_splines[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and se\n",
    "mean_spline = np.mean(spline_boot_array, axis=2)\n",
    "se_spline = np.std(spline_boot_array, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = np.asarray([0, 1, 2])\n",
    "\n",
    "# get se mesh for spline\n",
    "# tube_x, tube_y, tube_z = compute_tube_points(mean_spline[:, plot_dims], se_spline[:, plot_dims])\n",
    "\n",
    "# se_mesh = go.Mesh3d(\n",
    "#     x=tube_x.flatten(),\n",
    "#     y=tube_y.flatten(),\n",
    "#     z=tube_z.flatten(),\n",
    "#     i=[], j=[], k=[],  # You would need to compute triangle indices based on the grid structure\n",
    "#     color='lightblue',\n",
    "#     opacity=0.2,\n",
    "#     name='Uncertainty'\n",
    "# )\n",
    "\n",
    "\n",
    "plot_strings = [pca_cols[p] for p in plot_dims]\n",
    "\n",
    "fig = px.scatter_3d(hf_umap_df, x=plot_strings[0], y=plot_strings[1], z=plot_strings[2], opacity=1,\n",
    "                    color=\"temperature\", hover_data={\"predicted_stage_hpf\", \"experiment_date\", \"snip_id\"})\n",
    "\n",
    "fig.update_traces(marker=dict(size=5, showscale=False))\n",
    "\n",
    "fig.add_traces(go.Scatter3d(x=mean_spline[:, plot_dims[0]], y=mean_spline[:, plot_dims[1]], \n",
    "                            z=mean_spline[:, plot_dims[2]],\n",
    "                           mode=\"lines\", line=dict(color=\"darkblue\", width=4), name=\"reference curve\"))\n",
    "\n",
    "# fig.add_traces(go.Scatter3d(x=[P2[0]], y=[P2[1]], z=[P2[2]], mode=\"markers\"))\n",
    "\n",
    "# fig.add_traces(se_mesh)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"hotfish_pca_with_spline.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"hotfish_pca_with_spline.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Next, fit a polynomial surface to estimate embryo stages\n",
    "Let's experiment with fitting derivatives so we can utilize experimental clock time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define a pipeline that first transforms the input and then fits a linear model.\n",
    "degree = 2  # or any degree you choose\n",
    "model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=degree, include_bias=True)),\n",
    "    ('linear', LinearRegression())\n",
    "])\n",
    "\n",
    "frac_to_fit = 0.8\n",
    "X = ref_umap_df[pca_cols].values\n",
    "n_train = int(np.floor(frac_to_fit * X.shape[0]))\n",
    "X_indices = np.arange(X.shape[0])\n",
    "train_indices = np.random.choice(X_indices, n_train, replace=False)\n",
    "test_indices = X_indices[~np.isin(X_indices, train_indices)]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "\n",
    "y = ref_umap_df[\"predicted_stage_hpf\"].values\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "# Assume X is your (n_samples x N) input array and y is your (n_samples,) target (time).\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pd = model.predict(X_test)\n",
    "\n",
    "fig = px.scatter(x=y_test, y=y_pd)\n",
    "fig.show()\n",
    "# X_new = hf_umap_df[pca_cols].values\n",
    "# # You can then use the model to predict or analyze the polynomial surface.\n",
    "# hf_surf_predictions = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for hotfish data\n",
    "\n",
    "X_hf = hf_umap_df[pca_cols].values\n",
    "# You can then use the model to predict or analyze the polynomial surface.\n",
    "hf_stage_predictions = model.predict(X_hf)\n",
    "\n",
    "hf_umap_df[\"timepoint\"] = np.round(hf_umap_df[\"predicted_stage_hpf\"].to_numpy()).astype(int)\n",
    "hf_umap_df[\"mdl_stage_hpf\"] = hf_stage_predictions\n",
    "fig = px.scatter(hf_umap_df, x=\"timepoint\", y=hf_stage_predictions, color=\"temperature\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get time values for the WT spline \n",
    "spline_stage_pd = model.predict(mean_spline)\n",
    "spline_stage_df = pd.DataFrame(np.arange(num_points), columns=[\"knot_index\"])\n",
    "spline_stage_df[\"pd_stage_hpf\"] = spline_stage_pd\n",
    "spline_stage_df[pca_cols] = mean_spline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Can we visualize the developmenta \"surface\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.interpolate import griddata\n",
    "\n",
    "# # Create a grid over the domain of your data.\n",
    "# X0 = ref_umap_df[pca_cols].to_numpy()\n",
    "# z=-model.predict(X0) \n",
    "\n",
    "# # grid_x = np.linspace(x.min(), x.max(), 100)\n",
    "# # grid_y = np.linspace(y.min(), y.max(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2)\n",
    "\n",
    "# Compute the embedding\n",
    "umap_model.fit(ref_umap_df[pca_cols].values)\n",
    "embedding = umap_model.transform(ref_umap_df[pca_cols].values)\n",
    "hf_embedding = umap_model.transform(hf_umap_df[pca_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "# Create a grid over the domain of your data.\n",
    "x=embedding[:, 0]\n",
    "y=embedding[:, 1]\n",
    "z=-model.predict(X) / 60\n",
    "\n",
    "# fig = px.scatter_3d(x=x, y=y, z=z, color=z)\n",
    "# fig.show()\n",
    "grid_x = np.linspace(0.9*x.min(), 1.1*x.max(), 100)\n",
    "grid_y = np.linspace(0.9*y.min(), 1.1*y.max(), 100)\n",
    "grid_x, grid_y = np.meshgrid(grid_x, grid_y)\n",
    "\n",
    "xy_long = np.c_[grid_x.ravel()[:, None], grid_y.ravel()[:, None]]\n",
    "dist_vec = np.min(distance_matrix(xy_long, embedding), axis=1)\n",
    "# Interpolate the scattered data onto the grid.\n",
    "# grid_z = griddata(points=(x, y), values=z, xi=(grid_x, grid_y), method='cubic')\n",
    "\n",
    "# grid_x.shape\n",
    "# Create the surface plot.\n",
    "# fig = go.Figure(data=[go.Surface(z=grid_z, x=grid_x, y=grid_y)])\n",
    "# fig.update_layout(title=\"3D Surface from Scattered Data\", scene=dict(\n",
    "#                     xaxis_title='X', yaxis_title='Y', zaxis_title='Z'))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "# px.histogram(dist_vec)\n",
    "dist_thresh = 1.5\n",
    "dist_mat = dist_vec.reshape(100, 100)\n",
    "grid_z = griddata(points=(x, y), values=z, xi=(grid_x, grid_y), method='nearest')\n",
    "grid_z_smoothed = gaussian_filter(grid_z, sigma=2, mode=\"nearest\")\n",
    "grid_z_smoothed[dist_mat>dist_thresh] = np.nan\n",
    "\n",
    "hf_umap_df[\"mdl_stage_plot\"] = -hf_umap_df[\"mdl_stage_hpf\"].copy() / 60\n",
    "# Create the surface plot.\n",
    "fig = px.scatter_3d(x=hf_embedding[:, 0], y=hf_embedding[:, 1], z=hf_umap_df[\"mdl_stage_plot\"], color=hf_umap_df[\"temperature\"])\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "\n",
    "fig.add_trace(go.Surface(z=grid_z_smoothed, x=grid_x, y=grid_y, opacity=0.5))\n",
    "fig.update_layout(title=\"3D Surface from Scattered Data\", scene=dict(\n",
    "                    xaxis_title='X', yaxis_title='Y', zaxis_title='Z'))\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"ab_developmental_surface.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"ab_developmental_surface.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Calculate mean and standard deviation in embryo morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_umap_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_cohort_df = hf_umap_df.loc[:, [\"timepoint\", \"temperature\", \"mdl_stage_hpf\"] + pca_cols].groupby(\n",
    "                    [\"timepoint\", \"temperature\"]).agg([\"mean\", \"std\"]).reset_index()\n",
    "hf_cohort_df.columns.values\n",
    "hf_cohort_df.columns = ['_'.join(map(str, col)).strip() for col in hf_cohort_df.columns.values]\n",
    "hf_cohort_df.head()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims = np.asarray([0, 1, 2])\n",
    "mean_pca_cols = [col +\"_mean\" for col in pca_cols]\n",
    "plot_strings = [mean_pca_cols[p] for p in plot_dims]\n",
    "\n",
    "fig = px.scatter_3d(hf_cohort_df, x=plot_strings[0], y=plot_strings[1], z=plot_strings[2], opacity=1,\n",
    "                    color=\"temperature_\", hover_data={\"timepoint_\"})\n",
    "\n",
    "fig.update_traces(marker=dict(size=5, showscale=False))\n",
    "\n",
    "fig.add_traces(go.Scatter3d(x=mean_spline[:, plot_dims[0]], y=mean_spline[:, plot_dims[1]], \n",
    "                            z=mean_spline[:, plot_dims[2]],\n",
    "                           mode=\"lines\", line=dict(color=\"darkblue\", width=4), name=\"reference curve\"))\n",
    "\n",
    "# fig.add_traces(go.Scatter3d(x=[P2[0]], y=[P2[1]], z=[P2[2]], mode=\"markers\"))\n",
    "\n",
    "# fig.add_traces(se_mesh)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"avg_hotfish_pca_with_spline.png\"))\n",
    "fig.write_html(os.path.join(fig_path, \"avg_hotfish_pca_with_spline.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Use JAX to generate predicted developmental gradients at each point in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def make_jax_functions(model):\n",
    "\n",
    "    # Extract the PolynomialFeatures transformer and LinearRegression estimator.\n",
    "    poly = model.named_steps['poly']\n",
    "    linear = model.named_steps['linear']\n",
    "    \n",
    "    # Extract the exponents for each polynomial term. This is an (m x d) array.\n",
    "    powers = jnp.array(poly.powers_)\n",
    "    \n",
    "    # Extract the coefficients and intercept from the linear model.\n",
    "    theta = jnp.array(linear.coef_)\n",
    "    intercept = jnp.array(linear.intercept_)\n",
    "    \n",
    "    # def predict(x, theta, intercept):\n",
    "    #     \"\"\"\n",
    "    #     Computes predictions for a batch of inputs.\n",
    "    #     x: (n_samples x d) input array.\n",
    "    #     Returns an array of shape (n_samples,) with the model's predictions.\n",
    "    #     \"\"\"\n",
    "    #     # Compute polynomial features: for each sample, raise the input to each power\n",
    "    #     # and take the product across features. The result is an (n_samples x m) array.\n",
    "    #     poly_features = jnp.prod(jnp.power(x[:, None, :], powers), axis=2)\n",
    "    #     return jnp.dot(poly_features, theta) + intercept\n",
    "\n",
    "    # def loss_fn(params):\n",
    "    #     \"\"\"\n",
    "    #     Computes the mean-squared error loss on the dataset (X, y) given model parameters.\n",
    "    #     params: tuple (theta, intercept)\n",
    "    #     \"\"\"\n",
    "    #     preds = predict(X, params[0], params[1])\n",
    "    #     return jnp.mean((preds - y) ** 2)\n",
    "    \n",
    "    def predict_single(x, theta, intercept):\n",
    "        \"\"\"\n",
    "        A helper function that computes the prediction for a single input sample.\n",
    "        x: (d,) array.\n",
    "        Returns a scalar prediction.\n",
    "        \"\"\"\n",
    "        # For a single sample, x has shape (d,). \n",
    "        # The polynomial features are computed by raising x to each power in 'powers' \n",
    "        # (which has shape (m, d)) and taking the product over the d features.\n",
    "        poly_features = jnp.prod(jnp.power(x, powers), axis=1)\n",
    "        return jnp.dot(poly_features, theta) + intercept\n",
    "\n",
    "    def predict_and_grad(params, X_new):\n",
    "        \"\"\"\n",
    "        Given parameters (theta, intercept) and a new set of input data X_new,\n",
    "        returns:\n",
    "          - preds: the predictions for each input in X_new,\n",
    "          - grads: the gradient of the scalar prediction function with respect to the input,\n",
    "                   evaluated at each sample in X_new.\n",
    "        \"\"\"\n",
    "        # Define a function of a single sample.\n",
    "        f = lambda x: predict_single(x, params[0], params[1])\n",
    "        # Compute the gradient of f with respect to the input x.\n",
    "        grad_f = jax.grad(f)\n",
    "        # Vectorize both the function and its gradient over the batch dimension.\n",
    "        preds = jax.vmap(f)(X_new)\n",
    "        grads = jax.vmap(grad_f)(X_new)\n",
    "        return preds, grads\n",
    "\n",
    "    return predict_and_grad, (theta, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Calculate stage and morphological deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stage shift\n",
    "hf_cohort_df[\"stage_hpf_mean\"] = model.predict(hf_cohort_df[mean_pca_cols].values)\n",
    "hf_cohort_df[\"stage_shift_hpf\"] = hf_cohort_df[\"stage_hpf_mean\"] - hf_cohort_df[\"timepoint_\"]\n",
    "\n",
    "predict_and_grad, params = make_jax_functions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "sd_pca_cols = [col +\"_std\" for col in pca_cols]\n",
    "# Get morphological shift \n",
    "# Assume distances are small enough that they can be linearized\n",
    "# stage_dist_mat = distance_matrix(hf_cohort_df[\"stage_hpf_mean\"].values[:, None], spline_stage_df[\"pd_stage_hpf\"].values[:, None])\n",
    "stage_dist_mat = distance_matrix(hf_cohort_df[mean_pca_cols], spline_stage_df[pca_cols])\n",
    "hf_cohort_df[\"knot_index\"] = np.argmin(stage_dist_mat, axis=1)\n",
    "for row in tqdm(range(hf_cohort_df.shape[0])):\n",
    "    pca_obs = hf_cohort_df.loc[row, mean_pca_cols].to_numpy() # morph mean\n",
    "    pca_obs_var = hf_cohort_df.loc[row, sd_pca_cols].to_numpy()**2 # morph std\n",
    "    knot_i = hf_cohort_df.loc[row, \"knot_index\"]\n",
    "    pca_ref = spline_stage_df.loc[spline_stage_df[\"knot_index\"]==knot_i, pca_cols].to_numpy() # stage-matched comparison\n",
    "\n",
    "    # get phenotypic distance\n",
    "    hf_cohort_df.loc[row, \"morph_shift\"] = np.sqrt(np.sum((pca_obs - pca_ref)**2))\n",
    "\n",
    "    # record total variance\n",
    "    hf_cohort_df.loc[row, \"total_variance\"] = np.sum(pca_obs_var)\n",
    "\n",
    "    # use gradient to decompose variance\n",
    "    stage_pd, grad_pd = predict_and_grad(params, pca_obs[None, :])\n",
    "    grad_u = np.asarray(grad_pd / np.sqrt(np.sum(grad_pd**2)))[0]\n",
    "    var_null = 0\n",
    "    for n in range(100):\n",
    "        rand_u = np.random.permutation(grad_u.copy())\n",
    "        var_null += np.dot(rand_u, pca_obs_var)\n",
    "\n",
    "    hf_cohort_df.loc[row, \"stage_variance\"] = np.dot(grad_u, pca_obs_var)\n",
    "    \n",
    "    hf_cohort_df.loc[row, \"stage_variance_null\"] = var_null/100\n",
    "    \n",
    "    hf_cohort_df.loc[row, \"morph_variance\"] = hf_cohort_df.loc[row, \"total_variance\"] - hf_cohort_df.loc[row, \"stage_variance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.permutation(grad_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_cohort_df, x=\"timepoint_\", y=\"morph_shift\", color=\"temperature_\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_cohort_df, x=\"timepoint_\", y=\"stage_shift_hpf\", color=\"temperature_\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_cohort_df, x=\"timepoint_\", y=\"morph_variance\", color=\"temperature_\")\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_cohort_df, x=\"timepoint_\", y=\"stage_variance\", color=\"temperature_\")\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_cohort_df, x=\"timepoint_\", y=\"total_variance\", color=\"temperature_\")\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_cohort_df, x=\"morph_variance\", y=\"stage_variance\", color=\"temperature_\", symbol=\"timepoint_\")\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.update_layout(\n",
    "            height=800,\n",
    "            width=800,\n",
    "            xaxis=dict(range=[0, 1.7]), \n",
    "            yaxis=dict(range=[0, 1.7])\n",
    "        )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_cohort_df, x=\"stage_variance_null\", y=\"stage_variance\", color=\"temperature_\", symbol=\"timepoint_\")\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.update_layout(\n",
    "            height=800,\n",
    "            width=800,\n",
    "            xaxis=dict(range=[0, 0.5]), \n",
    "            yaxis=dict(range=[0, 0.5])\n",
    "        )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(hf_cohort_df, x=\"mdl_stage_hpf_std\", y=\"stage_variance\", color=\"temperature_\", symbol=\"timepoint_\")\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.update_layout(\n",
    "            height=800,\n",
    "            width=800,\n",
    "            # xaxis=dict(range=[0, 0.5]), \n",
    "            # yaxis=dict(range=[0, 0.5])\n",
    "        )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Make figure showing images for sanity check purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "\n",
    "image_path = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/training_data/20241107_ds/images/0/\"\n",
    "hf_snip_vec = hf_umap_df[\"snip_id\"].to_numpy()\n",
    "hf_time_vec = hf_umap_df[\"timepoint\"].to_numpy()\n",
    "hf_temp_vec = hf_umap_df[\"temperature\"].to_numpy()\n",
    "image_list = []\n",
    "for snip_id in hf_snip_vec:\n",
    "    im = io.imread(os.path.join(image_path, snip_id + \".jpg\"))\n",
    "    image_list.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "image_path = os.path.join(fig_path, \"cohort_images\", \"\")\n",
    "os.makedirs(image_path, exist_ok=True)\n",
    "im_shape = image_list[0].shape \n",
    "\n",
    "for time in np.unique(hf_time_vec):\n",
    "    for temp in np.unique(hf_temp_vec):\n",
    "        obs_indices = np.where((hf_time_vec==time) & (hf_temp_vec==temp))[0]\n",
    "        \n",
    "        # fig = go.Figure() # make_subplots(rows=2, cols=4)\n",
    "        \n",
    "        # Add each image to a subplot\n",
    "        top_list = []\n",
    "        bottom_list = []\n",
    "        for i in range(8):\n",
    "            if len(obs_indices) > i:\n",
    "                im = image_list[obs_indices[i]]\n",
    "            else:\n",
    "                im = np.zeros(im_shape, dtype=np.uint8)\n",
    "                \n",
    "            if i < 4:\n",
    "                top_list.append(im)\n",
    "            else:\n",
    "                bottom_list.append(im)\n",
    "\n",
    "        tiled_image = np.block([top_list,\n",
    "                                bottom_list])\n",
    "        \n",
    "        fig = px.imshow(tiled_image, color_continuous_scale=\"gray\", title=f\"{temp:02}C @{time:02}hpf\")\n",
    "\n",
    "        \n",
    "        # Update layout for better display\n",
    "        # fig.update_layout(\n",
    "        #     height=600,\n",
    "        #     width=1200,\n",
    "        #     title_text=\"Multiple Images in Plotly\"\n",
    "        # )\n",
    "        \n",
    "        fig.write_image(image_path + f\"embryo_images_tp{time:02}_temp{temp:02}.png\", engine=\"kaleido\")\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Is it possible to fit to the derivatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get point-over-point differences\n",
    "cols_to_diff = pca_cols + [\"experiment_time\"]\n",
    "diff_cols = [col + \"_diff\" for col in cols_to_diff]\n",
    "dt_cols = [col + \"_dt\" for col in cols_to_diff]\n",
    "ref_umap_df_dt = ref_umap_df.copy()\n",
    "ref_umap_df_dt[diff_cols] = ref_umap_df_dt.groupby('embryo_id')[cols_to_diff].diff()\n",
    "ref_umap_df_dt = ref_umap_df_dt.fillna(method='bfill') \n",
    "\n",
    "# we want to calculate the rate of time changes wrpt \n",
    "ref_umap_df_dt[dt_cols[:-1]] = np.divide(ref_umap_df_dt[diff_cols[-1]].values[:, None], ref_umap_df_dt[diff_cols[:-1]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have K measurement points in an N-dimensional space.\n",
    "# D_data: (K, N) array of points.\n",
    "# G_data: (K, N) array of measured gradients at those points.\n",
    "# d: polynomial degree\n",
    "\n",
    "def multiindex_list(N, d):\n",
    "    # Generate list of multi-indices (tuples) for N dimensions up to degree d.\n",
    "    # This is a helper function; many implementations exist.\n",
    "    indices = []\n",
    "    def rec(current, start, remaining):\n",
    "        if remaining == 0:\n",
    "            indices.append(tuple(current))\n",
    "        else:\n",
    "            for i in range(start, N):\n",
    "                new_current = current.copy()\n",
    "                new_current[i] += 1\n",
    "                rec(new_current, i, remaining-1)\n",
    "    # Include all degrees from 0 up to d\n",
    "    for degree in range(d+1):\n",
    "        # Initialize multi-index with zeros\n",
    "        base = [0]*N\n",
    "        # Recursively fill in\n",
    "        rec(base, 0, degree)\n",
    "    return indices\n",
    "\n",
    "def build_A(D_data):\n",
    "    for k in range(len(D_data)):\n",
    "        Dk = D_data[k]  # shape (N,)\n",
    "        for j in range(len(Dk)):\n",
    "            row = []\n",
    "            for alpha in multiindices:\n",
    "                # For the derivative with respect to D_j,\n",
    "                # the coefficient is: alpha[j] * Dk^(alpha - e_j)\n",
    "                # If alpha[j] == 0, this term is zero.\n",
    "                if alpha[j] == 0:\n",
    "                    row.append(0.0)\n",
    "                else:\n",
    "                    # Compute Dk^(alpha - e_j)\n",
    "                    term = 1.0\n",
    "                    for i in range(N):\n",
    "                        exponent = alpha[i] - (1 if i == j else 0)\n",
    "                        term *= Dk[i]**exponent if exponent > 0 else 1.0\n",
    "                    row.append(alpha[j] * term)\n",
    "            A.append(row)\n",
    "            \n",
    "    return np.array(A)\n",
    "\n",
    "def build_b(G_data):\n",
    "    for k in range(G_data.shape[0]):\n",
    "        for j in range(G_data.shape[1]):\n",
    "            b.append(G_data[k, j])\n",
    "            \n",
    "    return np.array(b)\n",
    "\n",
    "def evaluate_polynomial_array(D, multiindices, c):\n",
    "    \"\"\"\n",
    "    Evaluate the polynomial at multiple points.\n",
    "    \n",
    "    Parameters:\n",
    "    - D: numpy array of shape (M, N) where each row is an N-dimensional input.\n",
    "    - multiindices: list of tuples, each tuple being the exponents for one term.\n",
    "    - c: numpy array of coefficients corresponding to each multi-index.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: numpy array of shape (M,) with the computed polynomial values.\n",
    "    \"\"\"\n",
    "    D = np.asarray(D)  # Ensure D is a numpy array\n",
    "    M, N = D.shape\n",
    "    predictions = np.zeros(M)\n",
    "    \n",
    "    for coeff, alpha in zip(c, multiindices):\n",
    "        # Compute the term D^alpha for each point.\n",
    "        # Convert alpha to an array to enable broadcasting.\n",
    "        alpha_array = np.array(alpha)\n",
    "        # For each point, compute the product of each dimension raised to the corresponding power.\n",
    "        term = coeff * np.prod(D ** alpha_array, axis=1)\n",
    "        predictions += term\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(pca_cols)\n",
    "d = 2  # for example, quadratic polynomial\n",
    "\n",
    "# Get multi-index list for polynomial basis.\n",
    "multiindices = multiindex_list(N, d)\n",
    "num_terms = len(multiindices)\n",
    "\n",
    "# Build design matrix A and measurement vector b.\n",
    "# There will be K * N equations (each derivative component).\n",
    "A = []\n",
    "b = []\n",
    "D_data = ref_umap_df_dt[pca_cols].to_numpy()\n",
    "G_data = ref_umap_df_dt[dt_cols[:-1]].to_numpy()  \n",
    "\n",
    "A = build_A(D_data)\n",
    "b = build_b(G_data)\n",
    "\n",
    "# Solve the least squares problem\n",
    "c, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = evaluate_polynomial_array(D_data, multiindices, c) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = px.scatter(x=ref_umap_df[\"predicted_stage_hpf\"], y=prediction)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
