{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from glob2 import glob\n",
    "# from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/\"\n",
    "train_name = \"20241107_ds\"\n",
    "model_name = \"SeqVAE_z100_ne150_sweep_01_block01_iter030\" \n",
    "train_dir = os.path.join(root, \"training_data\", train_name, \"\")\n",
    "output_dir = os.path.join(train_dir, model_name) \n",
    "\n",
    "# get path to morph model\n",
    "training_path = sorted(glob(os.path.join(output_dir, \"*\")))[-1]\n",
    "training_name = os.path.dirname(training_path)\n",
    "morph_read_path = os.path.join(training_path, \"figures\", \"\")\n",
    "\n",
    "# set path to hooke projections\n",
    "hooke_model_name = \"bead_expt_linear\"\n",
    "latent_path = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/seq_data/emb_projections/latent_projections/\"\n",
    "hooke_model_path = os.path.join(latent_path, hooke_model_name, \"\")\n",
    "\n",
    "# path to save data\n",
    "out_path = os.path.join(root, \"results\", \"20240303\", \"\")\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# path to figures and data\n",
    "fig_path = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/slides/morphseq/20250312/morphseq_cca/\"\n",
    "os.makedirs(fig_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# morph latent encodings\n",
    "morph_df = pd.read_csv(out_path + \"hf_morph_df.csv\")\n",
    "\n",
    "# hooke latent encodings\n",
    "seq_df = pd.read_csv(out_path + \"hf_seq_df.csv\", index_col=0)\n",
    "\n",
    "# metadata df that allows us to link the two\n",
    "morphseq_df = pd.read_csv(os.path.join(root, \"metadata\", \"morphseq_metadata.csv\"))\n",
    "\n",
    "# load spline datasets for each space--we will use these to pretrain our MLP\n",
    "morph_spline_df = pd.read_csv(out_path + \"spline_morph_df.csv\")\n",
    "morph_spline_df = morph_spline_df.set_index(\"stage_hpf\")\n",
    "seq_spline_df = pd.read_csv(out_path + \"spline_seq_df.csv\")\n",
    "seq_spline_df = seq_spline_df.set_index(\"stage_hpf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Subset for hotfish2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hf_experiments = np.asarray([\"20240813_24hpf\", \"20240813_30hpf\", \"20240813_36hpf\"])\n",
    "hf_morphseq_df = morphseq_df.loc[np.isin(morphseq_df[\"experiment_date\"], hf_experiments), :].reset_index(drop=True)\n",
    "\n",
    "# subset morph \n",
    "# mu_cols = [col for col in morph_df.columns.tolist() if \"z_mu_b\" in col]\n",
    "pattern = r\"PCA_.*_bio\"\n",
    "pca_cols_morph = [col for col in morph_df.columns if re.search(pattern, col)]\n",
    "pca_cols_seq = [col for col in seq_df.columns if \"PCA\" in col]\n",
    "\n",
    "hf_morph_df = pd.DataFrame(hf_morphseq_df.loc[:, [\"snip_id\", \"sample\"]]).merge(morph_df, how=\"inner\", on=\"snip_id\")\n",
    "hf_morph_df = hf_morph_df.set_index(\"snip_id\")\n",
    "hf_morph_df = hf_morph_df.loc[:, pca_cols_morph + [\"sample\", \"mdl_stage_hpf\"]]\n",
    "\n",
    "\n",
    "# subset seq dataset\n",
    "hf_seq_df = pd.DataFrame(hf_morph_df.loc[:, \"sample\"]).merge(seq_df, how=\"inner\", right_index=True, left_on=\"sample\")\n",
    "hf_seq_df = hf_seq_df.set_index(\"sample\")\n",
    "print(hf_seq_df.shape)\n",
    "\n",
    "# get rid of sample col\n",
    "hf_morph_df = hf_morph_df.drop(labels=[\"sample\"], axis=1)\n",
    "print(hf_morph_df.shape)\n",
    "\n",
    "# filter out a couple observations that had QC problems\n",
    "hf_morphseq_df = hf_morphseq_df.loc[np.isin(hf_morphseq_df[\"snip_id\"], hf_morph_df.index), :].reset_index()\n",
    "hf_morphseq_df = hf_morphseq_df.merge(morph_df.loc[:, [\"snip_id\", \"mdl_stage_hpf\"]])\n",
    "print(hf_morphseq_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_morph_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Extract spline and obs columns to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# n_components = len(pca_cols_morph) # captures over 99% of variance in both modalities\n",
    "\n",
    "# get morph array\n",
    "# morph_pca = hf_morph_df[pca_cols_morph].to_numpy() #morph_pca.transform(hf_morph_df)\n",
    "\n",
    "# # get morph spline\n",
    "# morph_spline_pca = morph_spline_df[pca_cols_morph].to_numpy()\n",
    "\n",
    "# get seq array\n",
    " #morph_pca.transform(hf_morph_df)\n",
    "\n",
    "# get seq spline\n",
    "# seq_spline_pca = seq_spline_df[pca_cols_seq].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Use K-fold cross validation to identify the optimal MLP archicture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from itertools import product\n",
    "from tqdm import tqdm \n",
    "\n",
    "seq_pca = hf_seq_df[pca_cols_seq].to_numpy()\n",
    "# n_kf_splits = 5\n",
    "# # n_dim_out = morph_pca.shape[1]\n",
    "# y = hf_morph_df.loc[:, \"mdl_stage_hpf\"].to_numpy()\n",
    "\n",
    "# # designate parameters to sweep\n",
    "# n_dim_in_vec = [5, 10, 15, 20, 25, 50, 75, 100]\n",
    "# layer_size_list = [10, 25, 50, 100]\n",
    "\n",
    "# # generate layer variants\n",
    "# one_layer_configs = [(l,) for l in layer_size_list]\n",
    "# two_layer_configs = list(product(layer_size_list, layer_size_list))\n",
    "# # three_layer_configs = list(product(layer_size_list[:2], layer_size_list[:3], layer_size_list[:3]))\n",
    "# mdl_configs = one_layer_configs + two_layer_configs #+ three_layer_configs\n",
    "\n",
    "# # get full list of variants\n",
    "# model_specs = list(product(mdl_configs, n_dim_in_vec))\n",
    "# model_specs_arr = [[m[0], m[1]] for m in model_specs]\n",
    "\n",
    "# # Set up k-fold cross-validation (here, 5 folds)\n",
    "# kf = KFold(n_splits=n_kf_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# # get DF for training\n",
    "# mdl_df = pd.DataFrame(model_specs_arr, columns=[\"mdl_config\", \"n_dim_in\"])\n",
    "\n",
    "# for m in tqdm(range(mdl_df.shape[0])):\n",
    "\n",
    "#     n_dim_in = mdl_df.loc[m, \"n_dim_in\"]\n",
    "#     mdl_config = mdl_df.loc[m, \"mdl_config\"]\n",
    "\n",
    "#     X = seq_pca[:, :n_dim_in]\n",
    "    \n",
    "#     # initialize model\n",
    "#     mlp = MLPRegressor(random_state=42, max_iter=20000, hidden_layer_sizes=mdl_config)\n",
    "    \n",
    "#     # Evaluate the model using cross_val_score, with RÂ² as the scoring metric\n",
    "#     scores = cross_val_score(mlp, X, y, cv=kf, scoring='r2')\n",
    "    \n",
    "#     mdl_df.loc[m, \"score\"] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl_df.to_csv(os.path.join(out_path, \"stage_mlp_cv_scores.csv\"), index=False)\n",
    "mdl_df = pd.read_csv(os.path.join(out_path, \"stage_mlp_cv_scores.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(mdl_df, x=\"n_dim_in\", y=\"score\", hover_data=[\"mdl_config\"])\n",
    "\n",
    "fig.update_layout(width=600, height=600,\n",
    "                  xaxis=dict(title=\"number of seq PC components\"),\n",
    "                  yaxis=dict(title=\"CV score (R2)\"),\n",
    "                  title=\"MLP model performance\",\n",
    "                 font=dict(\n",
    "                    family=\"Arial, sans-serif\",\n",
    "                    size=18,  # Adjust this value to change the global font size\n",
    "                    color=\"black\"\n",
    "                ))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"mlp_cv_scores.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Adding components clearly hurts predictive power overall, but the trend is non-monotonic. Moreover, there is absolutely no rhyme or reason to which architecture does best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Next step: use bootstrap resampling to assess predictive performance\n",
    "This procedure is a little loopy but I think it will work. Idea is to fit model using N bootrap samples. For each fit, I will obtain predictions for whatever obervations are not included in the bootstrap sample. After it is all said and done, I should have a dataset with multiple unbiased predictions for each observation that I can use to get a decent gauge for true predictive capacity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "n_boots = 100\n",
    "boot_size = 150\n",
    "n_dim_in = 5 # nice to have richer transcriptional info\n",
    "mdl_config = (10,) # this was consitently a strong performer\n",
    "# n_dim_out = 3 #morph_pca.shape[1]\n",
    "\n",
    "np.random.seed(371)\n",
    "\n",
    "# index vector to select from\n",
    "n_obs = hf_morph_df.shape[0]\n",
    "boot_options = np.arange(n_obs)\n",
    "\n",
    "# snip IDs\n",
    "snip_ids = hf_morph_df.index\n",
    "\n",
    "# predictors\n",
    "X = seq_pca[:, :n_dim_in]\n",
    "Y = hf_morph_df.loc[:, \"mdl_stage_hpf\"].to_numpy()\n",
    "\n",
    "# initialize vectors\n",
    "boot_id_vec = []\n",
    "morph_pd_vec = []\n",
    "snip_id_vec = []\n",
    "\n",
    "for n in tqdm(range(n_boots)):\n",
    "    \n",
    "    # take bootstrap sample\n",
    "    boot_indices = np.random.choice(boot_options, boot_size, replace=True)\n",
    "    X_boot = X[boot_indices]\n",
    "    Y_boot = Y[boot_indices]\n",
    "\n",
    "    # initialize model\n",
    "    mlp = MLPRegressor(random_state=42, max_iter=20000, hidden_layer_sizes=mdl_config)\n",
    "\n",
    "    # fit\n",
    "    mlp.fit(X_boot, Y_boot)\n",
    "\n",
    "    # identify held-out samples and get predictions\n",
    "    test_indices = boot_options[~np.isin(boot_options, boot_indices)]\n",
    "\n",
    "    if len(test_indices) > 0:\n",
    "        X_test = X[test_indices]\n",
    "        Y_pd = mlp.predict(X_test)\n",
    "\n",
    "        # add info\n",
    "        boot_id_vec += [n]*len(test_indices)\n",
    "        snip_id_vec += [snip_ids[i] for i in test_indices]\n",
    "        morph_pd_vec.append(Y_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert vectors to DF and get summary stats\n",
    "morph_pd_df_full = pd.DataFrame(snip_id_vec, columns=[\"snip_id\"])\n",
    "\n",
    "# morph_pd_df_full[\"boot_id\"] = boot_id_vec\n",
    "morph_pd_df_full[\"seq_stage_hpf\"] = np.concatenate(morph_pd_vec)\n",
    "\n",
    "# get summary stats\n",
    "morph_pd_df = morph_pd_df_full.groupby(\"snip_id\").agg([\"mean\", \"std\", \"count\"])\n",
    "\n",
    "# Flatten the MultiIndex columns to a single level:\n",
    "morph_pd_df.columns = [f\"{col[0]}_{col[1]}\" for col in morph_pd_df.columns]\n",
    "morph_pd_df.columns = [col.replace(\"_mean\", \"\") for col in morph_pd_df.columns]\n",
    "\n",
    "# Optionally, you can rename the index back to a column if needed:\n",
    "morph_pd_df = morph_pd_df.reset_index()\n",
    "morph_pd_df = pd.DataFrame(hf_morphseq_df.loc[:, [\"snip_id\", \"temperature\", \"timepoint\", \"mdl_stage_hpf\"]]).merge(morph_pd_df, how=\"inner\", on=\"snip_id\")\n",
    "morph_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cols = [col for col in morph_pd_df.columns if \"_mean\" in col]\n",
    "\n",
    "fig = px.scatter(morph_pd_df, x=\"mdl_stage_hpf\", y=\"seq_stage_hpf\", error_y=\"seq_stage_hpf_std\",\n",
    "                            color=\"temperature\", hover_data=[\"timepoint\", \"snip_id\"])\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "\n",
    "fig.update_layout(width=800, height=600,\n",
    "                  xaxis=dict(title=\"morphological stage (actual)\"),\n",
    "                  yaxis=dict(title=\"morphological stage (predicted)\"),\n",
    "                  # title=\"PCA decomposition of morphVAE latent space\",\n",
    "                 font=dict(\n",
    "                    family=\"Arial, sans-serif\",\n",
    "                    size=16,  # Adjust this value to change the global font size\n",
    "                    color=\"black\"\n",
    "                ))\n",
    "\n",
    "tmin = np.floor(np.min([morph_pd_df[\"mdl_stage_hpf\"], morph_pd_df[\"seq_stage_hpf\"]])/5)*5\n",
    "tmax = np.ceil(np.max([morph_pd_df[\"mdl_stage_hpf\"], morph_pd_df[\"seq_stage_hpf\"]])/5)*5\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=tmin,\n",
    "    y0=tmin,\n",
    "    x1=tmax,\n",
    "    y1=tmax,\n",
    "    line=dict(\n",
    "        dash=\"dash\",\n",
    "        color=\"black\",\n",
    "        width=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"morph_stage_pd_vs_true.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### What about intra-group residuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_mean_df = morph_pd_df.loc[:, [\"temperature\", \"timepoint\", \"mdl_stage_hpf\", \"seq_stage_hpf\"]].groupby(\n",
    "                               [\"temperature\", \"timepoint\"]).agg([\"mean\", \"std\"])\n",
    "\n",
    "# Flatten the MultiIndex columns to a single level:\n",
    "stage_mean_df.columns = [f\"{col[0]}_{col[1]}\" for col in stage_mean_df.columns]\n",
    "stage_mean_df = stage_mean_df.reset_index()\n",
    "\n",
    "# join back onto original data frame\n",
    "keep_cols = [col for col in morph_pd_df.columns if \"std\" not in col]\n",
    "\n",
    "morph_pd_df = morph_pd_df.loc[:, keep_cols].merge(stage_mean_df, on=[\"temperature\", \"timepoint\"], how=\"left\")\n",
    "morph_pd_df[\"true_resid\"] = morph_pd_df[\"mdl_stage_hpf\"] - morph_pd_df[\"mdl_stage_hpf_mean\"]\n",
    "morph_pd_df[\"pd_resid\"] = morph_pd_df[\"seq_stage_hpf\"] - morph_pd_df[\"seq_stage_hpf_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(morph_pd_df, x=\"true_resid\", y=\"pd_resid\", color=\"temperature\", symbol=\"timepoint\")\n",
    "\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "\n",
    "fig.update_layout(#xaxis=dict(title=\"morphological stage (actual)\"),\n",
    "                  #yaxis=dict(title=\"morphological stage (predicted)\"),\n",
    "                  # title=\"PCA decomposition of morphVAE latent space\",\n",
    "                width=1000, height=800,\n",
    "                 font=dict(\n",
    "                    family=\"Arial, sans-serif\",\n",
    "                    size=16,  # Adjust this value to change the global font size\n",
    "                    color=\"black\"\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix_by_group = morph_pd_df.groupby([\"temperature\", \"timepoint\"])[['true_resid', 'pd_resid']].corr()\n",
    "corr_df = morph_pd_df.groupby([\"temperature\", \"timepoint\", \"mdl_stage_hpf_mean\", \"seq_stage_hpf_mean\", \n",
    "                                     \"mdl_stage_hpf_std\", \"seq_stage_hpf_std\"]).apply(\n",
    "                                        lambda x: x['true_resid'].cov(x['pd_resid'])).reset_index()\n",
    "corr_df = corr_df.rename(columns={0:\"cov\"})\n",
    "corr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(corr_df, x=\"mdl_stage_hpf_std\", y=\"cov\", color=\"temperature\", symbol=\"timepoint\", \n",
    "                 color_continuous_scale=\"RdBu_r\", range_color=[19, 37.5])\n",
    "\n",
    "fig.update_traces(marker=dict(size=10, line=dict(color=\"black\", width=1)))\n",
    "\n",
    "fig.update_layout(width=800, height=600,\n",
    "                  xaxis=dict(title=\"temporal dispersion (hrs)\"),\n",
    "                  yaxis=dict(title=\"seq-to-morph stage correlation\"),\n",
    "                  # title=\"PCA decomposition of morphVAE latent space\",\n",
    "                 font=dict(\n",
    "                    family=\"Arial, sans-serif\",\n",
    "                    size=16,  # Adjust this value to change the global font size\n",
    "                    color=\"black\"\n",
    "                ),\n",
    "                 coloraxis_colorbar=dict(\n",
    "                    x=1,  # Increase x to move the colorbar rightwards\n",
    "                    y=0.5,   # Center vertically (default is often around 0.5)\n",
    "                    len=0.5  # Adjust the length if needed\n",
    "                ))\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"stage_resid_covariance.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Step back and assess morph predictions more generally: are they better than just looking at the pop average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# get cohort averages\n",
    "morph_df_true = hf_morph_df.copy().reset_index()\n",
    "morph_df_true = morph_df_true.merge(morphseq_df.loc[:, [\"snip_id\", \"timepoint\", \"temperature\"]], how=\"left\", on=\"snip_id\")\n",
    "morph_df_mean = morph_df_true.drop(labels=[\"snip_id\"], axis=1).groupby([\"temperature\"]).agg([\"mean\"])\n",
    "\n",
    "# Flatten the MultiIndex columns to a single level:\n",
    "morph_df_mean.columns = [f\"{col[0]}_{col[1]}\" for col in morph_df_mean.columns]\n",
    "morph_df_mean = morph_df_mean.reset_index()\n",
    "\n",
    "# merge back to original obs\n",
    "morph_df_null = morph_df_true.loc[:, [\"snip_id\", \"timepoint\", \"temperature\"]].merge(\n",
    "                morph_df_mean, how=\"left\", on=[\"timepoint\", \"temperature\"])\n",
    "\n",
    "# extract just the PCA values to compare\n",
    "Y_pd = morph_pd_df[mean_cols].values\n",
    "Y_mean = morph_df_null[mean_cols].values\n",
    "Y_true = morph_df_true[pca_cols_morph[:n_dim_out]].values\n",
    "\n",
    "# calculate mse\n",
    "pd_error = (Y_true-Y_pd)**2\n",
    "null_error = (Y_true-Y_mean)**2\n",
    "\n",
    "# convert to DF\n",
    "pd_df = pd.DataFrame(pd_error, columns=pca_cols_morph[:n_dim_out])\n",
    "pd_df[\"total_se\"] = np.sum(pd_df[pca_cols_morph[:n_dim_out]], axis=1)\n",
    "pd_df[\"timepoint\"] = morph_df_true[\"timepoint\"].to_numpy()\n",
    "pd_df[\"temperature\"] = morph_df_true[\"temperature\"].to_numpy()\n",
    "pd_df_mean = pd_df.groupby([\"temperature\", \"timepoint\"]).agg([\"mean\"])\n",
    "pd_df_mean.columns = [f\"{col[0]}_{col[1]}\" for col in pd_df_mean.columns]\n",
    "pd_df_mean = pd_df_mean.reset_index()\n",
    "\n",
    "null_df = pd.DataFrame(null_error, columns=pca_cols_morph[:n_dim_out])\n",
    "null_df[\"total_se\"] = np.sum(null_df[pca_cols_morph[:n_dim_out]], axis=1)\n",
    "null_df[\"timepoint\"] = morph_df_true[\"timepoint\"].to_numpy()\n",
    "null_df[\"temperature\"] = morph_df_true[\"temperature\"].to_numpy()\n",
    "null_df_mean = null_df.groupby([\"temperature\", \"timepoint\"]).agg([\"mean\"])\n",
    "null_df_mean.columns = [f\"{col[0]}_{col[1]}\" for col in null_df_mean.columns]\n",
    "null_df_mean = null_df_mean.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "fig = px.scatter(pd_df_mean, x=\"total_se_mean\", y=null_df_mean[\"total_se_mean\"], color=\"temperature\", symbol=\"timepoint\")\n",
    "                # log_x=True, log_y=True)\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.update_layout(width=1000, height=800)\n",
    "fig.update_xaxes(range=[0, 4])\n",
    "fig.update_yaxes(range=[0, 4])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cols_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
