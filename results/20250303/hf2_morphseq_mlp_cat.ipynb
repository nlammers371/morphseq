{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Do Hooke embeddings contain predictive information over and above temperature and timepoint information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from glob2 import glob\n",
    "# from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/\"\n",
    "train_name = \"20241107_ds\"\n",
    "model_name = \"SeqVAE_z100_ne150_sweep_01_block01_iter030\" \n",
    "train_dir = os.path.join(root, \"training_data\", train_name, \"\")\n",
    "output_dir = os.path.join(train_dir, model_name) \n",
    "\n",
    "# get path to morph model\n",
    "training_path = sorted(glob(os.path.join(output_dir, \"*\")))[-1]\n",
    "training_name = os.path.dirname(training_path)\n",
    "morph_read_path = os.path.join(training_path, \"figures\", \"\")\n",
    "\n",
    "# set path to hooke projections\n",
    "hooke_model_name = \"bead_expt_linear\"\n",
    "latent_path = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/morphseq/seq_data/emb_projections/latent_projections/\"\n",
    "hooke_model_path = os.path.join(latent_path, hooke_model_name, \"\")\n",
    "\n",
    "# path to save data\n",
    "out_path = os.path.join(root, \"results\", \"20240303\", \"\")\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# path to figures and data\n",
    "fig_path = \"/Users/nick/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/slides/morphseq/20250312/morphseq_cca/\"\n",
    "os.makedirs(fig_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# morph latent encodings\n",
    "morph_df = pd.read_csv(out_path + \"hf_morph_df.csv\")\n",
    "\n",
    "# hooke latent encodings\n",
    "seq_df = pd.read_csv(out_path + \"hf_seq_df.csv\", index_col=0)\n",
    "\n",
    "# metadata df that allows us to link the two\n",
    "morphseq_df = pd.read_csv(os.path.join(root, \"metadata\", \"morphseq_metadata.csv\"))\n",
    "\n",
    "# load spline datasets for each space--we will use these to pretrain our MLP\n",
    "# morph_spline_df = pd.read_csv(out_path + \"spline_morph_df.csv\")\n",
    "# morph_spline_df = morph_spline_df.set_index(\"stage_hpf\")\n",
    "# seq_spline_df = pd.read_csv(out_path + \"spline_seq_df.csv\")\n",
    "# seq_spline_df = seq_spline_df.set_index(\"stage_hpf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Subset for hotfish2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hf_experiments = np.asarray([\"20240813_24hpf\", \"20240813_30hpf\", \"20240813_36hpf\"])\n",
    "hf_morphseq_df = morphseq_df.loc[np.isin(morphseq_df[\"experiment_date\"], hf_experiments), :].reset_index(drop=True)\n",
    "\n",
    "# subset morph \n",
    "# mu_cols = [col for col in morph_df.columns.tolist() if \"z_mu_b\" in col]\n",
    "pattern = r\"PCA_.*_bio\"\n",
    "pca_cols_morph = [col for col in morph_df.columns if re.search(pattern, col)]\n",
    "pca_cols_seq = [col for col in seq_df.columns if \"PCA\" in col]\n",
    "\n",
    "hf_morph_df = pd.DataFrame(hf_morphseq_df.loc[:, [\"snip_id\", \"timepoint\", \"temperature\", \"sample\"]]).merge(morph_df, how=\"inner\", on=\"snip_id\")\n",
    "hf_morph_df = hf_morph_df.set_index(\"snip_id\")\n",
    "hf_morph_df = hf_morph_df.loc[:, pca_cols_morph + [\"sample\", \"timepoint\", \"temperature\", ]]\n",
    "\n",
    "# subset seq dataset\n",
    "hf_seq_df = pd.DataFrame(hf_morph_df.loc[:, [\"sample\",  \"timepoint\", \"temperature\"]]).merge(\n",
    "                        seq_df, how=\"inner\", right_index=True, left_on=\"sample\")\n",
    "hf_seq_df = hf_seq_df.set_index(\"sample\")\n",
    "print(hf_seq_df.shape)\n",
    "\n",
    "# get rid of sample col\n",
    "hf_morph_df = hf_morph_df.drop(labels=[\"sample\"], axis=1)\n",
    "print(hf_morph_df.shape)\n",
    "\n",
    "# filter out a couple observations that had QC problems\n",
    "hf_morphseq_df = hf_morphseq_df.loc[np.isin(hf_morphseq_df[\"snip_id\"], hf_morph_df.index), :].reset_index()\n",
    "hf_morphseq_df = hf_morphseq_df.merge(morph_df.loc[:, [\"snip_id\", \"mdl_stage_hpf\"]])\n",
    "print(hf_morphseq_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Extract obs columns to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# n_components = len(pca_cols_morph) # captures over 99% of variance in both modalities\n",
    "\n",
    "# get morph array\n",
    "# morph_pca = hf_morph_df[pca_cols_morph].to_numpy() #morph_pca.transform(hf_morph_df)\n",
    "\n",
    "# get morph spline\n",
    "# morph_spline_pca = morph_spline_df[pca_cols_morph].to_numpy()\n",
    "\n",
    "# get seq array\n",
    "# seq_pca = hf_seq_df[pca_cols_seq].to_numpy() #morph_pca.transform(hf_morph_df)\n",
    "\n",
    "# get seq spline\n",
    "# seq_spline_pca = seq_spline_df[pca_cols_seq].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Add categorical vector encoding exp timepoint + temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# get all unique cohort vals\n",
    "tt_vals = hf_morphseq_df.loc[:, [\"timepoint\"]].drop_duplicates().reset_index(drop=True)\n",
    "tt_vals[\"cohort_id\"] = tt_vals.index\n",
    "\n",
    "# join on cohort_id\n",
    "# X_morph = hf_morph_df.merge(tt_vals, how=\"left\", on=[\"timepoint\", \"temperature\"]).drop(labels=[\"timepoint\", \"temperature\"], axis=1)\n",
    "X_seq = hf_seq_df.merge(tt_vals, how=\"left\", on=[\"timepoint\"]).drop(labels=[\"timepoint\", \"temperature\"], axis=1)\n",
    "\n",
    "# reformat\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), pca_cols_seq),\n",
    "        ('cat', OneHotEncoder(), ['cohort_id'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# build prediction arrays\n",
    "X_seq_full = preprocessor.fit_transform(X_seq)\n",
    "n_pca_cols = len(pca_cols_seq)\n",
    "X_seq_cat = X_seq_full[:, n_pca_cols:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Use K-fold cross validation to identify the optimal MLP archicture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from itertools import product\n",
    "from tqdm import tqdm \n",
    "\n",
    "n_kf_splits = 10\n",
    "morph_pca = hf_morph_df[pca_cols_morph].to_numpy()\n",
    "n_dim_out = 3 #morph_pca.shape[1]\n",
    "\n",
    "y = morph_pca[:, :n_dim_out]\n",
    "\n",
    "# designate parameters to sweep\n",
    "n_dim_in_vec = [3, 5, 10]\n",
    "layer_size_list = [10, 25, 50, 100]\n",
    "\n",
    "# generate layer variants\n",
    "one_layer_configs = [(l,) for l in layer_size_list]\n",
    "two_layer_configs = list(product(layer_size_list, layer_size_list))\n",
    "three_layer_configs = list(product(layer_size_list[:2], layer_size_list[:3], layer_size_list[:3]))\n",
    "mdl_configs = one_layer_configs + two_layer_configs + three_layer_configs\n",
    "\n",
    "# get full list of variants\n",
    "model_specs = list(product(mdl_configs, n_dim_in_vec))\n",
    "model_specs_arr = [[m[0], m[1]] for m in model_specs]\n",
    "\n",
    "# Set up k-fold cross-validation (here, 5 folds)\n",
    "kf = KFold(n_splits=n_kf_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# get DF for training\n",
    "mdl_df = pd.DataFrame(model_specs_arr, columns=[\"mdl_config\", \"n_dim_in\"])\n",
    "\n",
    "for m in tqdm(range(mdl_df.shape[0])):\n",
    "\n",
    "    n_dim_in = mdl_df.loc[m, \"n_dim_in\"]\n",
    "    mdl_config = mdl_df.loc[m, \"mdl_config\"]\n",
    "\n",
    "    X_cat = X_seq_full[:, n_pca_cols:]\n",
    "    X_pca = X_seq_full[:, :n_dim_in]\n",
    "    X_full = np.hstack((X_pca, X_cat))\n",
    "    \n",
    "    # initialize model\n",
    "    mlp0 = MLPRegressor(random_state=42, max_iter=2000, hidden_layer_sizes=mdl_config, tol=1e-3)\n",
    "    mlp1 = MLPRegressor(random_state=42, max_iter=2000, hidden_layer_sizes=mdl_config, tol=1e-3)\n",
    "    mlp2 = MLPRegressor(random_state=42, max_iter=2000, hidden_layer_sizes=mdl_config, tol=1e-3)\n",
    "    \n",
    "    # Evaluate the model using cross_val_score, with RÂ² as the scoring metric\n",
    "    scores0 = cross_val_score(mlp0, X_cat, y, cv=kf, scoring='r2')\n",
    "    scores1 = cross_val_score(mlp1, X_pca, y, cv=kf, scoring='r2')\n",
    "    scores2 = cross_val_score(mlp2, X_full, y, cv=kf, scoring='r2')\n",
    "    \n",
    "    mdl_df.loc[m, \"cat_score\"] = np.mean(scores0)\n",
    "    mdl_df.loc[m, \"pca_score\"] = np.mean(scores1)\n",
    "    mdl_df.loc[m, \"full_score\"] = np.mean(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl_df.to_csv(os.path.join(out_path, \"cat_mlp_cv_scores.csv\"), index=False)\n",
    "# mdl_df = pd.read_csv(os.path.join(out_path, \"mlp_cv_scores.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = \"full_score\"\n",
    "yp = \"pca_score\"\n",
    "\n",
    "fig = px.scatter(mdl_df, x=xp, y=yp, color=\"n_dim_in\", hover_data=[\"mdl_config\"])\n",
    "\n",
    "# Enforce equal scaling: one unit on x equals one unit on y.\n",
    "# Compute the overall minimum and maximum across both axes.\n",
    "min_val = min(mdl_df[xp].min(), mdl_df[yp].min()) - 0.1\n",
    "max_val = 1 #max(mdl_df[xp].max(), mdl_df[yp].max())\n",
    "\n",
    "# Update both axes to have the same range and enforce equal scaling.\n",
    "fig.update_xaxes(range=[min_val, max_val], scaleanchor=\"y\", scaleratio=1)\n",
    "fig.update_yaxes(range=[min_val, max_val], scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "fig.update_layout(width=600, height=600,\n",
    "                  xaxis=dict(title=\"cohort ID only\"),\n",
    "                  yaxis=dict(title=\"with seq pca\"),\n",
    "                  title=\"MLP model performance\",\n",
    "                 font=dict(\n",
    "                    family=\"Arial, sans-serif\",\n",
    "                    size=18,  # Adjust this value to change the global font size\n",
    "                    color=\"black\"\n",
    "                ))\n",
    "\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=min_val,\n",
    "    y0=min_val,\n",
    "    x1=max_val,\n",
    "    y1=max_val,\n",
    "    line=dict(\n",
    "        dash=\"dash\",\n",
    "        color=\"black\",\n",
    "        width=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"cat_mlp_cv_scores.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Not as interpretable as I'd hoped. There is no single architecture or input dim that reigns supreme. I could spend more time parsing this, but I think the main takeaway is that it does not matter too much, within reason. Moderately complex 2-layer models tend to do best ((10, 50), for instance). Overly simple or complex arhitectures generalize less well. Though even this trend is not absolute.\n",
    "\n",
    "The trend with number of input dimensions is more complicated. The optimimum does not change, but the average certainly varies: dropping at 10 and 15 components, recovering at 20 (??) and then dropping again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Next step: use bootstrap resampling to assess predictive performance\n",
    "This procedure is a little loopy but I think it will work. Idea is to fit model using N bootrap samples. For each fit, I will obtain predictions for whatever obervations are not included in the bootstrap sample. After it is all said and done, I should have a dataset with multiple unbiased predictions for each observation that I can use to get a decent gauge for true predictive capacity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "n_boots = 250\n",
    "boot_size = 250\n",
    "n_dim_in = 100 # nice to have richer transcriptional info\n",
    "mdl_config = (25, 100) # this was consitently a strong performer\n",
    "n_dim_out = morph_pca.shape[1]\n",
    "\n",
    "np.random.seed(371)\n",
    "\n",
    "# index vector to select from\n",
    "n_obs = hf_morph_df.shape[0]\n",
    "boot_options = np.arange(n_obs)\n",
    "\n",
    "# snip IDs\n",
    "snip_ids = hf_morph_df.index\n",
    "\n",
    "# predictors\n",
    "X = seq_pca[:, :n_dim_in]\n",
    "Y = morph_pca[:, :n_dim_out]\n",
    "\n",
    "# initialize vectors\n",
    "boot_id_vec = []\n",
    "morph_pd_vec = []\n",
    "snip_id_vec = []\n",
    "\n",
    "for n in tqdm(range(n_boots)):\n",
    "    \n",
    "    # take bootstrap sample\n",
    "    boot_indices = np.random.choice(boot_options, boot_size, replace=True)\n",
    "    X_boot = X[boot_indices]\n",
    "    Y_boot = Y[boot_indices]\n",
    "\n",
    "    # initialize model\n",
    "    mlp = MLPRegressor(random_state=42, max_iter=20000, hidden_layer_sizes=mdl_config, tol=1e-8)\n",
    "\n",
    "    # fit\n",
    "    mlp.fit(X_boot, Y_boot)\n",
    "\n",
    "    # identify held-out samples and get predictions\n",
    "    test_indices = boot_options[~np.isin(boot_options, boot_indices)]\n",
    "\n",
    "    if len(test_indices) > 0:\n",
    "        X_test = X[test_indices]\n",
    "        Y_pd = mlp.predict(X_test)\n",
    "\n",
    "        # add info\n",
    "        boot_id_vec += [n]*len(test_indices)\n",
    "        snip_id_vec += [snip_ids[i] for i in test_indices]\n",
    "        morph_pd_vec.append(Y_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert vectors to DF and get summary stats\n",
    "morph_pd_df_full = pd.DataFrame(snip_id_vec, columns=[\"snip_id\"])\n",
    "# morph_pd_df_full[\"boot_id\"] = boot_id_vec\n",
    "morph_pd_df_full[pca_cols_morph[:n_dim_out]] = np.vstack(morph_pd_vec)\n",
    "\n",
    "# get summary stats\n",
    "morph_pd_df = morph_pd_df_full.groupby(\"snip_id\").agg([\"mean\", \"std\", \"count\"])\n",
    "\n",
    "# Flatten the MultiIndex columns to a single level:\n",
    "morph_pd_df.columns = [f\"{col[0]}_{col[1]}\" for col in morph_pd_df.columns]\n",
    "\n",
    "# Optionally, you can rename the index back to a column if needed:\n",
    "morph_pd_df = morph_pd_df.reset_index()\n",
    "morph_pd_df = pd.DataFrame(hf_morphseq_df.loc[:, [\"snip_id\", \"temperature\", \"timepoint\", \"mdl_stage_hpf\"]]).merge(morph_pd_df, how=\"inner\", on=\"snip_id\")\n",
    "morph_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cols = [col for col in morph_pd_df.columns if \"_mean\" in col]\n",
    "\n",
    "fig = px.scatter_3d(morph_pd_df, x=mean_cols[0], y=mean_cols[1], z=mean_cols[2], color=\"temperature\", hover_data=[\"timepoint\", \"snip_id\"])\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(x=morph_pca[:, 0], y=morph_pca[:, 1], z=morph_pca[:, 2], color=hf_morphseq_df[\"temperature\"], \n",
    "                   hover_data=[morph_pd_df[\"snip_id\"]])\n",
    "\n",
    "# fig.add_traces(go.Scatter3d(x=morph_spline_pca[:, 0], y=morph_spline_pca[:, 1], z=morph_spline_pca[:, 2], mode=\"lines\"))\n",
    "\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.update_layout(title=\"morphology space\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Assess how well the model predicts morphological stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load staging model\n",
    "morph_stage_model = joblib.load(os.path.join(out_path, 'morph_stage_model.joblib'))\n",
    "\n",
    "# get predicted morphological stages using the seq->morph embeddings\n",
    "morph_pd_df[\"seq_stage_hpf\"] = morph_stage_model.predict(morph_pd_df[mean_cols].values)\n",
    "\n",
    "# stage_pd_vec_check = morph_stage_model.predict(morph_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(morph_pd_df, x=\"mdl_stage_hpf\", y=\"seq_stage_hpf\", color=\"temperature\")\n",
    "\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "\n",
    "fig.update_layout(xaxis=dict(title=\"morphological stage (actual)\"),\n",
    "                  yaxis=dict(title=\"morphological stage (predicted)\"),\n",
    "                  # title=\"PCA decomposition of morphVAE latent space\",\n",
    "                 font=dict(\n",
    "                    family=\"Arial, sans-serif\",\n",
    "                    size=16,  # Adjust this value to change the global font size\n",
    "                    color=\"black\"\n",
    "                ))\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=10,\n",
    "    y0=10,\n",
    "    x1=42,\n",
    "    y1=42,\n",
    "    line=dict(\n",
    "        dash=\"dash\",\n",
    "        color=\"black\",\n",
    "        width=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(fig_path, \"seq_to_morph_stage_scatter.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### What about intra-group residuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_mean_df = morph_pd_df.loc[:, [\"temperature\", \"timepoint\", \"mdl_stage_hpf\", \"seq_stage_hpf\"]].groupby(\n",
    "                               [\"temperature\", \"timepoint\"]).agg([\"mean\"])\n",
    "\n",
    "# Flatten the MultiIndex columns to a single level:\n",
    "stage_mean_df.columns = [f\"{col[0]}_{col[1]}\" for col in stage_mean_df.columns]\n",
    "stage_mean_df = stage_mean_df.reset_index()\n",
    "\n",
    "# join back onto original data frame\n",
    "morph_pd_df = morph_pd_df.merge(stage_mean_df, on=[\"temperature\", \"timepoint\"], how=\"left\")\n",
    "morph_pd_df[\"true_resid\"] = morph_pd_df[\"mdl_stage_hpf\"] - morph_pd_df[\"mdl_stage_hpf_mean\"]\n",
    "morph_pd_df[\"pd_resid\"] = morph_pd_df[\"seq_stage_hpf\"] - morph_pd_df[\"seq_stage_hpf_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(morph_pd_df, x=\"true_resid\", y=\"pd_resid\", color=\"temperature\", symbol=\"timepoint\")\n",
    "\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "\n",
    "fig.update_layout(#xaxis=dict(title=\"morphological stage (actual)\"),\n",
    "                  #yaxis=dict(title=\"morphological stage (predicted)\"),\n",
    "                  # title=\"PCA decomposition of morphVAE latent space\",\n",
    "                width=1000, height=800,\n",
    "                 font=dict(\n",
    "                    family=\"Arial, sans-serif\",\n",
    "                    size=16,  # Adjust this value to change the global font size\n",
    "                    color=\"black\"\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix_by_group = morph_pd_df.groupby([\"temperature\", \"timepoint\"])[['true_resid', 'pd_resid']].corr()\n",
    "corr_by_group = morph_pd_df.groupby([\"temperature\", \"timepoint\"]).apply(lambda x: x['true_resid'].corr(x['pd_resid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(corr_by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Step back and assess morph predictions more generally: are they better than just looking at the pop average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# get cohort averages\n",
    "morph_df_true = hf_morph_df.copy().reset_index()\n",
    "morph_df_true = morph_df_true.merge(morphseq_df.loc[:, [\"snip_id\", \"timepoint\", \"temperature\"]], how=\"left\", on=\"snip_id\")\n",
    "morph_df_mean = morph_df_true.drop(labels=[\"snip_id\"], axis=1).groupby([\"temperature\", \"timepoint\"]).agg([\"mean\"])\n",
    "\n",
    "# Flatten the MultiIndex columns to a single level:\n",
    "morph_df_mean.columns = [f\"{col[0]}_{col[1]}\" for col in morph_df_mean.columns]\n",
    "morph_df_mean = morph_df_mean.reset_index()\n",
    "\n",
    "# merge back to original obs\n",
    "morph_df_null = morph_df_true.loc[:, [\"snip_id\", \"timepoint\", \"temperature\"]].merge(\n",
    "                morph_df_mean, how=\"left\", on=[\"timepoint\", \"temperature\"])\n",
    "\n",
    "# extract just the PCA values to compare\n",
    "Y_pd = morph_pd_df[mean_cols].values\n",
    "Y_mean = morph_df_null[mean_cols].values\n",
    "Y_true = morph_df_true[pca_cols_morph[:n_dim_out]].values\n",
    "\n",
    "# calculate mse\n",
    "pd_error = (Y_true-Y_pd)**2\n",
    "null_error = (Y_true-Y_mean)**2\n",
    "\n",
    "# convert to DFz\n",
    "pd_df = pd.DataFrame(pd_error, columns=pca_cols_morph[:n_dim_out])\n",
    "pd_df[\"total_se\"] = np.sqrt(np.sum(pd_df[pca_cols_morph[:n_dim_out]], axis=1))\n",
    "pd_df[\"timepoint\"] = morph_df_true[\"timepoint\"].to_numpy()\n",
    "pd_df[\"temperature\"] = morph_df_true[\"temperature\"].to_numpy()\n",
    "pd_df_mean = pd_df.groupby([\"temperature\", \"timepoint\"]).agg([\"mean\"])\n",
    "pd_df_mean.columns = [f\"{col[0]}_{col[1]}\" for col in pd_df_mean.columns]\n",
    "pd_df_mean = pd_df_mean.reset_index()\n",
    "\n",
    "null_df = pd.DataFrame(null_error, columns=pca_cols_morph[:n_dim_out])\n",
    "null_df[\"total_se\"] = np.sqrt(np.sum(null_df[pca_cols_morph[:n_dim_out]], axis=1))\n",
    "null_df[\"timepoint\"] = morph_df_true[\"timepoint\"].to_numpy()\n",
    "null_df[\"temperature\"] = morph_df_true[\"temperature\"].to_numpy()\n",
    "null_df_mean = null_df.groupby([\"temperature\", \"timepoint\"]).agg([\"mean\"])\n",
    "null_df_mean.columns = [f\"{col[0]}_{col[1]}\" for col in null_df_mean.columns]\n",
    "null_df_mean = null_df_mean.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "fig = px.scatter(pd_df_mean, x=\"total_se_mean\", y=null_df_mean[\"total_se_mean\"], color=\"temperature\", symbol=\"timepoint\")\n",
    "                # log_x=True, log_y=True)\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.update_layout(width=1000, height=800)\n",
    "fig.update_xaxes(range=[0, 4])\n",
    "fig.update_yaxes(range=[0, 4])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cols_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
