{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import plotly.express as px\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.interpolate import CubicSpline, interp1d\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq\")\n",
    "\n",
    "# pert_comparisons = [\"wnt-i\", \"wt\"]\n",
    "pert_comparisons = [\"wnt-i\", \"tgfb-i\", \"wt\", \"lmx1b\", \"gdf3\"]\n",
    "\n",
    "color_map = {\n",
    "    \"wnt-i\": \"red\",\n",
    "    \"tgfb-i\": \"green\",\n",
    "    \"wt\": \"blue\",\n",
    "    \"lmx1b\": \"orange\",\n",
    "    \"gdf3\": \"purple\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories_3d(splines_final):\n",
    "    \"\"\"\n",
    "    Plots PCA trajectories for different perturbations and datasets in a 3D Plotly plot.\n",
    "\n",
    "    Parameters:\n",
    "    splines_final (pd.DataFrame): DataFrame containing the trajectory data with columns\n",
    "                                  ['dataset', 'Perturbation', 'point_index', 'PCA_1', 'PCA_2', 'PCA_3']\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Define perturbations and their corresponding colors\n",
    "    pert_comparisons = [\"wnt-i\", \"tgfb-i\", \"wt\", \"lmx1b\", \"gdf3\"]\n",
    "    \n",
    "    color_map = {\n",
    "        \"wnt-i\": \"red\",\n",
    "        \"tgfb-i\": \"green\",\n",
    "        \"wt\": \"blue\",\n",
    "        \"lmx1b\": \"orange\",\n",
    "        \"gdf3\": \"purple\"\n",
    "    }\n",
    "    \n",
    "    # Define dataset styles with dash styles\n",
    "    dataset_styles = {\n",
    "        \"all\": {\"dash\": \"solid\", \"name\": \"all\"},\n",
    "        \"hld\": {\"dash\": \"dash\", \"name\": \"hld\"},\n",
    "        \"hld_aligned\": {\"dash\": \"dot\", \"name\": \"hld aligned\"}\n",
    "    }\n",
    "    \n",
    "    # Initialize the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Iterate over each perturbation\n",
    "    for pert in pert_comparisons:\n",
    "        pert_data = splines_final[splines_final['Perturbation'] == pert]\n",
    "        color = color_map.get(pert, \"black\")  # Default to black if perturbation not found\n",
    "        \n",
    "        # Iterate over each dataset\n",
    "        for dataset, style in dataset_styles.items():\n",
    "            dataset_data = pert_data[pert_data['dataset'] == dataset]\n",
    "            \n",
    "            if dataset_data.empty:\n",
    "                continue  # Skip if there's no data for this dataset\n",
    "            \n",
    "            # Sort by point_index to ensure proper trajectory\n",
    "            dataset_data = dataset_data.sort_values(by='point_index')\n",
    "            \n",
    "            # Add trace\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=dataset_data['PCA_1'],\n",
    "                    y=dataset_data['PCA_2'],\n",
    "                    z=dataset_data['PCA_3'],\n",
    "                    mode='lines',\n",
    "                    name=f\"{pert} - {style['name']}\",\n",
    "                    line=dict(color=color, dash=style['dash'], width=4),\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "    \n",
    "    # Show the plot\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalPrincipalCurve:\n",
    "    def __init__(self, bandwidth=0.5, max_iter=100, tol=1e-4, angle_penalty_exp=2, h=None):\n",
    "        \"\"\"\n",
    "        Initialize the Local Principal Curve solver.\n",
    "        \"\"\"\n",
    "        self.bandwidth = bandwidth\n",
    "        self.h = h if h is not None else self.bandwidth\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.angle_penalty_exp = angle_penalty_exp\n",
    "\n",
    "        self.initializations = []\n",
    "        self.paths = []\n",
    "        self.cubic_splines_eq = []\n",
    "        self.cubic_splines = []\n",
    "\n",
    "    def _kernel_weights(self, dataset, x):\n",
    "        \"\"\"Compute Gaussian kernel weights w_i = K_h(X_i - x).\"\"\"\n",
    "        dists = np.linalg.norm(dataset - x, axis=1)\n",
    "        weights = np.exp(- (dists**2) / (2 * self.bandwidth**2))\n",
    "        w = weights / np.sum(weights)\n",
    "        return w\n",
    "\n",
    "    def _local_center_of_mass(self, dataset, x):\n",
    "        \"\"\"Compute µ_x, the local center of mass around x.\"\"\"\n",
    "        w = self._kernel_weights(dataset, x)\n",
    "        mu = np.sum(dataset.T * w, axis=1)\n",
    "        return mu\n",
    "\n",
    "    def _local_covariance(self, dataset, x, mu):\n",
    "        \"\"\"Compute the local covariance matrix Σ_x.\"\"\"\n",
    "        w = self._kernel_weights(dataset, x)\n",
    "        centered = dataset - mu\n",
    "        cov = np.zeros((dataset.shape[1], dataset.shape[1]))\n",
    "        for i in range(len(dataset)):\n",
    "            cov += w[i] * np.outer(centered[i], centered[i])\n",
    "        return cov\n",
    "\n",
    "    def _principal_component(self, cov, prev_vec=None):\n",
    "        \"\"\"Compute the first local principal component γ_x, with angle penalization.\"\"\"\n",
    "        vals, vecs = np.linalg.eig(cov)\n",
    "        idx = np.argsort(vals)[::-1]\n",
    "        vals = vals[idx]\n",
    "        vecs = vecs[:, idx]\n",
    "\n",
    "        gamma = vecs[:, 0]  # first principal component\n",
    "\n",
    "        # Sign flipping to maintain direction if prev_vec is given\n",
    "        if prev_vec is not None and np.linalg.norm(prev_vec) != 0:\n",
    "            cos_alpha = np.dot(gamma, prev_vec) / (np.linalg.norm(gamma)*np.linalg.norm(prev_vec))\n",
    "            if cos_alpha < 0:\n",
    "                gamma = -gamma\n",
    "\n",
    "            # Angle penalization\n",
    "            cos_alpha = np.dot(gamma, prev_vec) / (np.linalg.norm(gamma)*np.linalg.norm(prev_vec))\n",
    "            a_x = (abs(cos_alpha))**self.angle_penalty_exp\n",
    "            gamma = a_x * gamma + (1 - a_x) * prev_vec\n",
    "            gamma /= np.linalg.norm(gamma)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    def _forward_run(self, dataset, x_start):\n",
    "        \"\"\"Run the algorithm forward from a starting point using the full dataset.\"\"\"\n",
    "        x = x_start\n",
    "        path_x = [x]\n",
    "        prev_gamma = None\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            mu = self._local_center_of_mass(dataset, x)\n",
    "            cov = self._local_covariance(dataset, x, mu)\n",
    "            gamma = self._principal_component(cov, prev_vec=prev_gamma)\n",
    "\n",
    "            x_new = mu + self.h * gamma\n",
    "\n",
    "            # Check convergence\n",
    "            if np.linalg.norm(mu - x) < self.tol:\n",
    "                path_x.append(x_new)\n",
    "                break\n",
    "\n",
    "            path_x.append(x_new)\n",
    "            x = x_new\n",
    "            prev_gamma = gamma\n",
    "\n",
    "        return np.array(path_x)\n",
    "\n",
    "    def _backward_run(self, dataset, x0, gamma0):\n",
    "        \"\"\"Run the algorithm backwards from x(0) along -γ_x(0).\"\"\"\n",
    "        x = x0\n",
    "        path_x = [x]\n",
    "        prev_gamma = -gamma0\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            mu = self._local_center_of_mass(dataset, x)\n",
    "            cov = self._local_covariance(dataset, x, mu)\n",
    "            gamma = self._principal_component(cov, prev_vec=prev_gamma)\n",
    "\n",
    "            x_new = mu + self.h * gamma\n",
    "\n",
    "            if np.linalg.norm(mu - x) < self.tol:\n",
    "                path_x.append(x_new)\n",
    "                break\n",
    "\n",
    "            path_x.append(x_new)\n",
    "            x = x_new\n",
    "            prev_gamma = gamma\n",
    "\n",
    "        return np.array(path_x)\n",
    "\n",
    "    def _find_starting_point(self, dataset, start_point):\n",
    "        \"\"\"Ensure starting point is in dataset or choose closest.\"\"\"\n",
    "        if start_point is None:\n",
    "            idx = np.random.choice(len(dataset))\n",
    "            return dataset[idx], idx\n",
    "        else:\n",
    "            diffs = dataset - start_point\n",
    "            dists = np.linalg.norm(diffs, axis=1)\n",
    "            min_idx = np.argmin(dists)\n",
    "            closest_pt = dataset[min_idx]\n",
    "            if not np.allclose(closest_pt, start_point):\n",
    "                print(f\"Starting point not in dataset. Using closest point: {closest_pt}\")\n",
    "            return closest_pt, min_idx\n",
    "\n",
    "    def fit(self, dataset, start_points=None, remove_similar_end_start_points=True):\n",
    "        \"\"\"\n",
    "        Fit LPC on the dataset using possibly multiple starting points.\n",
    "        \"\"\"\n",
    "        dataset = np.array(dataset)\n",
    "        self.paths = []\n",
    "        self.initializations = []\n",
    "\n",
    "        if start_points is None:\n",
    "            start_points = [None]\n",
    "\n",
    "        for sp in start_points:\n",
    "            x0, _ = self._find_starting_point(dataset, sp)\n",
    "\n",
    "            forward_path = self._forward_run(dataset, x0)\n",
    "            if len(forward_path) > 1:\n",
    "                initial_gamma_direction = (forward_path[1] - forward_path[0]) / self.h\n",
    "            else:\n",
    "                initial_gamma_direction = np.zeros(dataset.shape[1])\n",
    "\n",
    "            if np.linalg.norm(initial_gamma_direction) > 0:\n",
    "                backward_path = self._backward_run(dataset, x0, initial_gamma_direction)\n",
    "                full_path = np.vstack([backward_path[::-1], forward_path[1:]])\n",
    "            else:\n",
    "                full_path = forward_path\n",
    "\n",
    "            # Check orientation: which end of the path is closer to the starting point x0?\n",
    "            dist_start_to_first = np.linalg.norm(x0 - full_path[0])\n",
    "            dist_start_to_last = np.linalg.norm(x0 - full_path[-1])\n",
    "\n",
    "            if dist_start_to_last < dist_start_to_first:\n",
    "                # Reverse the path if the last point is closer to the starting point\n",
    "                full_path = full_path[::-1]\n",
    "\n",
    "            if remove_similar_end_start_points:\n",
    "                start_pt = full_path[0]\n",
    "                end_pt = full_path[-1]\n",
    "\n",
    "                dist_to_start = np.linalg.norm(full_path - start_pt, axis=1)\n",
    "                dist_to_end = np.linalg.norm(full_path - end_pt, axis=1)\n",
    "\n",
    "                mask = np.ones(len(full_path), dtype=bool)\n",
    "                mask[(dist_to_start < self.tol) | (dist_to_end < self.tol)] = False\n",
    "                mask[0] = True  # Keep the first point\n",
    "                mask[-1] = True # Keep the last point\n",
    "\n",
    "                full_path = full_path[mask]\n",
    "\n",
    "            self.paths.append(full_path)\n",
    "            self.initializations.append(x0)\n",
    "\n",
    "        self._fit_cubic_splines_eq()\n",
    "        self._compute_equal_arc_length_spline_points()  # Compute equal arc-length points\n",
    "        return self.paths\n",
    "\n",
    "    def _fit_cubic_splines_eq(self):\n",
    "        \"\"\"Fit cubic splines (equations) for all paths.\"\"\"\n",
    "        self.cubic_splines_eq = []\n",
    "        for path in self.paths:\n",
    "            if len(path) < 4:\n",
    "                self.cubic_splines_eq.append(None)\n",
    "                continue\n",
    "            t = np.arange(len(path))\n",
    "            splines_dict = {}\n",
    "            for dim in range(path.shape[1]):\n",
    "                splines_dict[dim] = CubicSpline(t, path[:, dim])\n",
    "            self.cubic_splines_eq.append(splines_dict)\n",
    "\n",
    "    def _compute_cubic_spline_points(self, num_points=500):\n",
    "        \"\"\"\n",
    "        Compute parameterized points from each cubic spline equation.\n",
    "        This fills self.cubic_splines with arrays of evaluated points.\n",
    "        \"\"\"\n",
    "        self.cubic_splines = []\n",
    "        for i, eq in enumerate(self.cubic_splines_eq):\n",
    "            if eq is None:\n",
    "                self.cubic_splines.append(None)\n",
    "                continue\n",
    "            path = self.paths[i]\n",
    "            t_values = np.linspace(0, len(path) - 1, num_points)\n",
    "            spline_points = self.evaluate_cubic_spline(i, t_values)\n",
    "            self.cubic_splines.append(spline_points)\n",
    "\n",
    "    def evaluate_cubic_spline(self, path_idx, t_values):\n",
    "        \"\"\"Evaluate the cubic spline equation for a specific path at given parameter values.\"\"\"\n",
    "        if path_idx >= len(self.cubic_splines_eq) or self.cubic_splines_eq[path_idx] is None:\n",
    "            raise ValueError(f\"No cubic spline found for path index {path_idx}.\")\n",
    "        spline = self.cubic_splines_eq[path_idx]\n",
    "        points = np.array([spline[dim](t_values) for dim in range(len(spline))]).T\n",
    "        return points\n",
    "\n",
    "    def compute_arc_length(self, spline, t_min, t_max, num_samples=10000):\n",
    "        \"\"\"\n",
    "        Approximate the arc length of the spline by sampling it finely.\n",
    "        spline: dict of {dim: CubicSpline} for the given path.\n",
    "        t_min, t_max: the parameter range of the spline.\n",
    "        num_samples: number of samples to approximate arc length.\n",
    "        Returns:\n",
    "            t_values: the parameter values used for sampling\n",
    "            cumulative_length: cumulative arc length corresponding to t_values\n",
    "        \"\"\"\n",
    "        t_values = np.linspace(t_min, t_max, num_samples)\n",
    "        points = np.array([spline[dim](t_values) for dim in range(len(spline))]).T\n",
    "\n",
    "        distances = np.sqrt(np.sum(np.diff(points, axis=0)**2, axis=1))\n",
    "        cumulative_length = np.insert(np.cumsum(distances), 0, 0.0)\n",
    "        return t_values, cumulative_length\n",
    "\n",
    "    def get_uniformly_spaced_points(self, spline, num_points):\n",
    "        \"\"\"\n",
    "        Given a fitted spline (as a dict {dim: CubicSpline}), \n",
    "        return points spaced equally by arc length.\n",
    "        \"\"\"\n",
    "        # Parameter range: assuming t goes from 0 to (path_length - 1)\n",
    "        path_length = len(spline[0].x)\n",
    "        t_min = 0\n",
    "        t_max = path_length - 1\n",
    "\n",
    "        t_vals_dense, cum_length = self.compute_arc_length(spline, t_min, t_max, num_samples=5000)\n",
    "        total_length = cum_length[-1]\n",
    "\n",
    "        # Desired distances\n",
    "        desired_distances = np.linspace(0, total_length, num_points)\n",
    "\n",
    "        # Interpolate t-values for these distances\n",
    "        t_for_dist = interp1d(cum_length, t_vals_dense, kind='linear')(desired_distances)\n",
    "\n",
    "        # Evaluate spline at these t-values\n",
    "        uniform_points = np.array([spline[dim](t_for_dist) for dim in range(len(spline))]).T\n",
    "\n",
    "        return uniform_points\n",
    "\n",
    "    def _compute_equal_arc_length_spline_points(self, num_points=500):\n",
    "        \"\"\"\n",
    "        Compute points along each cubic spline that are equally spaced by arc length.\n",
    "        \"\"\"\n",
    "        self.cubic_splines = []\n",
    "        for i, eq in enumerate(self.cubic_splines_eq):\n",
    "            if eq is None:\n",
    "                self.equal_spaced_splines.append(None)\n",
    "                continue\n",
    "            spline_points = self.get_uniformly_spaced_points(eq, num_points)\n",
    "            self.cubic_splines.append(spline_points)\n",
    "\n",
    "    def plot_path_3d(self, path_idx=0,dataset=None):\n",
    "        \"\"\"Plot dataset and one LPC path in 3D for visualization.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "        dataset = np.array(dataset)\n",
    "        path = self.paths[path_idx]\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        if dataset: #plot dataset if one is give, usually what you fit onto\n",
    "            ax.scatter(dataset[:,0], dataset[:,1], dataset[:,2], alpha=0.5, label='Data')\n",
    "        ax.plot(path[:,0], path[:,1], path[:,2], 'r-', label='Local Principal Curve')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_cubic_spline_3d(self, path_idx, show_path=True):\n",
    "        \"\"\"\n",
    "        Plot the cubic spline for a specific path in 3D.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "        if path_idx >= len(self.paths):\n",
    "            raise IndexError(f\"Path index {path_idx} is out of range. Total paths: {len(self.paths)}.\")\n",
    "\n",
    "        path = self.cubic_splines[path_idx]\n",
    "\n",
    "        spline_points = self.cubic_splines[path_idx]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "        if show_path:\n",
    "            ax.scatter(path[:, 0], path[:, 1], path[:, 2], label=\"LPC Path\", alpha=0.5)\n",
    "\n",
    "        ax.plot(spline_points[:, 0], spline_points[:, 1], spline_points[:, 2], color=\"red\", label=\"Cubic Spline\")\n",
    "        ax.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Extract the Splines and perform Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spline(splines_df, dataset_label, perturbation):\n",
    "    sdf = splines_df[(splines_df[\"dataset\"] == dataset_label) & (splines_df[\"Perturbation\"] == perturbation)]\n",
    "    sdf = sdf.sort_values(\"point_index\")\n",
    "    points = sdf[[\"PCA_1\", \"PCA_2\", \"PCA_3\"]].values\n",
    "    return points\n",
    "\n",
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean((a - b)**2))\n",
    "\n",
    "def mean_l1_error(a, b):\n",
    "# a and b are Nx3 arrays of points.\n",
    "# Compute L1 distance for each point pair: sum of absolute differences across coordinates\n",
    "# Then take the mean over all points.\n",
    "    return np.mean(np.sum(np.abs(a - b), axis=1))\n",
    "    \n",
    "def centroid(X):\n",
    "    return np.mean(X, axis=0)\n",
    "\n",
    "def rmse(X, Y):\n",
    "    return np.sqrt(np.mean(np.sum((X - Y)**2, axis=1)))\n",
    "\n",
    "def quaternion_alignment(P, Q):\n",
    "    \"\"\"\n",
    "    Compute the optimal rotation using quaternions that aligns Q onto P.\n",
    "    Returns rotation matrix R and translation vector t.\n",
    "    \"\"\"\n",
    "    # Ensure P and Q have the same shape\n",
    "    assert P.shape == Q.shape, \"P and Q must have the same shape\"\n",
    "    \n",
    "    # 1. Compute centroids and center the points\n",
    "    P_cent = centroid(P)\n",
    "    Q_cent = centroid(Q)\n",
    "    P_prime = P - P_cent\n",
    "    Q_prime = Q - Q_cent\n",
    "    \n",
    "    # 2. Construct correlation matrix M\n",
    "    M = Q_prime.T @ P_prime\n",
    "    \n",
    "    # 3. Construct the Kearsley (Davenport) 4x4 matrix K\n",
    "    # Refer to the equations above\n",
    "    A = np.array([\n",
    "        [ M[0,0]+M[1,1]+M[2,2],   M[1,2]-M[2,1],         M[2,0]-M[0,2],         M[0,1]-M[1,0]       ],\n",
    "        [ M[1,2]-M[2,1],         M[0,0]-M[1,1]-M[2,2],  M[0,1]+M[1,0],         M[0,2]+M[2,0]       ],\n",
    "        [ M[2,0]-M[0,2],         M[0,1]+M[1,0],         M[1,1]-M[0,0]-M[2,2],  M[1,2]+M[2,1]       ],\n",
    "        [ M[0,1]-M[1,0],         M[0,2]+M[2,0],         M[1,2]+M[2,1],         M[2,2]-M[0,0]-M[1,1]]\n",
    "    ], dtype=np.float64)\n",
    "    A = A / 3.0\n",
    "    \n",
    "    # 4. Find the eigenvector of A with the highest eigenvalue\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(A)\n",
    "    max_idx = np.argmax(eigenvalues)\n",
    "    q = eigenvectors[:, max_idx]\n",
    "    q = q / np.linalg.norm(q)\n",
    "    \n",
    "    # 5. Convert quaternion q into rotation matrix R\n",
    "    # Quaternion format: q = [q0, q1, q2, q3]\n",
    "    q0, q1, q2, q3 = q\n",
    "    R = np.array([\n",
    "        [q0**2 + q1**2 - q2**2 - q3**2, 2*(q1*q2 - q0*q3),         2*(q1*q3 + q0*q2)],\n",
    "        [2*(q2*q1 + q0*q3),             q0**2 - q1**2 + q2**2 - q3**2, 2*(q2*q3 - q0*q1)],\n",
    "        [2*(q3*q1 - q0*q2),             2*(q3*q2 + q0*q1),             q0**2 - q1**2 - q2**2 + q3**2]\n",
    "    ])\n",
    "    \n",
    "    # 6. Compute translation\n",
    "    t = P_cent - R @ Q_cent\n",
    "    \n",
    "    return R, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/results/20241130/sweep_analysis/paired_models_and_metrics_df.csv\")\n",
    "merged_df_avg = merged_df[merged_df[\"Perturbation\"]==\"avg_pert\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "0.713932407655193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/7583594.1.trapnell-login.q/ipykernel_1628571/1082928410.py:34: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_all = pd.read_csv(path_all)\n",
      "/tmp/7583594.1.trapnell-login.q/ipykernel_1628571/1082928410.py:35: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_hld = pd.read_csv(path_hld)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA plot model_idx 71: F1 score 0.71,mweight 25, timeonly 1\n",
      "Processing wnt-i in all dataset...\n",
      "Subset size: 187\n",
      "Starting point not in dataset. Using closest point: [ 1.82168643  1.29012443 -0.26144278]\n",
      "Processing tgfb-i in all dataset...\n",
      "Subset size: 245\n",
      "Starting point not in dataset. Using closest point: [ 1.83927206  1.28596941 -0.29201842]\n",
      "Processing wt in all dataset...\n",
      "Subset size: 2437\n",
      "Starting point not in dataset. Using closest point: [ 1.76878228  1.7100162  -0.174527  ]\n",
      "Processing lmx1b in all dataset...\n",
      "Subset size: 776\n",
      "Starting point not in dataset. Using closest point: [ 1.84002696  1.34803321 -0.23661037]\n",
      "Processing gdf3 in all dataset...\n",
      "Subset size: 747\n",
      "Starting point not in dataset. Using closest point: [ 1.82765235  1.39959051 -0.05014554]\n",
      "Processing wnt-i in hld dataset...\n",
      "Subset size: 187\n",
      "Starting point not in dataset. Using closest point: [ 1.99630838  1.3154439  -0.20895961]\n",
      "Processing tgfb-i in hld dataset...\n",
      "Subset size: 245\n",
      "Starting point not in dataset. Using closest point: [ 2.04486115  1.44040819 -0.25376278]\n",
      "Processing wt in hld dataset...\n",
      "Subset size: 2437\n",
      "Starting point not in dataset. Using closest point: [ 1.96912577  1.7797467  -0.14410615]\n",
      "Processing lmx1b in hld dataset...\n",
      "Subset size: 776\n",
      "Starting point not in dataset. Using closest point: [ 2.00609791  1.37712739 -0.25303388]\n",
      "Processing gdf3 in hld dataset...\n",
      "Subset size: 747\n",
      "Starting point not in dataset. Using closest point: [ 2.01171486  1.3084113  -0.10475443]\n",
      "77\n",
      "0.7818578623821643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/7583594.1.trapnell-login.q/ipykernel_1628571/1082928410.py:34: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_all = pd.read_csv(path_all)\n",
      "/tmp/7583594.1.trapnell-login.q/ipykernel_1628571/1082928410.py:35: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_hld = pd.read_csv(path_hld)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA plot model_idx 77: F1 score 0.78,mweight 50, timeonly 0\n",
      "Processing wnt-i in all dataset...\n",
      "Subset size: 187\n",
      "Starting point not in dataset. Using closest point: [ 1.95702147  1.03924324 -0.65932128]\n",
      "Processing tgfb-i in all dataset...\n",
      "Subset size: 245\n",
      "Starting point not in dataset. Using closest point: [ 2.00553128  1.20161037 -0.74285054]\n",
      "Processing wt in all dataset...\n",
      "Subset size: 2437\n",
      "Starting point not in dataset. Using closest point: [ 1.64973178  1.44043901 -0.50209158]\n",
      "Processing lmx1b in all dataset...\n",
      "Subset size: 776\n",
      "Starting point not in dataset. Using closest point: [ 1.92653635  1.1202651  -0.60469241]\n",
      "Processing gdf3 in all dataset...\n",
      "Subset size: 747\n",
      "Starting point not in dataset. Using closest point: [ 1.67751863  0.91676987 -0.87060139]\n",
      "Processing wnt-i in hld dataset...\n",
      "Subset size: 187\n",
      "Starting point not in dataset. Using closest point: [-1.76951613  1.13909611  1.3465403 ]\n",
      "Processing tgfb-i in hld dataset...\n",
      "Subset size: 245\n",
      "Starting point not in dataset. Using closest point: [-1.86735925  0.6553896   1.52865182]\n",
      "Processing wt in hld dataset...\n",
      "Subset size: 2437\n",
      "Starting point not in dataset. Using closest point: [-1.55947915 -0.0294546   2.65161843]\n",
      "Processing lmx1b in hld dataset...\n",
      "Subset size: 776\n",
      "Starting point not in dataset. Using closest point: [-1.68734697  0.94254896  1.89794993]\n",
      "Processing gdf3 in hld dataset...\n",
      "Subset size: 747\n",
      "Starting point not in dataset. Using closest point: [-1.67292771  0.06052909  1.93294213]\n",
      "78\n",
      "0.7591386184650808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/7583594.1.trapnell-login.q/ipykernel_1628571/1082928410.py:34: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_all = pd.read_csv(path_all)\n",
      "/tmp/7583594.1.trapnell-login.q/ipykernel_1628571/1082928410.py:35: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_hld = pd.read_csv(path_hld)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA plot model_idx 78: F1 score 0.76,mweight 50, timeonly 1\n",
      "Processing wnt-i in all dataset...\n",
      "Subset size: 187\n",
      "Starting point not in dataset. Using closest point: [-1.54920605  6.35138069  1.18860077]\n",
      "Processing tgfb-i in all dataset...\n",
      "Subset size: 245\n",
      "Starting point not in dataset. Using closest point: [-1.69979066  5.84492894  0.92313777]\n",
      "Processing wt in all dataset...\n",
      "Subset size: 2437\n",
      "Starting point not in dataset. Using closest point: [-3.93511559  7.42517696  7.88852508]\n",
      "Processing lmx1b in all dataset...\n",
      "Subset size: 776\n",
      "Starting point not in dataset. Using closest point: [-0.78182236  6.34060658  4.69437493]\n",
      "Processing gdf3 in all dataset...\n",
      "Subset size: 747\n",
      "Starting point not in dataset. Using closest point: [-2.6893847   6.23954375  3.61233928]\n",
      "Processing wnt-i in hld dataset...\n",
      "Subset size: 187\n",
      "Starting point not in dataset. Using closest point: [-2.08180227  6.37145221  0.02439552]\n",
      "Processing tgfb-i in hld dataset...\n",
      "Subset size: 245\n",
      "Starting point not in dataset. Using closest point: [-1.87721075  6.90072798 -0.28544295]\n",
      "Processing wt in hld dataset...\n",
      "Subset size: 2437\n",
      "Starting point not in dataset. Using closest point: [-0.9679374  10.1503459  -1.54550902]\n",
      "Processing lmx1b in hld dataset...\n",
      "Subset size: 776\n",
      "Starting point not in dataset. Using closest point: [-1.22940281  6.78803092 -0.77204936]\n",
      "Processing gdf3 in hld dataset...\n",
      "Subset size: 747\n",
      "Starting point not in dataset. Using closest point: [-1.20448715  7.84319926 -0.24550455]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# pert_comparisons = [\"wnt-i\", \"wt\"]\n",
    "pert_comparisons = [\"wnt-i\", \"tgfb-i\", \"wt\", \"lmx1b\", \"gdf3\"]\n",
    "\n",
    "color_map = {\n",
    "    \"wnt-i\": \"red\",\n",
    "    \"tgfb-i\": \"green\",\n",
    "    \"wt\": \"blue\",\n",
    "    \"lmx1b\": \"orange\",\n",
    "    \"gdf3\": \"purple\"\n",
    "}\n",
    "\n",
    "####                        ####\n",
    "#### HERE WE LOAD  DATA     ####\n",
    "####                        ####    \n",
    "splines_final_dict = {}\n",
    "scaffold_align_metrics = []\n",
    "\n",
    "for model_index in [71,77,78]:\n",
    "    print(model_index)\n",
    "    path_all = merged_df_avg[merged_df_avg[\"model_index\"]==model_index][\"embryo_df_path_nohld\"].iloc[0]\n",
    "    path_hld = merged_df_avg[merged_df_avg[\"model_index\"]==model_index][\"embryo_df_path_hld\"].iloc[0]\n",
    "\n",
    "    print(merged_df_avg[merged_df_avg[\"model_index\"]==model_index][\"F1_score_all\"].iloc[0])\n",
    "\n",
    "    score    = merged_df_avg[merged_df_avg[\"model_index\"]==model_index][\"F1_score_all\"].iloc[0]\n",
    "    mweight  = merged_df_avg[merged_df_avg[\"model_index\"]==model_index][\"metric_weight\"].iloc[0]\n",
    "    timeonly = merged_df_avg[merged_df_avg[\"model_index\"]==model_index][\"time_only_flag\"].iloc[0]\n",
    " \n",
    "    df_all = pd.read_csv(path_all)\n",
    "    df_hld = pd.read_csv(path_hld)\n",
    "\n",
    "    title = f\"PCA plot model_idx {model_index}: F1 score {score:.2f},mweight {mweight}, timeonly {timeonly}\"\n",
    "\n",
    "    print(title)\n",
    "\n",
    "    # Define the comparisons (Multiclass) and obtain coloumns for data\n",
    "\n",
    "    z_mu_columns = [col for col in df_all.columns if 'z_mu' in col]    \n",
    "    z_mu_biological_columns = [col for col in z_mu_columns if \"b\" in col]\n",
    "\n",
    "\n",
    "    # Dictionary to store spline points for each dataset and perturbation\n",
    "    # Key: (dataset_label, perturbation), Value: array of spline points shape (num_spline_points, 3)\n",
    "    splines_dict = {}\n",
    "\n",
    "\n",
    "    # Replace this with your actual class or method for fitting local principal curves\n",
    "\n",
    "    ####                               ####\n",
    "    ####   HERE WE FIT THE SPLINE      ####\n",
    "    ####   (start from ealy timepoint) ####    \n",
    "\n",
    "    for df_label, df in [(\"all\", df_all), (\"hld\", df_hld)]:\n",
    "    \n",
    "        X = df[z_mu_biological_columns].values\n",
    "        pca = PCA(n_components=3)\n",
    "        pcs = pca.fit_transform(X)\n",
    "\n",
    "        perturbations = pert_comparisons\n",
    "\n",
    "        # Map perturbations to colors\n",
    "        if perturbations is None:\n",
    "            perturbations = df['phenotype'].unique()\n",
    "        color_discrete_map = {pert: px.colors.qualitative.Plotly[i % 10] for i, pert in enumerate(perturbations)}\n",
    "\n",
    "        # Prepare the color array\n",
    "        df['color'] = df['phenotype'].map(color_discrete_map)\n",
    "\n",
    "\n",
    "        df[\"PCA_1\"] = pcs[:,0]\n",
    "        df[\"PCA_2\"] = pcs[:,1]\n",
    "        df[\"PCA_3\"] = pcs[:,2]\n",
    "\n",
    "        for pert in pert_comparisons:\n",
    "            print(f\"Processing {pert} in {df_label} dataset...\")\n",
    "            \n",
    "            \n",
    "            ##################\n",
    "            #### First extact  pert and subset points for computational reason\n",
    "            pert_df = df[df[\"phenotype\"] == pert].reset_index(drop=True)\n",
    "\n",
    "            # Calcualte ealy time point\n",
    "            avg_early_timepoint = pert_df[\n",
    "                (pert_df[\"predicted_stage_hpf\"] >= pert_df[\"predicted_stage_hpf\"].min()) &\n",
    "                (pert_df[\"predicted_stage_hpf\"] < pert_df[\"predicted_stage_hpf\"].min() + 1)\n",
    "            ][[\"PCA_1\", \"PCA_2\", \"PCA_3\"]].mean().values\n",
    "\n",
    "            # Downsampling logic\n",
    "            if pert == \"wt\":\n",
    "                pert_df_subset = pert_df.sample(frac=0.05, random_state=42)\n",
    "            else:\n",
    "                pert_df_subset = pert_df.sample(frac=.1, random_state=42)\n",
    "            \n",
    "            print(f\"Subset size: {len(pert_df_subset)}\")\n",
    "\n",
    "            pert_3d_subset = pert_df_subset[[\"PCA_1\", \"PCA_2\", \"PCA_3\"]].values\n",
    "            ##################\n",
    "\n",
    "            # Fit the Local Principal Curve on the subset\n",
    "            lpc = LocalPrincipalCurve(bandwidth=.5, max_iter=500, tol=1e-4, angle_penalty_exp=2)\n",
    "            paths = lpc.fit(pert_3d_subset, start_points=[avg_early_timepoint],remove_similar_end_start_points=True)\n",
    "\n",
    "            # Extract the first path (assuming one main path)\n",
    "            spline_points = lpc.cubic_splines[0]  # shape: (num_points, 3)\n",
    "            # lpc.plot_cubic_spline_3d(0)\n",
    "\n",
    "            # Store the spline points in the dictionary\n",
    "            splines_dict[(df_label, pert)] = spline_points\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for (df_label, pert), spline_points in splines_dict.items():\n",
    "        # spline_points is an array of shape (num_points, 3)\n",
    "        for i, point in enumerate(spline_points[::-1], start=1):\n",
    "            # point is [PCA_1, PCA_2, PCA_3]\n",
    "            num_points = len(spline_points)\n",
    "            rows.append({\n",
    "                \"dataset\": df_label,\n",
    "                \"Perturbation\": pert,\n",
    "                \"point_index\":  num_points - i,\n",
    "                \"PCA_1\": point[0],\n",
    "                \"PCA_2\": point[1],\n",
    "                \"PCA_3\": point[2]\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    splines_df = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "    ####                               ####\n",
    "    #### HERE WE ALIGN AND MEASURE FIT ####\n",
    "    ####                               ####  \n",
    "\n",
    "    splines_dict_aligned = []\n",
    "\n",
    "    all_combined = []\n",
    "    hld_combined = []\n",
    "    hld_aligned_combined = []\n",
    "\n",
    "\n",
    "    #### calculate values for all perts and then the scaffold ####\n",
    "    for pert in pert_comparisons:\n",
    "        all_points = extract_spline(splines_df, \"all\", pert)\n",
    "        hld_points = extract_spline(splines_df, \"hld\", pert)\n",
    "\n",
    "        # Perform Kabsch alignment with quaternion calcs\n",
    "        R, t = quaternion_alignment(all_points, hld_points)\n",
    "        hld_aligned = (hld_points @ R.T) + t  # Alignment transformation\n",
    "\n",
    "        # Compute initial errors before alignment\n",
    "        initial_rmse = rmse(all_points, hld_points)\n",
    "        # Compute errors after alignment\n",
    "        aligned_rmse = rmse(all_points, hld_aligned)\n",
    "        \n",
    "        # Accumulate for scaffold comparison\n",
    "        all_combined.append(all_points)\n",
    "        hld_combined.append(hld_points)\n",
    "        hld_aligned_combined.append(hld_aligned)\n",
    "        \n",
    "        splines_dict_aligned.append({\"Perturbation\":pert, \"spline\":hld_aligned})\n",
    "        # Record metrics for this perturbation\n",
    "        scaffold_align_metrics.append({\n",
    "            \"model_index\": model_index,\n",
    "            'Perturbation': pert,\n",
    "            'Initial_RMSE': initial_rmse,\n",
    "            'Aligned_RMSE': aligned_rmse\n",
    "        })\n",
    "\n",
    "\n",
    "    # Concatenate all perturbation data for scaffold metrics\n",
    "    all_combined = np.concatenate(all_combined, axis=0)\n",
    "    hld_combined = np.concatenate(hld_combined, axis=0)\n",
    "    hld_aligned_combined = np.concatenate(hld_aligned_combined, axis=0)\n",
    "\n",
    "    # Compute scaffold-level errors\n",
    "    scaffold_initial_rmse = rmse(all_combined, hld_combined)\n",
    "    scaffold_aligned_rmse = rmse(all_combined, hld_aligned_combined)\n",
    "\n",
    "    #apend this \n",
    "    scaffold_align_metrics.append({\n",
    "        \"model_index\": model_index,\n",
    "        'Perturbation': 'avg_pert',\n",
    "        'Initial_RMSE': scaffold_initial_rmse,\n",
    "        'Aligned_RMSE': scaffold_aligned_rmse\n",
    "        })\n",
    "\n",
    "    # @chatgpt make a warning  message if the error doesnt decrease but i dont need to print this out live:\n",
    "    # # Print in the organized format (optional)\n",
    "    # print(f\"Perturbation: {pert}\")\n",
    "    # print(f\"  Initial RMSE: {initial_rmse:.4f}, Aligned RMSE: {aligned_rmse:.4f}\")\n",
    "    # print(f\"  Initial L1:   {initial_l1:.4f}, Aligned L1:   {aligned_l1:.4f}\")\n",
    "    # print(\"\\nOverall Scaffold (Individual Kabsch per Perturbation):\")\n",
    "    # print(f\"  Initial RMSE: {scaffold_initial_rmse:.4f}, Aligned RMSE: {scaffold_aligned_rmse:.4f}\")\n",
    "    # print(f\"  Initial L1:   {scaffold_initial_l1:.4f}, Aligned L1:   {scaffold_aligned_l1:.4f}\")\n",
    "\n",
    "##########\n",
    "\n",
    "    # Convert per-perturbation metrics to a DataFrame\n",
    "    scaffold_align_metrics_df = pd.DataFrame(scaffold_align_metrics)\n",
    "\n",
    "    # row object already created, heres logic to add it to the \n",
    "    for spline in splines_dict_aligned:\n",
    "        # spline_points is an array of shape (num_points, 3)\n",
    "        for i, point in enumerate(spline[\"spline\"]):\n",
    "            # point is [PCA_1, PCA_2, PCA_3]\n",
    "            rows.append({\n",
    "                \"dataset\": \"hld_aligned\",\n",
    "                \"Perturbation\": spline[\"Perturbation\"],\n",
    "                \"point_index\": i,\n",
    "                \"PCA_1\": point[0],\n",
    "                \"PCA_2\": point[1],\n",
    "                \"PCA_3\": point[2],\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    splines_final_df = pd.DataFrame(rows)\n",
    "\n",
    "    splines_final_dict[model_index] = splines_final_df\n",
    "\n",
    "\n",
    "splines_final_df_model_index = pd.concat(\n",
    "[df.assign(model_index=model_index) for model_index, df in splines_final_dict.items()],\n",
    "ignore_index=True\n",
    ")\n",
    "scaffold_align_metrics_df = pd.DataFrame(scaffold_align_metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics other than MSE alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _segment_direction_metrics(data_a, data_b, k=10):\n",
    "    \"\"\"\n",
    "    Compute SegmentColinearity and SegmentCovariance for two given sets of points `data_a` and `data_b`.\n",
    "    Both data_a and data_b are np.ndarray of shape (n, 3).\n",
    "\n",
    "    If there aren't enough points for k segments, returns (np.nan, np.nan).\n",
    "    \"\"\"\n",
    "    min_len = min(len(data_a), len(data_b))\n",
    "    data_a = data_a[:min_len]\n",
    "    data_b = data_b[:min_len]\n",
    "\n",
    "    if min_len < k + 1 or min_len == 0:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Define segments using data_b\n",
    "    segment_indices = np.linspace(0, min_len - 1, k + 1, dtype=int)\n",
    "\n",
    "    aligned_segment_vecs = []\n",
    "    all_segment_vecs = []\n",
    "\n",
    "    for i in range(k):\n",
    "        start_idx = segment_indices[i]\n",
    "        end_idx = segment_indices[i + 1]\n",
    "\n",
    "        start_b = data_b[start_idx]\n",
    "        end_b = data_b[end_idx]\n",
    "\n",
    "        # Find closest points in data_a to start_b and end_b\n",
    "        start_dists = np.linalg.norm(data_a - start_b, axis=1)\n",
    "        closest_start_idx = np.argmin(start_dists)\n",
    "        closest_start_a = data_a[closest_start_idx]\n",
    "\n",
    "        end_dists = np.linalg.norm(data_a - end_b, axis=1)\n",
    "        closest_end_idx = np.argmin(end_dists)\n",
    "        closest_end_a = data_a[closest_end_idx]\n",
    "\n",
    "        # Construct vectors\n",
    "        vec_a = closest_end_a - closest_start_a\n",
    "        vec_b = end_b - start_b\n",
    "\n",
    "        # Normalize\n",
    "        norm_a = np.linalg.norm(vec_a)\n",
    "        norm_b = np.linalg.norm(vec_b)\n",
    "        if norm_a > 0:\n",
    "            vec_a = vec_a / norm_a\n",
    "        else:\n",
    "            vec_a = np.zeros(3)\n",
    "        if norm_b > 0:\n",
    "            vec_b = vec_b / norm_b\n",
    "        else:\n",
    "            vec_b = np.zeros(3)\n",
    "\n",
    "        aligned_segment_vecs.append(vec_a)\n",
    "        all_segment_vecs.append(vec_b)\n",
    "\n",
    "    aligned_segment_vecs = np.array(aligned_segment_vecs)\n",
    "    all_segment_vecs = np.array(all_segment_vecs)\n",
    "\n",
    "    # Cosine similarities\n",
    "    cos_sims = []\n",
    "    for i in range(len(aligned_segment_vecs)):\n",
    "        va = aligned_segment_vecs[i].reshape(1, -1)\n",
    "        vb = all_segment_vecs[i].reshape(1, -1)\n",
    "        sim = cosine_similarity(va, vb)[0][0]\n",
    "        cos_sims.append(sim)\n",
    "\n",
    "    avg_cosine_sim = np.mean(cos_sims) if len(cos_sims) > 0 else np.nan\n",
    "\n",
    "    # Covariances\n",
    "    covariances = []\n",
    "    for dim_idx in range(3):\n",
    "        dim_a = aligned_segment_vecs[:, dim_idx]\n",
    "        dim_b = all_segment_vecs[:, dim_idx]\n",
    "        if len(dim_a) > 1:\n",
    "            cov = np.cov(dim_a, dim_b, bias=True)[0, 1]\n",
    "        else:\n",
    "            cov = np.nan\n",
    "        covariances.append(cov)\n",
    "    avg_cov = np.nanmean(covariances) if len(covariances) > 0 else np.nan\n",
    "\n",
    "    return (avg_cosine_sim, avg_cov)\n",
    "\n",
    "\n",
    "    # Split the dataset into 'all' and 'hld_aligned'\n",
    "    splines_all = splines_final_df[splines_final_df[\"dataset\"] == \"all\"]\n",
    "    splines_hld_aligned = splines_final_df[splines_final_df[\"dataset\"] == \"hld_aligned\"]\n",
    "\n",
    "def segment_direction_consistency(splines_final_df, k=10):\n",
    "    \"\"\"\n",
    "    Step 1 (Across): For each perturbation present in both datasets, compute SegmentColinearity and SegmentCovariance\n",
    "    by comparing splines_hld_aligned and splines_all.\n",
    "\n",
    "    Step 2 (Within): Compute these metrics for all unique pairs of perturbations within each dataset\n",
    "    (both splines_hld_aligned and splines_all separately).\n",
    "    Then compute the mean and std of these pairwise metrics for each dataset.\n",
    "\n",
    "    Returns:\n",
    "    - across_df: DataFrame with ['Perturbation', 'SegmentColinearity', 'SegmentCovariance']\n",
    "    - within_hld_aligned_df: DataFrame with ['Metric', 'Mean', 'Std'] for pairwise metrics within splines_hld_aligned\n",
    "    - within_all_df: DataFrame with ['Metric', 'Mean', 'Std'] for pairwise metrics within splines_all\n",
    "    \"\"\"\n",
    "\n",
    "    splines_all = splines_final_df[splines_final_df[\"dataset\"] == \"all\"]\n",
    "    splines_hld = splines_final_df[splines_final_df[\"dataset\"] == \"hld\"]\n",
    "    splines_hld_aligned = splines_final_df[splines_final_df[\"dataset\"] == \"hld_aligned\"]\n",
    "    \n",
    "    pca_columns = [\"PCA_1\", \"PCA_2\", \"PCA_3\"]\n",
    "    for col in pca_columns:\n",
    "        if col not in splines_hld_aligned.columns or col not in splines_all.columns:\n",
    "            raise ValueError(f\"Missing required PCA column: {col}\")\n",
    "\n",
    "            \n",
    "\n",
    "    # Across computations\n",
    "    perts_aligned = set(splines_hld_aligned[\"Perturbation\"].unique())\n",
    "    perts_all = set(splines_all[\"Perturbation\"].unique())\n",
    "    common_perts = perts_aligned.intersection(perts_all)\n",
    "\n",
    "    across_results = []\n",
    "    for pert in common_perts:\n",
    "        data_a_df = splines_hld_aligned[splines_hld_aligned[\"Perturbation\"] == pert].sort_values(\"point_index\")\n",
    "        data_b_df = splines_all[splines_all[\"Perturbation\"] == pert].sort_values(\"point_index\")\n",
    "        data_a = data_a_df[pca_columns].values\n",
    "        data_b = data_b_df[pca_columns].values\n",
    "\n",
    "        sim, cov = _segment_direction_metrics(data_a, data_b, k=k)\n",
    "        across_results.append({\"Perturbation\": pert, \"SegmentColinearity\": sim, \"SegmentCovariance\": cov})\n",
    "\n",
    "    across_df = pd.DataFrame(across_results)\n",
    "\n",
    "    # Calculate column means (excluding the Perturbation column)\n",
    "    mean_row = across_df.iloc[:, 1:].mean()\n",
    "    mean_row[\"Perturbation\"] = \"avg_pert\"\n",
    "\n",
    "    # Append the mean row to the DataFrame\n",
    "    across_df = pd.concat([across_df, pd.DataFrame([mean_row])], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Within computations for splines_hld\n",
    "    perts_in_aligned = list(perts_aligned)\n",
    "    within_values_colinearity_hld = []\n",
    "    within_values_covariance_hld  = []\n",
    "\n",
    "    for i in range(len(perts_in_aligned)):\n",
    "        for j in range(i+1, len(perts_in_aligned)):\n",
    "            pert1 = perts_in_aligned[i]\n",
    "            pert2 = perts_in_aligned[j]\n",
    "\n",
    "            data_pert1 = splines_hld[splines_hld[\"Perturbation\"] == pert1].sort_values(\"point_index\")[pca_columns].values\n",
    "            data_pert2 = splines_hld[splines_hld[\"Perturbation\"] == pert2].sort_values(\"point_index\")[pca_columns].values\n",
    "\n",
    "            sim, cov = _segment_direction_metrics(data_pert1, data_pert2, k=k)\n",
    "            if not np.isnan(sim):\n",
    "                within_values_colinearity_hld.append(sim)\n",
    "            if not np.isnan(cov):\n",
    "                within_values_covariance_hld.append(cov)\n",
    "\n",
    "    metrics_hld = []\n",
    "    for metric_name, vals in [(\"SegmentColinearity\", within_values_colinearity_hld), \n",
    "                              (\"SegmentCovariance\",  within_values_covariance_hld)]:\n",
    "        mean_val = np.nanmean(vals) if len(vals) > 0 else np.nan\n",
    "        std_val = np.nanstd(vals) if len(vals) > 0 else np.nan\n",
    "        metrics_hld.append({\"Metric\": metric_name, \"Mean\": mean_val, \"Std\": std_val})\n",
    "\n",
    "    within_hld_df = pd.DataFrame(metrics_hld)\n",
    "\n",
    "    # Within computations for splines_all\n",
    "    perts_in_all = list(perts_all)\n",
    "    within_values_colinearity_all = []\n",
    "    within_values_covariance_all = []\n",
    "\n",
    "    for i in range(len(perts_in_all)):\n",
    "        for j in range(i+1, len(perts_in_all)):\n",
    "            pert1 = perts_in_all[i]\n",
    "            pert2 = perts_in_all[j]\n",
    "\n",
    "            data_pert1 = splines_all[splines_all[\"Perturbation\"] == pert1].sort_values(\"point_index\")[pca_columns].values\n",
    "            data_pert2 = splines_all[splines_all[\"Perturbation\"] == pert2].sort_values(\"point_index\")[pca_columns].values\n",
    "\n",
    "            sim, cov = _segment_direction_metrics(data_pert1, data_pert2, k=k)\n",
    "            if not np.isnan(sim):\n",
    "                within_values_colinearity_all.append(sim)\n",
    "            if not np.isnan(cov):\n",
    "                within_values_covariance_all.append(cov)\n",
    "\n",
    "    metrics_all_list = []\n",
    "    for metric_name, vals in [(\"SegmentColinearity\", within_values_colinearity_all), \n",
    "                              (\"SegmentCovariance\", within_values_covariance_all)]:\n",
    "        mean_val = np.nanmean(vals) if len(vals) > 0 else np.nan\n",
    "        std_val = np.nanstd(vals) if len(vals) > 0 else np.nan\n",
    "        metrics_all_list.append({\"Metric\": metric_name, \"Mean\": mean_val, \"Std\": std_val})\n",
    "\n",
    "    within_all_df = pd.DataFrame(metrics_all_list)\n",
    "\n",
    "    return across_df, within_hld_df, within_all_df\n",
    "\n",
    "def calculate_dispersion_metrics(splines_final_df, n=5):\n",
    "    \"\"\"\n",
    "    Calculates dispersion metrics for each dataset, including:\n",
    "    - Dispersion Coefficient (slope of dispersion vs. point_index, normalized to [0, 1])\n",
    "    - Initial Dispersion (average dispersion of the first n points)\n",
    "    - Last Dispersion (average dispersion of the last n points)\n",
    "\n",
    "    Parameters:\n",
    "    - splines_final_df (pd.DataFrame): DataFrame containing all PCA trajectories with 'dataset' column.\n",
    "    - n (int): Number of initial and last points to consider for initial and last dispersion.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with columns ['Dataset', 'disp_coefficient', 'dispersion_first_n', 'dispersion_last_n'].\n",
    "    \"\"\"\n",
    "    # Extract subsets\n",
    "    splines_all = splines_final_df[splines_final_df[\"dataset\"] == \"all\"]\n",
    "    splines_hld = splines_final_df[splines_final_df[\"dataset\"] == \"hld\"]\n",
    "    splines_hld_aligned = splines_final_df[splines_final_df[\"dataset\"] == \"hld_aligned\"]\n",
    "\n",
    "    # Ensure PCA columns are present\n",
    "    pca_columns = [\"PCA_1\", \"PCA_2\", \"PCA_3\"]\n",
    "    for col in pca_columns:\n",
    "        if col not in splines_final_df.columns:\n",
    "            raise ValueError(f\"Missing required PCA column: {col}\")\n",
    "\n",
    "    # Get unique datasets\n",
    "    datasets = splines_final_df[\"dataset\"].unique()\n",
    "\n",
    "    # Initialize list to store results\n",
    "    results = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        if dataset == \"hld_aligned\":\n",
    "            continue\n",
    "        # Filter data for the current dataset\n",
    "        dataset_df = splines_final_df[splines_final_df[\"dataset\"] == dataset]\n",
    "\n",
    "        # Get unique point_indices\n",
    "        point_indices = sorted(dataset_df[\"point_index\"].unique())\n",
    "\n",
    "        # Initialize lists to store dispersion and point_index\n",
    "        dispersion_list = []\n",
    "        point_index_list = []\n",
    "\n",
    "        # Initialize lists to store initial and last dispersions\n",
    "        initial_dispersions = []\n",
    "        last_dispersions = []\n",
    "\n",
    "        for pid in point_indices:\n",
    "            # Filter data for the current point_index\n",
    "            point_df = dataset_df[dataset_df[\"point_index\"] == pid]\n",
    "\n",
    "            # Calculate dispersion: average Euclidean distance from centroid\n",
    "            dispersion = compute_dispersion(point_df, pca_columns)\n",
    "\n",
    "            # Append to lists\n",
    "            dispersion_list.append(dispersion)\n",
    "            point_index_list.append(pid)\n",
    "\n",
    "            # If within first n points, store for initial dispersion\n",
    "            if pid < n:\n",
    "                initial_dispersions.append(dispersion)\n",
    "\n",
    "            # If within last n points, store for last dispersion\n",
    "            if pid >= max(point_indices) - n + 1:\n",
    "                last_dispersions.append(dispersion)\n",
    "\n",
    "        # Check if there are enough points for regression\n",
    "        if len(point_index_list) < 2:\n",
    "            print(f\"Warning: Dataset '{dataset}' has less than 2 unique point_indices. Setting disp_coefficient to NaN.\")\n",
    "            disp_coefficient = np.nan\n",
    "        else:\n",
    "            # Prepare data for linear regression\n",
    "            X = np.array(point_index_list).reshape(-1, 1)  # Shape: (num_points, 1)\n",
    "            y = np.array(dispersion_list)  # Shape: (num_points,)\n",
    "\n",
    "            # Fit linear regression\n",
    "            reg = LinearRegression().fit(X, y)\n",
    "            disp_coefficient = reg.coef_[0]\n",
    "            disp_coefficient *= len(point_indices)  # Normalize to [0, 1]\n",
    "\n",
    "        # Calculate average initial dispersion\n",
    "        dispersion_first_n = np.mean(initial_dispersions) if initial_dispersions else np.nan\n",
    "        if np.isnan(dispersion_first_n):\n",
    "            print(f\"Warning: Dataset '{dataset}' has no points within the first {n} point_indices.\")\n",
    "\n",
    "        # Calculate average last dispersion\n",
    "        dispersion_last_n = np.mean(last_dispersions) if last_dispersions else np.nan\n",
    "        if np.isnan(dispersion_last_n):\n",
    "            print(f\"Warning: Dataset '{dataset}' has no points within the last {n} point_indices.\")\n",
    "\n",
    "        # Append results\n",
    "        results.append({\n",
    "            \"Dataset\": dataset,\n",
    "            \"disp_coefficient\": disp_coefficient,\n",
    "            \"dispersion_first_n\": dispersion_first_n,\n",
    "            \"dispersion_last_n\": dispersion_last_n\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def compute_dispersion(df, pca_columns):\n",
    "    \"\"\"\n",
    "    Computes the average Euclidean distance of points from their centroid.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing PCA coordinates.\n",
    "    - pca_columns (list): List of PCA column names.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Average Euclidean distance (dispersion).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate centroid\n",
    "    centroid = df[pca_columns].mean().values\n",
    "    \n",
    "    # Calculate Euclidean distances from centroid\n",
    "    distances = np.linalg.norm(df[pca_columns].values - centroid, axis=1)\n",
    "    \n",
    "    # Return average distance\n",
    "    return distances.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "Metric  model_index  SegmentColinearity_mean_within_hld  \\\n",
      "Mean             71                            0.746983   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_hld  SegmentColinearity_mean_within_all  \\\n",
      "Mean                             0.172636                            0.777472   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_all  \n",
      "Mean                             0.165758  \n",
      "Dispersion Metrics for Each Dataset:\n",
      "   model_index  disp_coefficient_all  dispersion_first_n_all  \\\n",
      "0           71              1.358857                0.116642   \n",
      "\n",
      "   dispersion_last_n_all  disp_coefficient_hld  dispersion_first_n_hld  \\\n",
      "0               1.357972              1.514239                0.101214   \n",
      "\n",
      "   dispersion_last_n_hld  \n",
      "0               1.592155  \n",
      "77\n",
      "Metric  model_index  SegmentColinearity_mean_within_hld  \\\n",
      "Mean             77                            0.500365   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_hld  SegmentColinearity_mean_within_all  \\\n",
      "Mean                             0.106559                            0.600711   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_all  \n",
      "Mean                             0.135649  \n",
      "Dispersion Metrics for Each Dataset:\n",
      "   model_index  disp_coefficient_all  dispersion_first_n_all  \\\n",
      "0           77              1.751113                 0.34721   \n",
      "\n",
      "   dispersion_last_n_all  disp_coefficient_hld  dispersion_first_n_hld  \\\n",
      "0               1.716981              1.388928                0.574597   \n",
      "\n",
      "   dispersion_last_n_hld  \n",
      "0                2.06553  \n",
      "78\n",
      "Metric  model_index  SegmentColinearity_mean_within_hld  \\\n",
      "Mean             78                            0.350225   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_hld  SegmentColinearity_mean_within_all  \\\n",
      "Mean                             0.099266                            0.242118   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_all  \n",
      "Mean                             0.062459  \n",
      "Dispersion Metrics for Each Dataset:\n",
      "   model_index  disp_coefficient_all  dispersion_first_n_all  \\\n",
      "0           78              2.245373                4.585672   \n",
      "\n",
      "   dispersion_last_n_all  disp_coefficient_hld  dispersion_first_n_hld  \\\n",
      "0               6.168105              4.360539                1.747128   \n",
      "\n",
      "   dispersion_last_n_hld  \n",
      "0               7.569243  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key in splines_final_df_model_index[\"model_index\"].unique():\n",
    "    print(key)\n",
    "    splines_final_df = splines_final_df_model_index[splines_final_df_model_index[\"model_index\"] == key]\n",
    "\n",
    "    splines_all = splines_final_df[splines_final_df[\"dataset\"]==\"all\"]\n",
    "    splines_hld_aligned = splines_final_df[splines_final_df[\"dataset\"]==\"hld_aligned\"]\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Analyze Datasets\n",
    "    # -------------------------------------\n",
    "    # Segment Direction Consistency\n",
    "    # After loading or creating splines_hld_aligned and splines_all DataFrames:\n",
    "    across_seg_df, within_hld_seg_df, within_all_seg_df = segment_direction_consistency(splines_final_df, k=100)\n",
    "    \n",
    "\n",
    "    # Add the model_index column with the key value\n",
    "    across_seg_df.insert(0, \"model_index\", key)\n",
    "\n",
    "    # Create a new DataFrame to collect renamed columns\n",
    "    within_hld_renamed = within_hld_seg_df[[\"Metric\", \"Mean\"]].copy()\n",
    "    within_hld_renamed[\"Metric\"] += \"_mean_within_hld\"  # Add suffix\n",
    "    within_hld_renamed = within_hld_renamed.set_index(\"Metric\").T  # Transpose for easy appending\n",
    "\n",
    "    within_all_renamed = within_all_seg_df[[\"Metric\", \"Mean\"]].copy()\n",
    "    within_all_renamed[\"Metric\"] += \"_mean_within_all\"  # Add suffix\n",
    "    within_all_renamed = within_all_renamed.set_index(\"Metric\").T  # Transpose for easy appending\n",
    "\n",
    "    # Combine the renamed DataFrames into one row\n",
    "    within_seg_measures = pd.concat([within_hld_renamed, within_all_renamed], axis=1)\n",
    "\n",
    "    within_seg_measures.insert(0, \"model_index\", key)\n",
    "    print(within_seg_measures)\n",
    "\n",
    "    # Calculate Dispersion Metrics\n",
    "    dispersion_metrics_df = calculate_dispersion_metrics(splines_final_df, n=5)\n",
    "\n",
    "\n",
    "    # Split and rename columns based on Dataset\n",
    "    disp_all = dispersion_metrics_df[dispersion_metrics_df[\"Dataset\"] == \"all\"].drop(\"Dataset\", axis=1)\n",
    "    disp_all.columns = [col + \"_all\" for col in disp_all.columns]\n",
    "    disp_all = disp_all.reset_index(drop=True)\n",
    "\n",
    "    disp_hld = dispersion_metrics_df[dispersion_metrics_df[\"Dataset\"] == \"hld\"].drop(\"Dataset\", axis=1)\n",
    "    disp_hld.columns = [col + \"_hld\" for col in disp_hld.columns]\n",
    "    disp_hld = disp_hld.reset_index(drop=True)\n",
    "\n",
    "    # Combine into a single row\n",
    "    combined_dispersion_df = pd.concat([disp_all, disp_hld], axis=1)\n",
    "\n",
    "    # Add the model_index column\n",
    "    combined_dispersion_df.insert(0, \"model_index\", key)\n",
    "\n",
    "    # Final output\n",
    "    print(\"Dispersion Metrics for Each Dataset:\")\n",
    "    print(combined_dispersion_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # fig = plot_trajectories_3d(splines_final_df)\n",
    "    # fig.write_html(f\"/net/trapnell/vol1/home/mdcolon/proj/morphseq/results/20241211/splines_aligned_{key}_quaternion_2.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model_index: 71\n",
      "Segment Direction Measures:\n",
      "Metric  model_index  SegmentColinearity_mean_within_hld  \\\n",
      "Mean             71                            0.746983   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_hld  model_index  \\\n",
      "Mean                             0.172636           71   \n",
      "\n",
      "Metric  SegmentColinearity_mean_within_all  SegmentCovariance_mean_within_all  \n",
      "Mean                              0.777472                           0.165758  \n",
      "Dispersion Metrics for Each Dataset:\n",
      "   model_index  disp_coefficient_all  dispersion_first_n_all  \\\n",
      "0           71              1.358857                0.116642   \n",
      "\n",
      "   dispersion_last_n_all  disp_coefficient_hld  dispersion_first_n_hld  \\\n",
      "0               1.357972              1.514239                0.101214   \n",
      "\n",
      "   dispersion_last_n_hld  \n",
      "0               1.592155  \n",
      "Processing model_index: 77\n",
      "Segment Direction Measures:\n",
      "Metric  model_index  SegmentColinearity_mean_within_hld  \\\n",
      "Mean             77                            0.500365   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_hld  model_index  \\\n",
      "Mean                             0.106559           77   \n",
      "\n",
      "Metric  SegmentColinearity_mean_within_all  SegmentCovariance_mean_within_all  \n",
      "Mean                              0.600711                           0.135649  \n",
      "Dispersion Metrics for Each Dataset:\n",
      "   model_index  disp_coefficient_all  dispersion_first_n_all  \\\n",
      "0           77              1.751113                 0.34721   \n",
      "\n",
      "   dispersion_last_n_all  disp_coefficient_hld  dispersion_first_n_hld  \\\n",
      "0               1.716981              1.388928                0.574597   \n",
      "\n",
      "   dispersion_last_n_hld  \n",
      "0                2.06553  \n",
      "Processing model_index: 78\n",
      "Segment Direction Measures:\n",
      "Metric  model_index  SegmentColinearity_mean_within_hld  \\\n",
      "Mean             78                            0.350225   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_hld  model_index  \\\n",
      "Mean                             0.099266           78   \n",
      "\n",
      "Metric  SegmentColinearity_mean_within_all  SegmentCovariance_mean_within_all  \n",
      "Mean                              0.242118                           0.062459  \n",
      "Dispersion Metrics for Each Dataset:\n",
      "   model_index  disp_coefficient_all  dispersion_first_n_all  \\\n",
      "0           78              2.245373                4.585672   \n",
      "\n",
      "   dispersion_last_n_all  disp_coefficient_hld  dispersion_first_n_hld  \\\n",
      "0               6.168105              4.360539                1.747128   \n",
      "\n",
      "   dispersion_last_n_hld  \n",
      "0               7.569243  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>Perturbation</th>\n",
       "      <th>SegmentColinearity</th>\n",
       "      <th>SegmentCovariance</th>\n",
       "      <th>SegmentColinearity_mean_within_hld</th>\n",
       "      <th>SegmentCovariance_mean_within_hld</th>\n",
       "      <th>SegmentColinearity_mean_within_all</th>\n",
       "      <th>SegmentCovariance_mean_within_all</th>\n",
       "      <th>disp_coefficient_all</th>\n",
       "      <th>dispersion_first_n_all</th>\n",
       "      <th>dispersion_last_n_all</th>\n",
       "      <th>disp_coefficient_hld</th>\n",
       "      <th>dispersion_first_n_hld</th>\n",
       "      <th>dispersion_last_n_hld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>tgfb-i</td>\n",
       "      <td>0.979616</td>\n",
       "      <td>0.183081</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>0.961767</td>\n",
       "      <td>0.145345</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>0.969415</td>\n",
       "      <td>0.195387</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>lmx1b</td>\n",
       "      <td>0.919742</td>\n",
       "      <td>0.245564</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>wt</td>\n",
       "      <td>0.993592</td>\n",
       "      <td>0.285032</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71</td>\n",
       "      <td>avg_pert</td>\n",
       "      <td>0.964826</td>\n",
       "      <td>0.210882</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77</td>\n",
       "      <td>tgfb-i</td>\n",
       "      <td>0.581980</td>\n",
       "      <td>0.132881</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>0.878107</td>\n",
       "      <td>0.066824</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>77</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>0.716081</td>\n",
       "      <td>0.155529</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>77</td>\n",
       "      <td>lmx1b</td>\n",
       "      <td>0.521959</td>\n",
       "      <td>0.159994</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>77</td>\n",
       "      <td>wt</td>\n",
       "      <td>0.789793</td>\n",
       "      <td>0.236710</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>77</td>\n",
       "      <td>avg_pert</td>\n",
       "      <td>0.697584</td>\n",
       "      <td>0.150388</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>78</td>\n",
       "      <td>tgfb-i</td>\n",
       "      <td>0.523307</td>\n",
       "      <td>0.114477</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>78</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>0.515576</td>\n",
       "      <td>0.163390</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>78</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>0.357943</td>\n",
       "      <td>0.115720</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>78</td>\n",
       "      <td>lmx1b</td>\n",
       "      <td>0.467148</td>\n",
       "      <td>0.079519</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78</td>\n",
       "      <td>wt</td>\n",
       "      <td>0.100287</td>\n",
       "      <td>0.020143</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>78</td>\n",
       "      <td>avg_pert</td>\n",
       "      <td>0.392852</td>\n",
       "      <td>0.098650</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_index Perturbation  SegmentColinearity  SegmentCovariance  \\\n",
       "0            71       tgfb-i            0.979616           0.183081   \n",
       "1            71        wnt-i            0.961767           0.145345   \n",
       "2            71         gdf3            0.969415           0.195387   \n",
       "3            71        lmx1b            0.919742           0.245564   \n",
       "4            71           wt            0.993592           0.285032   \n",
       "5            71     avg_pert            0.964826           0.210882   \n",
       "6            77       tgfb-i            0.581980           0.132881   \n",
       "7            77        wnt-i            0.878107           0.066824   \n",
       "8            77         gdf3            0.716081           0.155529   \n",
       "9            77        lmx1b            0.521959           0.159994   \n",
       "10           77           wt            0.789793           0.236710   \n",
       "11           77     avg_pert            0.697584           0.150388   \n",
       "12           78       tgfb-i            0.523307           0.114477   \n",
       "13           78        wnt-i            0.515576           0.163390   \n",
       "14           78         gdf3            0.357943           0.115720   \n",
       "15           78        lmx1b            0.467148           0.079519   \n",
       "16           78           wt            0.100287           0.020143   \n",
       "17           78     avg_pert            0.392852           0.098650   \n",
       "\n",
       "    SegmentColinearity_mean_within_hld  SegmentCovariance_mean_within_hld  \\\n",
       "0                             0.746983                           0.172636   \n",
       "1                             0.746983                           0.172636   \n",
       "2                             0.746983                           0.172636   \n",
       "3                             0.746983                           0.172636   \n",
       "4                             0.746983                           0.172636   \n",
       "5                             0.746983                           0.172636   \n",
       "6                             0.500365                           0.106559   \n",
       "7                             0.500365                           0.106559   \n",
       "8                             0.500365                           0.106559   \n",
       "9                             0.500365                           0.106559   \n",
       "10                            0.500365                           0.106559   \n",
       "11                            0.500365                           0.106559   \n",
       "12                            0.350225                           0.099266   \n",
       "13                            0.350225                           0.099266   \n",
       "14                            0.350225                           0.099266   \n",
       "15                            0.350225                           0.099266   \n",
       "16                            0.350225                           0.099266   \n",
       "17                            0.350225                           0.099266   \n",
       "\n",
       "    SegmentColinearity_mean_within_all  SegmentCovariance_mean_within_all  \\\n",
       "0                             0.777472                           0.165758   \n",
       "1                             0.777472                           0.165758   \n",
       "2                             0.777472                           0.165758   \n",
       "3                             0.777472                           0.165758   \n",
       "4                             0.777472                           0.165758   \n",
       "5                             0.777472                           0.165758   \n",
       "6                             0.600711                           0.135649   \n",
       "7                             0.600711                           0.135649   \n",
       "8                             0.600711                           0.135649   \n",
       "9                             0.600711                           0.135649   \n",
       "10                            0.600711                           0.135649   \n",
       "11                            0.600711                           0.135649   \n",
       "12                            0.242118                           0.062459   \n",
       "13                            0.242118                           0.062459   \n",
       "14                            0.242118                           0.062459   \n",
       "15                            0.242118                           0.062459   \n",
       "16                            0.242118                           0.062459   \n",
       "17                            0.242118                           0.062459   \n",
       "\n",
       "    disp_coefficient_all  dispersion_first_n_all  dispersion_last_n_all  \\\n",
       "0               1.358857                0.116642               1.357972   \n",
       "1               1.358857                0.116642               1.357972   \n",
       "2               1.358857                0.116642               1.357972   \n",
       "3               1.358857                0.116642               1.357972   \n",
       "4               1.358857                0.116642               1.357972   \n",
       "5               1.358857                0.116642               1.357972   \n",
       "6               1.751113                0.347210               1.716981   \n",
       "7               1.751113                0.347210               1.716981   \n",
       "8               1.751113                0.347210               1.716981   \n",
       "9               1.751113                0.347210               1.716981   \n",
       "10              1.751113                0.347210               1.716981   \n",
       "11              1.751113                0.347210               1.716981   \n",
       "12              2.245373                4.585672               6.168105   \n",
       "13              2.245373                4.585672               6.168105   \n",
       "14              2.245373                4.585672               6.168105   \n",
       "15              2.245373                4.585672               6.168105   \n",
       "16              2.245373                4.585672               6.168105   \n",
       "17              2.245373                4.585672               6.168105   \n",
       "\n",
       "    disp_coefficient_hld  dispersion_first_n_hld  dispersion_last_n_hld  \n",
       "0               1.514239                0.101214               1.592155  \n",
       "1               1.514239                0.101214               1.592155  \n",
       "2               1.514239                0.101214               1.592155  \n",
       "3               1.514239                0.101214               1.592155  \n",
       "4               1.514239                0.101214               1.592155  \n",
       "5               1.514239                0.101214               1.592155  \n",
       "6               1.388928                0.574597               2.065530  \n",
       "7               1.388928                0.574597               2.065530  \n",
       "8               1.388928                0.574597               2.065530  \n",
       "9               1.388928                0.574597               2.065530  \n",
       "10              1.388928                0.574597               2.065530  \n",
       "11              1.388928                0.574597               2.065530  \n",
       "12              4.360539                1.747128               7.569243  \n",
       "13              4.360539                1.747128               7.569243  \n",
       "14              4.360539                1.747128               7.569243  \n",
       "15              4.360539                1.747128               7.569243  \n",
       "16              4.360539                1.747128               7.569243  \n",
       "17              4.360539                1.747128               7.569243  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------\n",
    "\n",
    "def rename_within_metrics(df, suffix, key):\n",
    "    \"\"\"Renames columns in within metrics DataFrame with a given suffix.\"\"\"\n",
    "    renamed_df = df[[\"Metric\", \"Mean\"]].copy()\n",
    "    renamed_df[\"Metric\"] += suffix  # Add suffix\n",
    "    renamed_df = renamed_df.set_index(\"Metric\").T  # Transpose for easy appending\n",
    "    renamed_df.insert(0, \"model_index\", key)  # Add model_index\n",
    "    return renamed_df\n",
    "\n",
    "def process_dispersion_metrics(df, key):\n",
    "    \"\"\"Processes and renames dispersion metrics DataFrame.\"\"\"\n",
    "    disp_all = df[df[\"Dataset\"] == \"all\"].drop(\"Dataset\", axis=1)\n",
    "    disp_all.columns = [col + \"_all\" for col in disp_all.columns]\n",
    "    disp_hld = df[df[\"Dataset\"] == \"hld\"].drop(\"Dataset\", axis=1)\n",
    "    disp_hld.columns = [col + \"_hld\" for col in disp_hld.columns]\n",
    "    \n",
    "    combined_df = pd.concat([disp_all.reset_index(drop=True), disp_hld.reset_index(drop=True)], axis=1)\n",
    "    combined_df.insert(0, \"model_index\", key)  # Add model_index\n",
    "    return combined_df\n",
    "\n",
    "def process_segment_direction(splines_final_df, key):\n",
    "    \"\"\"Calculates and processes segment direction consistency metrics.\"\"\"\n",
    "    across_seg_df, within_hld_seg_df, within_all_seg_df = segment_direction_consistency(splines_final_df, k=100)\n",
    "    across_seg_df.insert(0, \"model_index\", key)  # Add model_index\n",
    "    \n",
    "    within_hld_renamed = rename_within_metrics(within_hld_seg_df, \"_mean_within_hld\", key)\n",
    "    within_all_renamed = rename_within_metrics(within_all_seg_df, \"_mean_within_all\", key)\n",
    "    \n",
    "    within_seg_measures = pd.concat([within_hld_renamed, within_all_renamed], axis=1)\n",
    "    return across_seg_df, within_seg_measures\n",
    "\n",
    "def combine_results_dict(results_dict):\n",
    "    \"\"\"\n",
    "    Combines the results dictionary into a single DataFrame.\n",
    "    Handles duplicate 'model_index' columns by ensuring uniqueness during merge.\n",
    "    \"\"\"\n",
    "    final_list_of_dfs = []\n",
    "    \n",
    "    for model_index, metrics in results_dict.items():\n",
    "        # Start with across_seg_df as the base since it has multiple perturbations\n",
    "        if \"across_seg_df\" not in metrics:\n",
    "            continue  # If for some reason this key doesn't have across_seg_df, skip\n",
    "        \n",
    "        base_df = metrics[\"across_seg_df\"].copy()\n",
    "\n",
    "        # Drop duplicate 'model_index' columns from other metrics before merging\n",
    "        if \"within_seg_measures\" in metrics:\n",
    "            temp_within = metrics[\"within_seg_measures\"].copy()\n",
    "            temp_within = temp_within.loc[:, ~temp_within.columns.duplicated()]  # Remove duplicate columns\n",
    "            base_df = base_df.merge(temp_within, on=\"model_index\", how=\"left\")\n",
    "        \n",
    "        if \"dispersion_metrics\" in metrics:\n",
    "            temp_disp = metrics[\"dispersion_metrics\"].copy()\n",
    "            temp_disp = temp_disp.loc[:, ~temp_disp.columns.duplicated()]  # Remove duplicate columns\n",
    "            base_df = base_df.merge(temp_disp, on=\"model_index\", how=\"left\")\n",
    "\n",
    "        # Append to list\n",
    "        final_list_of_dfs.append(base_df)\n",
    "\n",
    "    # Concatenate all model results\n",
    "    if final_list_of_dfs:\n",
    "        final_results_df = pd.concat(final_list_of_dfs, ignore_index=True)\n",
    "    else:\n",
    "        final_results_df = pd.DataFrame()\n",
    "\n",
    "    return final_results_df\n",
    "\n",
    "# Example usage:\n",
    "# results_df = combine_results_dict(results_dict)\n",
    "# This will produce a DataFrame with each row corresponding to a (model_index, Perturbation) pair,\n",
    "# and columns from across_seg_df, within_seg_measures, and dispersion_metrics.\n",
    "\n",
    "# -------------------------------\n",
    "# Main Loop\n",
    "# -------------------------------\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for key in splines_final_df_model_index[\"model_index\"].unique():\n",
    "    print(f\"Processing model_index: {key}\")\n",
    "    \n",
    "    # Filter data for the current model index\n",
    "    splines_final_df = splines_final_df_model_index[splines_final_df_model_index[\"model_index\"] == key]\n",
    "    \n",
    "    # Process segment direction consistency\n",
    "    across_seg_df, within_seg_measures = process_segment_direction(splines_final_df, key)\n",
    "    print(\"Segment Direction Measures:\")\n",
    "    print(within_seg_measures)\n",
    "\n",
    "    # Calculate dispersion metrics\n",
    "    dispersion_metrics_df = calculate_dispersion_metrics(splines_final_df, n=5)\n",
    "    combined_dispersion_df = process_dispersion_metrics(dispersion_metrics_df, key)\n",
    "    print(\"Dispersion Metrics for Each Dataset:\")\n",
    "    print(combined_dispersion_df)\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    results_dict[key] = {\n",
    "        \"across_seg_df\": across_seg_df,\n",
    "        \"within_seg_measures\": within_seg_measures,\n",
    "        \"dispersion_metrics\": combined_dispersion_df\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Access Results Example\n",
    "# -------------------------------\n",
    "results_df = combine_results_dict(results_dict)\n",
    "results_df\n",
    "\n",
    "scaffold_align_metrics_df.merge(results_df, on=[\"model_index\", \"Perturbation\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform the merge on the \"model_index\" column\n",
    "# merged_df = scaffold_align_metrics_df.merge(results_df, on=\"model_index\", how=\"left\")\n",
    "\n",
    "# # Display the merged DataFrame\n",
    "# print(\"Merged DataFrame:\")\n",
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>Perturbation</th>\n",
       "      <th>Initial_RMSE</th>\n",
       "      <th>Aligned_RMSE</th>\n",
       "      <th>SegmentColinearity</th>\n",
       "      <th>SegmentCovariance</th>\n",
       "      <th>SegmentColinearity_mean_within_hld</th>\n",
       "      <th>SegmentCovariance_mean_within_hld</th>\n",
       "      <th>SegmentColinearity_mean_within_all</th>\n",
       "      <th>SegmentCovariance_mean_within_all</th>\n",
       "      <th>disp_coefficient_all</th>\n",
       "      <th>dispersion_first_n_all</th>\n",
       "      <th>dispersion_last_n_all</th>\n",
       "      <th>disp_coefficient_hld</th>\n",
       "      <th>dispersion_first_n_hld</th>\n",
       "      <th>dispersion_last_n_hld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>0.234647</td>\n",
       "      <td>0.186641</td>\n",
       "      <td>0.961767</td>\n",
       "      <td>0.145345</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>tgfb-i</td>\n",
       "      <td>0.292043</td>\n",
       "      <td>0.190456</td>\n",
       "      <td>0.979616</td>\n",
       "      <td>0.183081</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>wt</td>\n",
       "      <td>0.194967</td>\n",
       "      <td>0.147977</td>\n",
       "      <td>0.993592</td>\n",
       "      <td>0.285032</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>lmx1b</td>\n",
       "      <td>0.452274</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>0.919742</td>\n",
       "      <td>0.245564</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>0.281640</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>0.969415</td>\n",
       "      <td>0.195387</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71</td>\n",
       "      <td>avg_pert</td>\n",
       "      <td>0.304050</td>\n",
       "      <td>0.196193</td>\n",
       "      <td>0.964826</td>\n",
       "      <td>0.210882</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>4.639583</td>\n",
       "      <td>0.312645</td>\n",
       "      <td>0.878107</td>\n",
       "      <td>0.066824</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77</td>\n",
       "      <td>tgfb-i</td>\n",
       "      <td>3.798049</td>\n",
       "      <td>0.830231</td>\n",
       "      <td>0.581980</td>\n",
       "      <td>0.132881</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>77</td>\n",
       "      <td>wt</td>\n",
       "      <td>3.238206</td>\n",
       "      <td>0.821606</td>\n",
       "      <td>0.789793</td>\n",
       "      <td>0.236710</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>77</td>\n",
       "      <td>lmx1b</td>\n",
       "      <td>2.721069</td>\n",
       "      <td>1.372218</td>\n",
       "      <td>0.521959</td>\n",
       "      <td>0.159994</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>77</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>3.571621</td>\n",
       "      <td>0.357176</td>\n",
       "      <td>0.716081</td>\n",
       "      <td>0.155529</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>77</td>\n",
       "      <td>avg_pert</td>\n",
       "      <td>3.649591</td>\n",
       "      <td>0.833381</td>\n",
       "      <td>0.697584</td>\n",
       "      <td>0.150388</td>\n",
       "      <td>0.500365</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.600711</td>\n",
       "      <td>0.135649</td>\n",
       "      <td>1.751113</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>1.716981</td>\n",
       "      <td>1.388928</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>2.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>78</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>6.548273</td>\n",
       "      <td>2.478656</td>\n",
       "      <td>0.515576</td>\n",
       "      <td>0.163390</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>78</td>\n",
       "      <td>tgfb-i</td>\n",
       "      <td>4.691686</td>\n",
       "      <td>1.601334</td>\n",
       "      <td>0.523307</td>\n",
       "      <td>0.114477</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>78</td>\n",
       "      <td>wt</td>\n",
       "      <td>11.801723</td>\n",
       "      <td>7.197676</td>\n",
       "      <td>0.100287</td>\n",
       "      <td>0.020143</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>78</td>\n",
       "      <td>lmx1b</td>\n",
       "      <td>5.510354</td>\n",
       "      <td>0.734352</td>\n",
       "      <td>0.467148</td>\n",
       "      <td>0.079519</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>8.889869</td>\n",
       "      <td>5.433229</td>\n",
       "      <td>0.357943</td>\n",
       "      <td>0.115720</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>78</td>\n",
       "      <td>avg_pert</td>\n",
       "      <td>7.919170</td>\n",
       "      <td>4.256144</td>\n",
       "      <td>0.392852</td>\n",
       "      <td>0.098650</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>0.242118</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>2.245373</td>\n",
       "      <td>4.585672</td>\n",
       "      <td>6.168105</td>\n",
       "      <td>4.360539</td>\n",
       "      <td>1.747128</td>\n",
       "      <td>7.569243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_index Perturbation  Initial_RMSE  Aligned_RMSE  SegmentColinearity  \\\n",
       "0            71        wnt-i      0.234647      0.186641            0.961767   \n",
       "1            71       tgfb-i      0.292043      0.190456            0.979616   \n",
       "2            71           wt      0.194967      0.147977            0.993592   \n",
       "3            71        lmx1b      0.452274      0.289614            0.919742   \n",
       "4            71         gdf3      0.281640      0.124806            0.969415   \n",
       "5            71     avg_pert      0.304050      0.196193            0.964826   \n",
       "6            77        wnt-i      4.639583      0.312645            0.878107   \n",
       "7            77       tgfb-i      3.798049      0.830231            0.581980   \n",
       "8            77           wt      3.238206      0.821606            0.789793   \n",
       "9            77        lmx1b      2.721069      1.372218            0.521959   \n",
       "10           77         gdf3      3.571621      0.357176            0.716081   \n",
       "11           77     avg_pert      3.649591      0.833381            0.697584   \n",
       "12           78        wnt-i      6.548273      2.478656            0.515576   \n",
       "13           78       tgfb-i      4.691686      1.601334            0.523307   \n",
       "14           78           wt     11.801723      7.197676            0.100287   \n",
       "15           78        lmx1b      5.510354      0.734352            0.467148   \n",
       "16           78         gdf3      8.889869      5.433229            0.357943   \n",
       "17           78     avg_pert      7.919170      4.256144            0.392852   \n",
       "\n",
       "    SegmentCovariance  SegmentColinearity_mean_within_hld  \\\n",
       "0            0.145345                            0.746983   \n",
       "1            0.183081                            0.746983   \n",
       "2            0.285032                            0.746983   \n",
       "3            0.245564                            0.746983   \n",
       "4            0.195387                            0.746983   \n",
       "5            0.210882                            0.746983   \n",
       "6            0.066824                            0.500365   \n",
       "7            0.132881                            0.500365   \n",
       "8            0.236710                            0.500365   \n",
       "9            0.159994                            0.500365   \n",
       "10           0.155529                            0.500365   \n",
       "11           0.150388                            0.500365   \n",
       "12           0.163390                            0.350225   \n",
       "13           0.114477                            0.350225   \n",
       "14           0.020143                            0.350225   \n",
       "15           0.079519                            0.350225   \n",
       "16           0.115720                            0.350225   \n",
       "17           0.098650                            0.350225   \n",
       "\n",
       "    SegmentCovariance_mean_within_hld  SegmentColinearity_mean_within_all  \\\n",
       "0                            0.172636                            0.777472   \n",
       "1                            0.172636                            0.777472   \n",
       "2                            0.172636                            0.777472   \n",
       "3                            0.172636                            0.777472   \n",
       "4                            0.172636                            0.777472   \n",
       "5                            0.172636                            0.777472   \n",
       "6                            0.106559                            0.600711   \n",
       "7                            0.106559                            0.600711   \n",
       "8                            0.106559                            0.600711   \n",
       "9                            0.106559                            0.600711   \n",
       "10                           0.106559                            0.600711   \n",
       "11                           0.106559                            0.600711   \n",
       "12                           0.099266                            0.242118   \n",
       "13                           0.099266                            0.242118   \n",
       "14                           0.099266                            0.242118   \n",
       "15                           0.099266                            0.242118   \n",
       "16                           0.099266                            0.242118   \n",
       "17                           0.099266                            0.242118   \n",
       "\n",
       "    SegmentCovariance_mean_within_all  disp_coefficient_all  \\\n",
       "0                            0.165758              1.358857   \n",
       "1                            0.165758              1.358857   \n",
       "2                            0.165758              1.358857   \n",
       "3                            0.165758              1.358857   \n",
       "4                            0.165758              1.358857   \n",
       "5                            0.165758              1.358857   \n",
       "6                            0.135649              1.751113   \n",
       "7                            0.135649              1.751113   \n",
       "8                            0.135649              1.751113   \n",
       "9                            0.135649              1.751113   \n",
       "10                           0.135649              1.751113   \n",
       "11                           0.135649              1.751113   \n",
       "12                           0.062459              2.245373   \n",
       "13                           0.062459              2.245373   \n",
       "14                           0.062459              2.245373   \n",
       "15                           0.062459              2.245373   \n",
       "16                           0.062459              2.245373   \n",
       "17                           0.062459              2.245373   \n",
       "\n",
       "    dispersion_first_n_all  dispersion_last_n_all  disp_coefficient_hld  \\\n",
       "0                 0.116642               1.357972              1.514239   \n",
       "1                 0.116642               1.357972              1.514239   \n",
       "2                 0.116642               1.357972              1.514239   \n",
       "3                 0.116642               1.357972              1.514239   \n",
       "4                 0.116642               1.357972              1.514239   \n",
       "5                 0.116642               1.357972              1.514239   \n",
       "6                 0.347210               1.716981              1.388928   \n",
       "7                 0.347210               1.716981              1.388928   \n",
       "8                 0.347210               1.716981              1.388928   \n",
       "9                 0.347210               1.716981              1.388928   \n",
       "10                0.347210               1.716981              1.388928   \n",
       "11                0.347210               1.716981              1.388928   \n",
       "12                4.585672               6.168105              4.360539   \n",
       "13                4.585672               6.168105              4.360539   \n",
       "14                4.585672               6.168105              4.360539   \n",
       "15                4.585672               6.168105              4.360539   \n",
       "16                4.585672               6.168105              4.360539   \n",
       "17                4.585672               6.168105              4.360539   \n",
       "\n",
       "    dispersion_first_n_hld  dispersion_last_n_hld  \n",
       "0                 0.101214               1.592155  \n",
       "1                 0.101214               1.592155  \n",
       "2                 0.101214               1.592155  \n",
       "3                 0.101214               1.592155  \n",
       "4                 0.101214               1.592155  \n",
       "5                 0.101214               1.592155  \n",
       "6                 0.574597               2.065530  \n",
       "7                 0.574597               2.065530  \n",
       "8                 0.574597               2.065530  \n",
       "9                 0.574597               2.065530  \n",
       "10                0.574597               2.065530  \n",
       "11                0.574597               2.065530  \n",
       "12                1.747128               7.569243  \n",
       "13                1.747128               7.569243  \n",
       "14                1.747128               7.569243  \n",
       "15                1.747128               7.569243  \n",
       "16                1.747128               7.569243  \n",
       "17                1.747128               7.569243  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaffold_align_metrics_df.merge(results_df, on=[\"model_index\", \"Perturbation\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/7583594.1.trapnell-login.q/ipykernel_1628571/4173618693.py:35: DtypeWarning:\n",
      "\n",
      "Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n",
      "/tmp/7583594.1.trapnell-login.q/ipykernel_1628571/4173618693.py:36: DtypeWarning:\n",
      "\n",
      "Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA plot model_idx 71: F1 score 0.71, mweight 25, timeonly 1\n",
      "Processing wnt-i in all dataset...\n",
      "Subset size: 187\n",
      "Starting point not in dataset. Using closest point: [ 1.82168643  1.29012443 -0.26144278]\n",
      "Processing wnt-i in hld dataset...\n",
      "Subset size: 187\n",
      "Starting point not in dataset. Using closest point: [ 1.99630838  1.3154439  -0.20895961]\n",
      "Processing tgfb-i in all dataset...\n",
      "Subset size: 245\n",
      "Starting point not in dataset. Using closest point: [ 1.83927206  1.28596941 -0.29201842]\n",
      "Processing tgfb-i in hld dataset...\n",
      "Subset size: 245\n",
      "Starting point not in dataset. Using closest point: [ 2.04486115  1.44040819 -0.25376278]\n",
      "Processing wt in all dataset...\n",
      "Subset size: 2437\n",
      "Starting point not in dataset. Using closest point: [ 1.76878228  1.7100162  -0.174527  ]\n",
      "Processing wt in hld dataset...\n",
      "Subset size: 2437\n",
      "Starting point not in dataset. Using closest point: [ 1.96912577  1.7797467  -0.14410615]\n",
      "Processing lmx1b in all dataset...\n",
      "Subset size: 776\n",
      "Starting point not in dataset. Using closest point: [ 1.84002696  1.34803321 -0.23661037]\n",
      "Processing lmx1b in hld dataset...\n",
      "Subset size: 776\n",
      "Starting point not in dataset. Using closest point: [ 2.00609791  1.37712739 -0.25303388]\n",
      "Processing gdf3 in all dataset...\n",
      "Subset size: 747\n",
      "Starting point not in dataset. Using closest point: [ 1.82765235  1.39959051 -0.05014554]\n",
      "Processing gdf3 in hld dataset...\n",
      "Subset size: 747\n",
      "Starting point not in dataset. Using closest point: [ 2.01171486  1.3084113  -0.10475443]\n",
      "Segment Direction Measures:\n",
      "Metric  model_index  SegmentColinearity_mean_within_hld  \\\n",
      "Mean             71                            0.746983   \n",
      "\n",
      "Metric  SegmentCovariance_mean_within_hld  model_index  \\\n",
      "Mean                             0.172636           71   \n",
      "\n",
      "Metric  SegmentColinearity_mean_within_all  SegmentCovariance_mean_within_all  \n",
      "Mean                              0.777472                           0.165758  \n",
      "Dispersion Metrics for Each Dataset:\n",
      "   model_index  disp_coefficient_all  dispersion_first_n_all  \\\n",
      "0           71              1.358857                0.116642   \n",
      "\n",
      "   dispersion_last_n_all  disp_coefficient_hld  dispersion_first_n_hld  \\\n",
      "0               1.357972              1.514239                0.101214   \n",
      "\n",
      "   dispersion_last_n_hld  \n",
      "0               1.592155  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>Perturbation</th>\n",
       "      <th>Initial_RMSE</th>\n",
       "      <th>Aligned_RMSE</th>\n",
       "      <th>SegmentColinearity</th>\n",
       "      <th>SegmentCovariance</th>\n",
       "      <th>SegmentColinearity_mean_within_hld</th>\n",
       "      <th>SegmentCovariance_mean_within_hld</th>\n",
       "      <th>SegmentColinearity_mean_within_all</th>\n",
       "      <th>SegmentCovariance_mean_within_all</th>\n",
       "      <th>disp_coefficient_all</th>\n",
       "      <th>dispersion_first_n_all</th>\n",
       "      <th>dispersion_last_n_all</th>\n",
       "      <th>disp_coefficient_hld</th>\n",
       "      <th>dispersion_first_n_hld</th>\n",
       "      <th>dispersion_last_n_hld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>0.234647</td>\n",
       "      <td>0.186641</td>\n",
       "      <td>0.961767</td>\n",
       "      <td>0.145345</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>tgfb-i</td>\n",
       "      <td>0.292043</td>\n",
       "      <td>0.190456</td>\n",
       "      <td>0.979616</td>\n",
       "      <td>0.183081</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>wt</td>\n",
       "      <td>0.194967</td>\n",
       "      <td>0.147977</td>\n",
       "      <td>0.993592</td>\n",
       "      <td>0.285032</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>lmx1b</td>\n",
       "      <td>0.452274</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>0.919742</td>\n",
       "      <td>0.245564</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>0.281640</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>0.969415</td>\n",
       "      <td>0.195387</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71</td>\n",
       "      <td>avg_pert</td>\n",
       "      <td>0.304050</td>\n",
       "      <td>0.196193</td>\n",
       "      <td>0.964826</td>\n",
       "      <td>0.210882</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_index Perturbation  Initial_RMSE  Aligned_RMSE  SegmentColinearity  \\\n",
       "0           71        wnt-i      0.234647      0.186641            0.961767   \n",
       "1           71       tgfb-i      0.292043      0.190456            0.979616   \n",
       "2           71           wt      0.194967      0.147977            0.993592   \n",
       "3           71        lmx1b      0.452274      0.289614            0.919742   \n",
       "4           71         gdf3      0.281640      0.124806            0.969415   \n",
       "5           71     avg_pert      0.304050      0.196193            0.964826   \n",
       "\n",
       "   SegmentCovariance  SegmentColinearity_mean_within_hld  \\\n",
       "0           0.145345                            0.746983   \n",
       "1           0.183081                            0.746983   \n",
       "2           0.285032                            0.746983   \n",
       "3           0.245564                            0.746983   \n",
       "4           0.195387                            0.746983   \n",
       "5           0.210882                            0.746983   \n",
       "\n",
       "   SegmentCovariance_mean_within_hld  SegmentColinearity_mean_within_all  \\\n",
       "0                           0.172636                            0.777472   \n",
       "1                           0.172636                            0.777472   \n",
       "2                           0.172636                            0.777472   \n",
       "3                           0.172636                            0.777472   \n",
       "4                           0.172636                            0.777472   \n",
       "5                           0.172636                            0.777472   \n",
       "\n",
       "   SegmentCovariance_mean_within_all  disp_coefficient_all  \\\n",
       "0                           0.165758              1.358857   \n",
       "1                           0.165758              1.358857   \n",
       "2                           0.165758              1.358857   \n",
       "3                           0.165758              1.358857   \n",
       "4                           0.165758              1.358857   \n",
       "5                           0.165758              1.358857   \n",
       "\n",
       "   dispersion_first_n_all  dispersion_last_n_all  disp_coefficient_hld  \\\n",
       "0                0.116642               1.357972              1.514239   \n",
       "1                0.116642               1.357972              1.514239   \n",
       "2                0.116642               1.357972              1.514239   \n",
       "3                0.116642               1.357972              1.514239   \n",
       "4                0.116642               1.357972              1.514239   \n",
       "5                0.116642               1.357972              1.514239   \n",
       "\n",
       "   dispersion_first_n_hld  dispersion_last_n_hld  \n",
       "0                0.101214               1.592155  \n",
       "1                0.101214               1.592155  \n",
       "2                0.101214               1.592155  \n",
       "3                0.101214               1.592155  \n",
       "4                0.101214               1.592155  \n",
       "5                0.101214               1.592155  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# pert_comparisons = [\"wnt-i\", \"wt\"]\n",
    "pert_comparisons = [\"wnt-i\", \"tgfb-i\", \"wt\", \"lmx1b\", \"gdf3\"]\n",
    "\n",
    "color_map = {\n",
    "    \"wnt-i\": \"red\",\n",
    "    \"tgfb-i\": \"green\",\n",
    "    \"wt\": \"blue\",\n",
    "    \"lmx1b\": \"orange\",\n",
    "    \"gdf3\": \"purple\"\n",
    "}\n",
    "\n",
    "splines_final_dict = {}\n",
    "scaffold_align_metrics = []\n",
    "results_dict = {}\n",
    "\n",
    "# Example model_indices, adjust as needed\n",
    "model_indices = [71]\n",
    "\n",
    "for model_index in model_indices:\n",
    "    print(model_index)\n",
    "    path_all = merged_df_avg[merged_df_avg[\"model_index\"] == model_index][\"embryo_df_path_nohld\"].iloc[0]\n",
    "    path_hld = merged_df_avg[merged_df_avg[\"model_index\"] == model_index][\"embryo_df_path_hld\"].iloc[0]\n",
    "\n",
    "    score = merged_df_avg[merged_df_avg[\"model_index\"] == model_index][\"F1_score_all\"].iloc[0]\n",
    "    mweight = merged_df_avg[merged_df_avg[\"model_index\"] == model_index][\"metric_weight\"].iloc[0]\n",
    "    timeonly = merged_df_avg[merged_df_avg[\"model_index\"] == model_index][\"time_only_flag\"].iloc[0]\n",
    "\n",
    "    df_all = pd.read_csv(path_all)\n",
    "    df_hld = pd.read_csv(path_hld)\n",
    "\n",
    "    # Identify biological Z columns\n",
    "    z_mu_columns = [col for col in df_all.columns if 'z_mu' in col]    \n",
    "    z_mu_biological_columns = [col for col in z_mu_columns if \"b\" in col]\n",
    "\n",
    "    # Dictionary to store dataframes with PCA columns added\n",
    "    data_dict = {}\n",
    "\n",
    "    # Compute PCA and augment dataframes for both \"all\" and \"hld\"\n",
    "    for df_label, df_raw in [(\"all\", df_all), (\"hld\", df_hld)]:\n",
    "        X = df_raw[z_mu_biological_columns].values\n",
    "        pca = PCA(n_components=3)\n",
    "        pcs = pca.fit_transform(X)\n",
    "\n",
    "        df_raw[\"PCA_1\"] = pcs[:,0]\n",
    "        df_raw[\"PCA_2\"] = pcs[:,1]\n",
    "        df_raw[\"PCA_3\"] = pcs[:,2]\n",
    "\n",
    "        # Color mapping for perturbations\n",
    "        perturbations = pert_comparisons\n",
    "        color_discrete_map = {pert: px.colors.qualitative.Plotly[i % 10] for i, pert in enumerate(perturbations)}\n",
    "        df_raw['color'] = df_raw['phenotype'].map(color_discrete_map)\n",
    "\n",
    "        # Store the augmented dataframe\n",
    "        data_dict[df_label] = df_raw\n",
    "\n",
    "    # Dictionary to store spline points for each dataset and perturbation\n",
    "    # Key: (df_label, pert), Value: array of spline points shape (num_spline_points, 3)\n",
    "    splines_dict = {}\n",
    "\n",
    "    # Fit splines for each perturbation and dataset in a single combined loop\n",
    "    for pert in pert_comparisons:\n",
    "        for df_label, df in data_dict.items():\n",
    "            print(f\"Processing {pert} in {df_label} dataset...\")\n",
    "\n",
    "            pert_df = df[df[\"phenotype\"] == pert].reset_index(drop=True)\n",
    "\n",
    "            # Calculate early time point\n",
    "            avg_early_timepoint = pert_df[\n",
    "                (pert_df[\"predicted_stage_hpf\"] >= pert_df[\"predicted_stage_hpf\"].min()) &\n",
    "                (pert_df[\"predicted_stage_hpf\"] < pert_df[\"predicted_stage_hpf\"].min() + 1)\n",
    "            ][[\"PCA_1\", \"PCA_2\", \"PCA_3\"]].mean().values\n",
    "\n",
    "            # Downsampling logic\n",
    "            if pert == \"wt\":\n",
    "                pert_df_subset = pert_df.sample(frac=0.05, random_state=42)\n",
    "            else:\n",
    "                pert_df_subset = pert_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "            print(f\"Subset size: {len(pert_df_subset)}\")\n",
    "\n",
    "            pert_3d_subset = pert_df_subset[[\"PCA_1\", \"PCA_2\", \"PCA_3\"]].values\n",
    "\n",
    "            # Fit the Local Principal Curve on the subset\n",
    "            lpc = LocalPrincipalCurve(bandwidth=.5, max_iter=500, tol=1e-4, angle_penalty_exp=2)\n",
    "            paths = lpc.fit(pert_3d_subset, start_points=[avg_early_timepoint], remove_similar_end_start_points=True)\n",
    "\n",
    "            # Extract the first path (assuming one main path)\n",
    "            spline_points = lpc.cubic_splines[0]  # shape: (num_points, 3)\n",
    "            splines_dict[(df_label, pert)] = spline_points\n",
    "\n",
    "    # Convert spline data to a DataFrame\n",
    "    rows = []\n",
    "    for (df_label, pert), spline_points in splines_dict.items():\n",
    "        num_points = len(spline_points)\n",
    "        for i, point in enumerate(spline_points[::-1], start=1):\n",
    "            rows.append({\n",
    "                \"dataset\": df_label,\n",
    "                \"Perturbation\": pert,\n",
    "                \"point_index\": num_points - i,\n",
    "                \"PCA_1\": point[0],\n",
    "                \"PCA_2\": point[1],\n",
    "                \"PCA_3\": point[2]\n",
    "            })\n",
    "\n",
    "    splines_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Alignment and scaffold metrics\n",
    "    splines_dict_aligned = []\n",
    "    all_combined = []\n",
    "    hld_combined = []\n",
    "    hld_aligned_combined = []\n",
    "\n",
    "    for pert in pert_comparisons:\n",
    "        all_points = extract_spline(splines_df, \"all\", pert)\n",
    "        hld_points = extract_spline(splines_df, \"hld\", pert)\n",
    "\n",
    "        # Perform Kabsch alignment\n",
    "        R, t = quaternion_alignment(all_points, hld_points)\n",
    "        hld_aligned = (hld_points @ R.T) + t\n",
    "\n",
    "        # Compute errors\n",
    "        initial_rmse = rmse(all_points, hld_points)\n",
    "        aligned_rmse = rmse(all_points, hld_aligned)\n",
    "\n",
    "        # Accumulate for scaffold comparison\n",
    "        all_combined.append(all_points)\n",
    "        hld_combined.append(hld_points)\n",
    "        hld_aligned_combined.append(hld_aligned)\n",
    "\n",
    "        splines_dict_aligned.append({\"Perturbation\": pert, \"spline\": hld_aligned})\n",
    "        scaffold_align_metrics.append({\n",
    "            \"model_index\": model_index,\n",
    "            'Perturbation': pert,\n",
    "            'Initial_RMSE': initial_rmse,\n",
    "            'Aligned_RMSE': aligned_rmse\n",
    "        })\n",
    "\n",
    "    # Compute scaffold-level metrics\n",
    "    all_combined = np.concatenate(all_combined, axis=0)\n",
    "    hld_combined = np.concatenate(hld_combined, axis=0)\n",
    "    hld_aligned_combined = np.concatenate(hld_aligned_combined, axis=0)\n",
    "\n",
    "    scaffold_initial_rmse = rmse(all_combined, hld_combined)\n",
    "    scaffold_aligned_rmse = rmse(all_combined, hld_aligned_combined)\n",
    "\n",
    "    scaffold_align_metrics.append({\n",
    "        \"model_index\": model_index,\n",
    "        'Perturbation': 'avg_pert',\n",
    "        'Initial_RMSE': scaffold_initial_rmse,\n",
    "        'Aligned_RMSE': scaffold_aligned_rmse\n",
    "    })\n",
    "\n",
    "    scaffold_align_metrics_df = pd.DataFrame(scaffold_align_metrics)\n",
    "\n",
    "    # Add aligned spline points to DataFrame\n",
    "    for spline in splines_dict_aligned:\n",
    "        for i, point in enumerate(spline[\"spline\"]):\n",
    "            rows.append({\n",
    "                \"dataset\": \"hld_aligned\",\n",
    "                \"Perturbation\": spline[\"Perturbation\"],\n",
    "                \"point_index\": i,\n",
    "                \"PCA_1\": point[0],\n",
    "                \"PCA_2\": point[1],\n",
    "                \"PCA_3\": point[2],\n",
    "            })\n",
    "\n",
    "    #store the spline in dict\n",
    "    splines_final_df = pd.DataFrame(rows)\n",
    "    splines_final_dict[model_index] = splines_final_df\n",
    "    \n",
    "    # Process segment direction consistency\n",
    "    across_seg_df, within_seg_measures = process_segment_direction(splines_final_df, model_index)\n",
    "    print(\"Segment Direction Measures:\")\n",
    "    print(within_seg_measures)\n",
    "\n",
    "    # Calculate dispersion metrics\n",
    "    dispersion_metrics_df = calculate_dispersion_metrics(splines_final_df, n=5)\n",
    "    combined_dispersion_df = process_dispersion_metrics(dispersion_metrics_df, model_index)\n",
    "    print(\"Dispersion Metrics for Each Dataset:\")\n",
    "    print(combined_dispersion_df)\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    results_dict[model_index] = {\n",
    "        \"across_seg_df\": across_seg_df,\n",
    "        \"within_seg_measures\": within_seg_measures,\n",
    "        \"dispersion_metrics\": combined_dispersion_df\n",
    "    }\n",
    "    \n",
    "    results_df = combine_results_dict(results_dict)\n",
    "\n",
    "    scaffold_align_metrics_df_final = scaffold_align_metrics_df.merge(results_df, on=[\"model_index\", \"Perturbation\"], how=\"left\")\n",
    "\n",
    "scaffold_align_metrics_df_final\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>Perturbation</th>\n",
       "      <th>Initial_RMSE</th>\n",
       "      <th>Aligned_RMSE</th>\n",
       "      <th>SegmentColinearity</th>\n",
       "      <th>SegmentCovariance</th>\n",
       "      <th>SegmentColinearity_mean_within_hld</th>\n",
       "      <th>SegmentCovariance_mean_within_hld</th>\n",
       "      <th>SegmentColinearity_mean_within_all</th>\n",
       "      <th>SegmentCovariance_mean_within_all</th>\n",
       "      <th>disp_coefficient_all</th>\n",
       "      <th>dispersion_first_n_all</th>\n",
       "      <th>dispersion_last_n_all</th>\n",
       "      <th>disp_coefficient_hld</th>\n",
       "      <th>dispersion_first_n_hld</th>\n",
       "      <th>dispersion_last_n_hld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>0.234647</td>\n",
       "      <td>0.186641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>tgfb-i</td>\n",
       "      <td>0.292043</td>\n",
       "      <td>0.190456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>wt</td>\n",
       "      <td>0.194967</td>\n",
       "      <td>0.147977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>lmx1b</td>\n",
       "      <td>0.452274</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>0.281640</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71</td>\n",
       "      <td>avg_pert</td>\n",
       "      <td>0.304050</td>\n",
       "      <td>0.196193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_index Perturbation  Initial_RMSE  Aligned_RMSE  SegmentColinearity  \\\n",
       "0           71        wnt-i      0.234647      0.186641                 NaN   \n",
       "1           71       tgfb-i      0.292043      0.190456                 NaN   \n",
       "2           71           wt      0.194967      0.147977                 NaN   \n",
       "3           71        lmx1b      0.452274      0.289614                 NaN   \n",
       "4           71         gdf3      0.281640      0.124806                 NaN   \n",
       "5           71     avg_pert      0.304050      0.196193                 NaN   \n",
       "\n",
       "   SegmentCovariance  SegmentColinearity_mean_within_hld  \\\n",
       "0                NaN                                 NaN   \n",
       "1                NaN                                 NaN   \n",
       "2                NaN                                 NaN   \n",
       "3                NaN                                 NaN   \n",
       "4                NaN                                 NaN   \n",
       "5                NaN                                 NaN   \n",
       "\n",
       "   SegmentCovariance_mean_within_hld  SegmentColinearity_mean_within_all  \\\n",
       "0                                NaN                                 NaN   \n",
       "1                                NaN                                 NaN   \n",
       "2                                NaN                                 NaN   \n",
       "3                                NaN                                 NaN   \n",
       "4                                NaN                                 NaN   \n",
       "5                                NaN                                 NaN   \n",
       "\n",
       "   SegmentCovariance_mean_within_all  disp_coefficient_all  \\\n",
       "0                                NaN                   NaN   \n",
       "1                                NaN                   NaN   \n",
       "2                                NaN                   NaN   \n",
       "3                                NaN                   NaN   \n",
       "4                                NaN                   NaN   \n",
       "5                                NaN                   NaN   \n",
       "\n",
       "   dispersion_first_n_all  dispersion_last_n_all  disp_coefficient_hld  \\\n",
       "0                     NaN                    NaN                   NaN   \n",
       "1                     NaN                    NaN                   NaN   \n",
       "2                     NaN                    NaN                   NaN   \n",
       "3                     NaN                    NaN                   NaN   \n",
       "4                     NaN                    NaN                   NaN   \n",
       "5                     NaN                    NaN                   NaN   \n",
       "\n",
       "   dispersion_first_n_hld  dispersion_last_n_hld  \n",
       "0                     NaN                    NaN  \n",
       "1                     NaN                    NaN  \n",
       "2                     NaN                    NaN  \n",
       "3                     NaN                    NaN  \n",
       "4                     NaN                    NaN  \n",
       "5                     NaN                    NaN  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaffold_align_metrics_df.merge(results_df, on=[\"model_index\", \"Perturbation\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>Perturbation</th>\n",
       "      <th>Initial_RMSE</th>\n",
       "      <th>Aligned_RMSE</th>\n",
       "      <th>SegmentColinearity</th>\n",
       "      <th>SegmentCovariance</th>\n",
       "      <th>SegmentColinearity_mean_within_hld</th>\n",
       "      <th>SegmentCovariance_mean_within_hld</th>\n",
       "      <th>SegmentColinearity_mean_within_all</th>\n",
       "      <th>SegmentCovariance_mean_within_all</th>\n",
       "      <th>disp_coefficient_all</th>\n",
       "      <th>dispersion_first_n_all</th>\n",
       "      <th>dispersion_last_n_all</th>\n",
       "      <th>disp_coefficient_hld</th>\n",
       "      <th>dispersion_first_n_hld</th>\n",
       "      <th>dispersion_last_n_hld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>0.234647</td>\n",
       "      <td>0.186641</td>\n",
       "      <td>0.961767</td>\n",
       "      <td>0.145345</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>tgfb-i</td>\n",
       "      <td>0.292043</td>\n",
       "      <td>0.190456</td>\n",
       "      <td>0.979616</td>\n",
       "      <td>0.183081</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>wt</td>\n",
       "      <td>0.194967</td>\n",
       "      <td>0.147977</td>\n",
       "      <td>0.993592</td>\n",
       "      <td>0.285032</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>lmx1b</td>\n",
       "      <td>0.452274</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>0.919742</td>\n",
       "      <td>0.245564</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>0.281640</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>0.969415</td>\n",
       "      <td>0.195387</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71</td>\n",
       "      <td>avg_pert</td>\n",
       "      <td>0.304050</td>\n",
       "      <td>0.196193</td>\n",
       "      <td>0.964826</td>\n",
       "      <td>0.210882</td>\n",
       "      <td>0.746983</td>\n",
       "      <td>0.172636</td>\n",
       "      <td>0.777472</td>\n",
       "      <td>0.165758</td>\n",
       "      <td>1.358857</td>\n",
       "      <td>0.116642</td>\n",
       "      <td>1.357972</td>\n",
       "      <td>1.514239</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>1.592155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_index Perturbation  Initial_RMSE  Aligned_RMSE  SegmentColinearity  \\\n",
       "0           71        wnt-i      0.234647      0.186641            0.961767   \n",
       "1           71       tgfb-i      0.292043      0.190456            0.979616   \n",
       "2           71           wt      0.194967      0.147977            0.993592   \n",
       "3           71        lmx1b      0.452274      0.289614            0.919742   \n",
       "4           71         gdf3      0.281640      0.124806            0.969415   \n",
       "5           71     avg_pert      0.304050      0.196193            0.964826   \n",
       "\n",
       "   SegmentCovariance  SegmentColinearity_mean_within_hld  \\\n",
       "0           0.145345                            0.746983   \n",
       "1           0.183081                            0.746983   \n",
       "2           0.285032                            0.746983   \n",
       "3           0.245564                            0.746983   \n",
       "4           0.195387                            0.746983   \n",
       "5           0.210882                            0.746983   \n",
       "\n",
       "   SegmentCovariance_mean_within_hld  SegmentColinearity_mean_within_all  \\\n",
       "0                           0.172636                            0.777472   \n",
       "1                           0.172636                            0.777472   \n",
       "2                           0.172636                            0.777472   \n",
       "3                           0.172636                            0.777472   \n",
       "4                           0.172636                            0.777472   \n",
       "5                           0.172636                            0.777472   \n",
       "\n",
       "   SegmentCovariance_mean_within_all  disp_coefficient_all  \\\n",
       "0                           0.165758              1.358857   \n",
       "1                           0.165758              1.358857   \n",
       "2                           0.165758              1.358857   \n",
       "3                           0.165758              1.358857   \n",
       "4                           0.165758              1.358857   \n",
       "5                           0.165758              1.358857   \n",
       "\n",
       "   dispersion_first_n_all  dispersion_last_n_all  disp_coefficient_hld  \\\n",
       "0                0.116642               1.357972              1.514239   \n",
       "1                0.116642               1.357972              1.514239   \n",
       "2                0.116642               1.357972              1.514239   \n",
       "3                0.116642               1.357972              1.514239   \n",
       "4                0.116642               1.357972              1.514239   \n",
       "5                0.116642               1.357972              1.514239   \n",
       "\n",
       "   dispersion_first_n_hld  dispersion_last_n_hld  \n",
       "0                0.101214               1.592155  \n",
       "1                0.101214               1.592155  \n",
       "2                0.101214               1.592155  \n",
       "3                0.101214               1.592155  \n",
       "4                0.101214               1.592155  \n",
       "5                0.101214               1.592155  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaffold_align_metrics_df_final[\"mo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>Perturbation</th>\n",
       "      <th>point_index</th>\n",
       "      <th>PCA_1</th>\n",
       "      <th>PCA_2</th>\n",
       "      <th>PCA_3</th>\n",
       "      <th>model_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>499</td>\n",
       "      <td>0.512206</td>\n",
       "      <td>-0.449847</td>\n",
       "      <td>1.558961</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>498</td>\n",
       "      <td>0.517657</td>\n",
       "      <td>-0.454282</td>\n",
       "      <td>1.554024</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>497</td>\n",
       "      <td>0.522607</td>\n",
       "      <td>-0.458513</td>\n",
       "      <td>1.548412</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>496</td>\n",
       "      <td>0.527217</td>\n",
       "      <td>-0.462680</td>\n",
       "      <td>1.542470</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all</td>\n",
       "      <td>wnt-i</td>\n",
       "      <td>495</td>\n",
       "      <td>0.531631</td>\n",
       "      <td>-0.466741</td>\n",
       "      <td>1.536310</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>hld_aligned</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>495</td>\n",
       "      <td>-3.847806</td>\n",
       "      <td>0.444283</td>\n",
       "      <td>-2.961415</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>hld_aligned</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>496</td>\n",
       "      <td>-3.818904</td>\n",
       "      <td>0.437477</td>\n",
       "      <td>-3.071533</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>hld_aligned</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>497</td>\n",
       "      <td>-3.793124</td>\n",
       "      <td>0.414569</td>\n",
       "      <td>-3.180309</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>hld_aligned</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>498</td>\n",
       "      <td>-3.766719</td>\n",
       "      <td>0.386207</td>\n",
       "      <td>-3.287675</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>hld_aligned</td>\n",
       "      <td>gdf3</td>\n",
       "      <td>499</td>\n",
       "      <td>-3.723473</td>\n",
       "      <td>0.361417</td>\n",
       "      <td>-3.388682</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset Perturbation  point_index     PCA_1     PCA_2     PCA_3  \\\n",
       "0              all        wnt-i          499  0.512206 -0.449847  1.558961   \n",
       "1              all        wnt-i          498  0.517657 -0.454282  1.554024   \n",
       "2              all        wnt-i          497  0.522607 -0.458513  1.548412   \n",
       "3              all        wnt-i          496  0.527217 -0.462680  1.542470   \n",
       "4              all        wnt-i          495  0.531631 -0.466741  1.536310   \n",
       "...            ...          ...          ...       ...       ...       ...   \n",
       "22495  hld_aligned         gdf3          495 -3.847806  0.444283 -2.961415   \n",
       "22496  hld_aligned         gdf3          496 -3.818904  0.437477 -3.071533   \n",
       "22497  hld_aligned         gdf3          497 -3.793124  0.414569 -3.180309   \n",
       "22498  hld_aligned         gdf3          498 -3.766719  0.386207 -3.287675   \n",
       "22499  hld_aligned         gdf3          499 -3.723473  0.361417 -3.388682   \n",
       "\n",
       "       model_index  \n",
       "0               71  \n",
       "1               71  \n",
       "2               71  \n",
       "3               71  \n",
       "4               71  \n",
       "...            ...  \n",
       "22495           78  \n",
       "22496           78  \n",
       "22497           78  \n",
       "22498           78  \n",
       "22499           78  \n",
       "\n",
       "[22500 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splines_final_df_model_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae_env_cluster",
   "language": "python",
   "name": "vae_env_cluster"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
